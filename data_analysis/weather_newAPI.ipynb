{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04ce0d6d",
   "metadata": {},
   "source": [
    "# Retrieve Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c1334c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "\n",
    "# 保存先ディレクトリの作成\n",
    "output_dir = 'weather_data'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Vancouverの現地時間で期間を設定\n",
    "start_date_local = '2025-12-17'\n",
    "end_date_local = '2025-12-24'\n",
    "\n",
    "# 日付範囲を生成 (Vancouver時間)\n",
    "dates = pd.date_range(start=start_date_local, end=end_date_local, freq='D', tz='America/Vancouver')\n",
    "\n",
    "weekly_data = []\n",
    "week_start_date = dates[0]\n",
    "\n",
    "base_url = \"https://api.weather.gc.ca/collections/swob-realtime/items\"\n",
    "bbox = \"-123.35,49.00,-122.40,49.40\"\n",
    "properties = \"date_tm-value,stn_nam-value,air_temp,air_temp-qa,dwpt_temp,dwpt_temp-qa,rel_hum,rel_hum-qa,avg_wnd_spd_10m_pst10mts,avg_wnd_spd_10m_pst1hr,rnfl_amt_pst1mt,rnfl_amt_pst1mt-qa,rnfl_amt_pst1hr,rnfl_amt_pst1hr-qa,vis,vis-qa,avg_vis_pst10mts\"\n",
    "\n",
    "print(f\"Fetching data from {dates[0]} to {dates[-1]} (Vancouver Time)...\")\n",
    "\n",
    "for i in range(len(dates) - 1):\n",
    "    start_dt = dates[i]\n",
    "    end_dt = dates[i+1]\n",
    "    \n",
    "    # API用にUTCに変換\n",
    "    start_utc = start_dt.tz_convert('UTC').strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "    end_utc = end_dt.tz_convert('UTC').strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "    \n",
    "    print(f\"Processing: {start_utc} -> {end_utc}\")\n",
    "    \n",
    "    params = {\n",
    "        \"f\": \"json\",\n",
    "        \"bbox\": bbox,\n",
    "        \"datetime\": f\"{start_utc}/{end_utc}\",\n",
    "        \"properties\": properties,\n",
    "        \"sortby\": \"date_tm-value\",\n",
    "        \"limit\": \"10000\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(base_url, params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        if 'features' in data and len(data['features']) > 0:\n",
    "            df = pd.json_normalize(data['features'])\n",
    "            # カラム名のクリーニング\n",
    "            df.columns = [c.replace('properties.', '') for c in df.columns]\n",
    "            weekly_data.append(df)\n",
    "            print(f\"  Fetched {len(df)} records\")\n",
    "        else:\n",
    "            print(\"  No data found\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "        \n",
    "    time.sleep(3) # APIへの負荷軽減\n",
    "\n",
    "    # 1週間ごと、または最後のループで保存\n",
    "    if (i + 1) % 7 == 0 or i == len(dates) - 2:\n",
    "        if weekly_data:\n",
    "            weather_chunk = pd.concat(weekly_data, ignore_index=True)\n",
    "            \n",
    "            # 不要なカラムの削除\n",
    "            cols_to_drop = ['id', 'type', 'geometry.type']\n",
    "            weather_chunk = weather_chunk.drop(columns=[c for c in cols_to_drop if c in weather_chunk.columns])\n",
    "            \n",
    "            # ファイル名の生成 (開始日_終了日)\n",
    "            filename = f\"{output_dir}/weather_vancouver_{week_start_date.strftime('%Y%m%d')}_{end_dt.strftime('%Y%m%d')}.csv\"\n",
    "            weather_chunk.to_csv(filename, index=False)\n",
    "            \n",
    "            print(f\"Saved {len(weather_chunk)} records to {filename}\")\n",
    "            \n",
    "            # 次の週のためにリセット\n",
    "            weekly_data = []\n",
    "            # weather変数を更新（後続のセルのために最後のチャンクを残す）\n",
    "            weather = weather_chunk\n",
    "        \n",
    "        # 次の週の開始日を設定\n",
    "        if i < len(dates) - 2:\n",
    "            week_start_date = dates[i+1]\n",
    "\n",
    "print(\"Data collection complete.\")\n",
    "if 'weather' in locals():\n",
    "    display(weather.head())\n",
    "else:\n",
    "    print(\"No data collected.\")\n",
    "    weather = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83515cd6",
   "metadata": {},
   "source": [
    "# Exploring Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f1d561",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "output_dir = 'weather_data'\n",
    "\n",
    "data_list = os.listdir(output_dir)\n",
    "weather = pd.DataFrame()\n",
    "for file in data_list:\n",
    "    if file.endswith('.csv'):\n",
    "        file_path = os.path.join(output_dir, file)\n",
    "        temp_df = pd.read_csv(file_path)\n",
    "        if 'weather' in locals() and not weather.empty:\n",
    "            weather = pd.concat([weather, temp_df], ignore_index=True)\n",
    "        else:\n",
    "            weather = temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b8ba65",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_check = ['vis', 'rnfl_amt_pst1mt', 'rnfl_amt_pst1hr', 'air_temp', 'rel_hum', 'dwpt_temp', 'avg_wnd_spd_10m_pst10mts', 'avg_wnd_spd_10m_pst1hr']\n",
    "\n",
    "# Calculate the number of null values for each column grouped by station name\n",
    "result = weather.groupby('stn_nam-value')[cols_to_check].apply(lambda x: x.isnull().sum())\n",
    "result['total_rows'] = weather.groupby('stn_nam-value').size()\n",
    "result[['total_rows']+cols_to_check]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b6ae7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnfl_amt_pst1hrが全て欠損しているステーションを特定\n",
    "rain_null_check = weather.groupby('stn_nam-value')['rnfl_amt_pst1hr'].apply(lambda x: x.isnull().all())\n",
    "exclude_stations = rain_null_check[rain_null_check].index.tolist()\n",
    "exclude_stations = list(set(exclude_stations))\n",
    "\n",
    "weather = weather[~weather['stn_nam-value'].isin(exclude_stations)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc6b45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of null values for each column grouped by station name\n",
    "result = weather.groupby('stn_nam-value')[cols_to_check].apply(lambda x: x.isnull().sum())\n",
    "result['total_rows'] = weather.groupby('stn_nam-value').size()\n",
    "result[['total_rows']+cols_to_check]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5dba92",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather['date_tm-value'] = pd.to_datetime(weather['date_tm-value'])\n",
    "weather['local_time'] = weather['date_tm-value'].dt.tz_convert('America/Vancouver')\n",
    "weather['time_bucket'] = weather['local_time'].dt.floor('10min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e3850b",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.groupby(['stn_nam-value', 'time_bucket'])[cols_to_check].mean().isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ca41d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "# geometry info\n",
    "if isinstance(weather['geometry.coordinates'].iloc[0], str):\n",
    "    weather['geometry.coordinates'] = weather['geometry.coordinates'].apply(ast.literal_eval)\n",
    "station_coords = weather.groupby('stn_nam-value')['geometry.coordinates'].first()\n",
    "coords_df = pd.DataFrame(station_coords.tolist(), index=station_coords.index)\n",
    "coords_df = coords_df.iloc[:, :2]\n",
    "coords_df.columns = ['lon', 'lat']\n",
    "coords_df.index.name = 'station_name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce79383",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords_df.index.name = 'station_name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408cdf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords_df.to_csv('weather_data_full/station_coordinates_mst.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b8139e",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d18a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "output_dir = 'weather_data'\n",
    "\n",
    "data_list = os.listdir(output_dir)\n",
    "weather_all = pd.DataFrame()\n",
    "for file in data_list:\n",
    "    if file.endswith('.csv'):\n",
    "        file_path = os.path.join(output_dir, file)\n",
    "        weather = pd.read_csv(file_path)\n",
    "        weather = weather.copy()\n",
    "        \n",
    "        weather = weather[~weather['stn_nam-value'].isin(exclude_stations)]\n",
    "\n",
    "        # local_time_10min_blockを作成\n",
    "        weather['date_tm-value'] = pd.to_datetime(weather['date_tm-value'])\n",
    "        weather['local_time'] = weather['date_tm-value'].dt.tz_convert('America/Vancouver')\n",
    "        weather['time_bucket'] = weather['local_time'].dt.floor('10min')\n",
    "\n",
    "        # aggregation\n",
    "        agg_wind = weather.groupby(['stn_nam-value', 'time_bucket'])['avg_wnd_spd_10m_pst10mts'].mean().round(2)\n",
    "        agg_temp = weather.groupby(['stn_nam-value', 'time_bucket'])['air_temp'].mean().round(2)\n",
    "        agg_dew_point = weather.groupby(['stn_nam-value', 'time_bucket'])['dwpt_temp'].mean().round(2)\n",
    "        agg_hum = weather.groupby(['stn_nam-value', 'time_bucket'])['rel_hum'].mean().round(2)\n",
    "        agg_rain = weather.groupby(['stn_nam-value', 'time_bucket'])['rnfl_amt_pst1hr'].mean().round(2)\n",
    "\n",
    "\n",
    "        # merge\n",
    "        weather_agg = pd.concat([agg_wind, agg_temp, agg_dew_point, agg_hum, agg_rain], axis=1).reset_index()\n",
    "        weather_agg.columns = ['station_name', 'time_bucket', 'wind_speed', 'temperature', 'dew_point', 'relative_humidity', 'rainfall_amount']\n",
    "\n",
    "        # calculate humidex\n",
    "        weather_agg['humidex'] = round(weather_agg['temperature'] + (0.5555 * (6.11 * np.exp(5417.7530 * (1/273.15 - 1/(weather_agg['dew_point'] + 273.15))) - 10)), 2)\n",
    "        \n",
    "        output_file = f\"{output_dir}_agg/aggregated_{file}\"\n",
    "        weather_agg.to_csv(output_file, index=False)\n",
    "        weather_all = pd.concat([weather_all, weather_agg], ignore_index=True)\n",
    "        print(f\"Processed and saved: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125e15e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba06871",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_all.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cba57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_all.to_csv('weather_data_full/weather_data_aggregated_all_stations.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd54854",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.path.abspath('.')))\n",
    "from src.data_connection import DatabaseConnector\n",
    "\n",
    "db_connector = DatabaseConnector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8ed4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "region = db_connector.read_sql(f\"\"\"\n",
    "select region_id, center_lon, center_lat from \n",
    "gtfs_static.regions;\n",
    "\"\"\")\n",
    "region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f432420b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "weather_all = pd.read_csv('weather_data_full/weather_data_aggregated_all_stations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa69692",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282759be",
   "metadata": {},
   "source": [
    "# Match Regions to Nearest Weather Stations\n",
    "Calculate the distance between each region's center and all weather stations to find the nearest one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3545b942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load station coordinates\n",
    "station_coords = pd.read_csv('weather_data_full/station_coordinates_mst.csv')\n",
    "\n",
    "def get_nearest_station(row, stations_df):\n",
    "    # Calculate Euclidean distance\n",
    "    distances = np.sqrt(\n",
    "        (stations_df['lon'] - row['center_lon'])**2 + \n",
    "        (stations_df['lat'] - row['center_lat'])**2\n",
    "    )\n",
    "    nearest_idx = distances.idxmin()\n",
    "    return stations_df.loc[nearest_idx, 'station_name']\n",
    "\n",
    "# Find nearest station for each region\n",
    "region['nearest_station'] = region.apply(lambda row: get_nearest_station(row, station_coords), axis=1)\n",
    "\n",
    "region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7fcd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "region.to_csv('weather_data_full/regions_with_nearest_stations.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3641b862",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_all['station_name'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
