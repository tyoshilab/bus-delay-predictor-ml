{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b01d3a0",
   "metadata": {},
   "source": [
    "# 04. Model XGBoost\n",
    "XGBoost models implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a8ce81",
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nimport xgboost as xgb\nfrom sklearn.preprocessing import RobustScaler\nimport utils\n\n# Load Split Data using shared function\ndf_train, df_valid, df_test, df_process, split_info = utils.load_split_data_with_combined()\n\nif df_process is None:\n    raise RuntimeError(\"Data not found. Please run 02_process_data.ipynb first.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d485aa32",
   "metadata": {},
   "outputs": [],
   "source": "# Create Sequences using shared function\nn_past_trips = 5\ndata = utils.prepare_model_data(df_train, df_test, df_process, n_past_trips=n_past_trips)\n\n# Extract variables\nX_delays_train, X_features_train, X_agg_train, y_train = \\\n    data['X_delays_train'], data['X_features_train'], data['X_agg_train'], data['y_train']\nX_delays_test, X_features_test, X_agg_test, y_test = \\\n    data['X_delays_test'], data['X_features_test'], data['X_agg_test'], data['y_test']\nn_stops = data['n_stops']"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49aec8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Data for XGBoost\n",
    "if df_process is not None:\n",
    "    X_train_flat = np.concatenate([\n",
    "        X_delays_train.reshape(len(X_delays_train), -1),\n",
    "        X_features_train,\n",
    "        X_agg_train\n",
    "    ], axis=1)\n",
    "\n",
    "    X_test_flat = np.concatenate([\n",
    "        X_delays_test.reshape(len(X_delays_test), -1),\n",
    "        X_features_test,\n",
    "        X_agg_test\n",
    "    ], axis=1)\n",
    "\n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_flat)\n",
    "    X_test_scaled = scaler.transform(X_test_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651a4e6e",
   "metadata": {},
   "outputs": [],
   "source": "# Train XGBoost (Per Stop)\nevaluation_results = []\n\nprint(\"Training XGBoost...\")\ny_pred_xgb_all = []\n\nxgb_params = {\n    \"n_estimators\": 100,\n    \"max_depth\": 5,\n    \"learning_rate\": 0.1,\n    \"n_jobs\": -1,\n    \"random_state\": 42\n}\n\nfor stop_idx in range(n_stops):\n    model = xgb.XGBRegressor(**xgb_params)\n    model.fit(X_train_scaled, y_train[:, stop_idx])\n    pred = model.predict(X_test_scaled)\n    y_pred_xgb_all.append(pred)\n\ny_pred_xgb_all = np.array(y_pred_xgb_all).T\n\nresult_xgb = utils.evaluate_model(\n    y_test, y_pred_xgb_all,\n    model_name=\"XGBoost (Per Stop)\",\n    config={\"params\": xgb_params, \"n_past_trips\": n_past_trips, \"scaler\": \"RobustScaler\"}\n)\nevaluation_results.append(result_xgb)\nprint(result_xgb.summary())"
  },
  {
   "cell_type": "markdown",
   "id": "prclm8qcvwj",
   "metadata": {},
   "source": [
    "# Entity Embeddings for Categorical Variables\n",
    "\n",
    "カテゴリ変数（route_id, stop_id など）に対して埋め込み表現（Entity Embeddings）を学習します。\n",
    "これにより、LabelEncodingよりも意味のある表現が得られ、精度向上が期待できます。\n",
    "\n",
    "Phase 2への準備として、埋め込みの学習と保存を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ow66zk6ygtc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Entity Embeddings...\n",
      "  route_id: 222 categories -> 8D embedding\n",
      "  direction_id: 2 categories -> 2D embedding\n",
      "  region_id: 21 categories -> 4D embedding\n",
      "\n",
      "Training entity embeddings...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1767000960.659281  430231 service.cc:145] XLA service 0xffc3780 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1767000960.659335  430231 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 3060 Ti, Compute Capability 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 23/970\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 65688.9531 - mae: 162.5432"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1767000962.756538  430231 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m970/970\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - loss: 50874.6719 - mae: 148.6498 - val_loss: 49150.5859 - val_mae: 147.8472\n",
      "Epoch 2/10\n",
      "\u001b[1m970/970\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 47473.1445 - mae: 144.9365 - val_loss: 48924.0664 - val_mae: 147.4582\n",
      "Epoch 3/10\n",
      "\u001b[1m970/970\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 47182.0430 - mae: 144.3790 - val_loss: 48483.4688 - val_mae: 146.5894\n",
      "Epoch 4/10\n",
      "\u001b[1m970/970\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 46905.0000 - mae: 143.9731 - val_loss: 48237.5625 - val_mae: 145.3760\n",
      "Epoch 5/10\n",
      "\u001b[1m970/970\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 46712.8633 - mae: 143.6628 - val_loss: 48295.0273 - val_mae: 145.9314\n",
      "Epoch 6/10\n",
      "\u001b[1m970/970\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 46639.8359 - mae: 143.6346 - val_loss: 48130.2852 - val_mae: 144.0833\n",
      "Epoch 7/10\n",
      "\u001b[1m970/970\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 46521.9492 - mae: 143.4680 - val_loss: 48179.2422 - val_mae: 144.7015\n",
      "Epoch 8/10\n",
      "\u001b[1m970/970\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 46426.4727 - mae: 143.4148 - val_loss: 48209.0977 - val_mae: 145.5360\n",
      "Epoch 9/10\n",
      "\u001b[1m970/970\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 46317.1758 - mae: 143.3218 - val_loss: 48415.1680 - val_mae: 146.5183\n",
      "Epoch 10/10\n",
      "\u001b[1m970/970\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 46203.3867 - mae: 143.1316 - val_loss: 48249.9648 - val_mae: 145.9807\n",
      "Saved embeddings to data/processed_data/entity_embeddings.pkl\n",
      "\n",
      "Embedding shapes:\n",
      "  route_id: (222, 8)\n",
      "  direction_id: (2, 2)\n",
      "  region_id: (21, 4)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Flatten, Concatenate, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "\n",
    "class EntityEmbedder:\n",
    "    \"\"\"\n",
    "    Entity Embeddings for categorical variables.\n",
    "    \n",
    "    Learns dense vector representations for categorical features using a neural network,\n",
    "    then exports the embeddings for use in tree-based models like XGBoost.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dims=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embedding_dims: Dict of {column_name: embedding_dimension}\n",
    "                           If None, uses rule of thumb: min(50, n_categories // 2)\n",
    "        \"\"\"\n",
    "        self.embedding_dims = embedding_dims or {}\n",
    "        self.encoders = {}\n",
    "        self.embeddings = {}\n",
    "        self.model = None\n",
    "        \n",
    "    def _get_embedding_dim(self, n_categories, col_name):\n",
    "        \"\"\"Get embedding dimension for a categorical column\"\"\"\n",
    "        if col_name in self.embedding_dims:\n",
    "            return self.embedding_dims[col_name]\n",
    "        # Rule of thumb: min(50, n_categories // 2), at least 2\n",
    "        return max(2, min(50, n_categories // 2))\n",
    "    \n",
    "    def fit(self, df_train, categorical_cols, target_col, \n",
    "            numerical_cols=None, epochs=10, batch_size=256, verbose=1):\n",
    "        \"\"\"\n",
    "        Train entity embeddings using a simple neural network.\n",
    "        \n",
    "        Args:\n",
    "            df_train: Training DataFrame\n",
    "            categorical_cols: List of categorical column names\n",
    "            target_col: Target column name\n",
    "            numerical_cols: List of numerical column names (optional)\n",
    "            epochs: Number of training epochs\n",
    "            batch_size: Batch size\n",
    "            verbose: Verbosity level\n",
    "        \"\"\"\n",
    "        numerical_cols = numerical_cols or []\n",
    "        \n",
    "        # Encode categorical variables\n",
    "        cat_inputs = []\n",
    "        cat_embeddings = []\n",
    "        \n",
    "        for col in categorical_cols:\n",
    "            le = LabelEncoder()\n",
    "            df_train[f'{col}_encoded'] = le.fit_transform(df_train[col].astype(str))\n",
    "            self.encoders[col] = le\n",
    "            \n",
    "            n_categories = len(le.classes_)\n",
    "            emb_dim = self._get_embedding_dim(n_categories, col)\n",
    "            \n",
    "            # Create embedding layer\n",
    "            inp = Input(shape=(1,), name=f'{col}_input')\n",
    "            emb = Embedding(n_categories, emb_dim, name=f'{col}_embedding')(inp)\n",
    "            emb = Flatten()(emb)\n",
    "            \n",
    "            cat_inputs.append(inp)\n",
    "            cat_embeddings.append(emb)\n",
    "            \n",
    "            print(f\"  {col}: {n_categories} categories -> {emb_dim}D embedding\")\n",
    "        \n",
    "        # Numerical inputs\n",
    "        if numerical_cols:\n",
    "            num_input = Input(shape=(len(numerical_cols),), name='numerical_input')\n",
    "            all_inputs = cat_inputs + [num_input]\n",
    "            all_features = cat_embeddings + [num_input]\n",
    "        else:\n",
    "            all_inputs = cat_inputs\n",
    "            all_features = cat_embeddings\n",
    "        \n",
    "        # Combine all features\n",
    "        if len(all_features) > 1:\n",
    "            x = Concatenate()(all_features)\n",
    "        else:\n",
    "            x = all_features[0]\n",
    "        \n",
    "        # Simple network for learning embeddings\n",
    "        x = Dense(64, activation='relu')(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        x = Dense(32, activation='relu')(x)\n",
    "        output = Dense(1, activation='linear')(x)\n",
    "        \n",
    "        self.model = Model(inputs=all_inputs, outputs=output)\n",
    "        self.model.compile(optimizer=Adam(0.001), loss='mse', metrics=['mae'])\n",
    "        \n",
    "        # Prepare training data\n",
    "        X_cat = [df_train[f'{col}_encoded'].values for col in categorical_cols]\n",
    "        if numerical_cols:\n",
    "            X_num = df_train[numerical_cols].values\n",
    "            X_all = X_cat + [X_num]\n",
    "        else:\n",
    "            X_all = X_cat\n",
    "        \n",
    "        y = df_train[target_col].values\n",
    "        \n",
    "        # Train\n",
    "        print(\"\\nTraining entity embeddings...\")\n",
    "        self.model.fit(\n",
    "            X_all, y,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_split=0.2,\n",
    "            verbose=verbose\n",
    "        )\n",
    "        \n",
    "        # Extract learned embeddings\n",
    "        for col in categorical_cols:\n",
    "            emb_layer = self.model.get_layer(f'{col}_embedding')\n",
    "            self.embeddings[col] = emb_layer.get_weights()[0]\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, df, categorical_cols):\n",
    "        \"\"\"\n",
    "        Transform categorical columns to their embedding representations.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame to transform\n",
    "            categorical_cols: List of categorical columns to transform\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with embedding columns\n",
    "        \"\"\"\n",
    "        result = df.copy()\n",
    "        \n",
    "        for col in categorical_cols:\n",
    "            if col not in self.encoders:\n",
    "                raise ValueError(f\"Column {col} was not fitted\")\n",
    "            \n",
    "            le = self.encoders[col]\n",
    "            emb = self.embeddings[col]\n",
    "            \n",
    "            # Handle unseen categories\n",
    "            encoded = []\n",
    "            for val in df[col].astype(str):\n",
    "                if val in le.classes_:\n",
    "                    encoded.append(le.transform([val])[0])\n",
    "                else:\n",
    "                    encoded.append(0)  # Default to first category\n",
    "            encoded = np.array(encoded)\n",
    "            \n",
    "            # Get embeddings\n",
    "            emb_values = emb[encoded]\n",
    "            \n",
    "            # Add embedding columns\n",
    "            for i in range(emb_values.shape[1]):\n",
    "                result[f'{col}_emb_{i}'] = emb_values[:, i]\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def save(self, path):\n",
    "        \"\"\"Save embeddings to file\"\"\"\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'encoders': self.encoders,\n",
    "                'embeddings': self.embeddings,\n",
    "                'embedding_dims': self.embedding_dims\n",
    "            }, f)\n",
    "        print(f\"Saved embeddings to {path}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        \"\"\"Load embeddings from file\"\"\"\n",
    "        with open(path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        embedder = cls(embedding_dims=data['embedding_dims'])\n",
    "        embedder.encoders = data['encoders']\n",
    "        embedder.embeddings = data['embeddings']\n",
    "        return embedder\n",
    "\n",
    "\n",
    "# Train Entity Embeddings (Optional - can be slow)\n",
    "TRAIN_EMBEDDINGS = True  # Set to True to train embeddings\n",
    "\n",
    "if TRAIN_EMBEDDINGS and df_process is not None:\n",
    "    print(\"Training Entity Embeddings...\")\n",
    "    \n",
    "    # Prepare trip-level data for embedding training\n",
    "    trip_data = df_train.groupby('trip_key').agg({\n",
    "        'route_id': 'first',\n",
    "        'direction_id': 'first',\n",
    "        'region_id': 'first',\n",
    "        'arrival_delay_agg': 'mean',  # Target: average delay per trip\n",
    "        'hour': 'first',\n",
    "        'day_of_week': 'first',\n",
    "        'is_rush_hour': 'first'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Train embedder\n",
    "    embedder = EntityEmbedder(embedding_dims={\n",
    "        'route_id': 8,      # Reduce route to 8D\n",
    "        'direction_id': 2,  # Binary -> 2D\n",
    "        'region_id': 4      # Region -> 4D\n",
    "    })\n",
    "    \n",
    "    embedder.fit(\n",
    "        trip_data,\n",
    "        categorical_cols=['route_id', 'direction_id', 'region_id'],\n",
    "        target_col='arrival_delay_agg',\n",
    "        numerical_cols=['hour', 'day_of_week', 'is_rush_hour'],\n",
    "        epochs=10,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Save embeddings for future use\n",
    "    embedder.save('data/processed_data/entity_embeddings.pkl')\n",
    "    \n",
    "    print(\"\\nEmbedding shapes:\")\n",
    "    for col, emb in embedder.embeddings.items():\n",
    "        print(f\"  {col}: {emb.shape}\")\n",
    "else:\n",
    "    print(\"Entity Embeddings training skipped. Set TRAIN_EMBEDDINGS=True to train.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2tn9duih5h",
   "metadata": {},
   "outputs": [],
   "source": "# Model Comparison Table and Save Results\nutils.display_and_save_results(evaluation_results, 'data/evaluation_results_xgboost.json')"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}