{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bdcded9",
   "metadata": {},
   "source": [
    "# 06. Model ST-GNN\n",
    "ST-GNN model implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c03702",
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom sklearn.preprocessing import StandardScaler\nimport utils\n\n# ST-GNN specific data loading with fixed splits\ndef load_real_translink_data(\n    stops_txt_path='data/google_transit/stops.txt',\n    time_bin='15min'\n):\n    \"\"\"Load data using fixed train/valid/test splits\"\"\"\n\n    # Use shared function for data loading\n    df_train, df_valid, df_test, df, split_info = utils.load_split_data_with_combined()\n    \n    if df is None:\n        return None, None, None, None, None, None, None\n    \n    train_ratio = split_info['train_ratio_actual']\n\n    # Filter routes\n    target_routes = [6641, 6636, 37810, 6622, 6705, 6627, 16718, 6624, 37807, 6617]\n    df = df[df['route_id'].isin(target_routes)]\n\n    df['actual_arrival_time'] = pd.to_datetime(df['actual_arrival_time'], utc=True)\n    df['arrival_delay'] = df['arrival_delay_agg']\n    df['scheduled_arrival_time'] = pd.to_datetime(df['scheduled_arrival_time'], utc=True)\n\n    unique_stops = df['stop_id'].unique()\n    unique_stops.sort()\n    stop_to_idx = {stop_id: i for i, stop_id in enumerate(unique_stops)}\n    num_stops = len(unique_stops)\n\n    # Stops meta\n    try:\n        stops_meta = pd.read_csv(stops_txt_path)\n        stops_meta['stop_id'] = stops_meta['stop_id'].astype(str)\n        unique_stops_str = [str(s) for s in unique_stops]\n        stops_meta = stops_meta[stops_meta['stop_id'].isin(unique_stops_str)]\n        stops_df = pd.DataFrame({'stop_id': unique_stops_str})\n        stops_df = stops_df.merge(stops_meta[['stop_id', 'stop_lat', 'stop_lon']], on='stop_id', how='left')\n    except:\n        stops_df = pd.DataFrame({'stop_id': unique_stops})\n\n    # Edges\n    df = df.sort_values(['trip_id', 'actual_arrival_time'])\n    df['next_trip_id'] = df['trip_id'].shift(-1)\n    df['next_stop_id'] = df['stop_id'].shift(-1)\n\n    valid_connections = df[df['trip_id'] == df['next_trip_id']]\n    edges = set()\n    for _, row in valid_connections.iterrows():\n        src = stop_to_idx[row['stop_id']]\n        dst = stop_to_idx[row['next_stop_id']]\n        if src != dst:\n            edges.add((src, dst))\n    edge_list = list(edges)\n\n    # Time series features\n    df.set_index('actual_arrival_time', inplace=True)\n    travel_time_matrix = df.pivot_table(index=pd.Grouper(freq=time_bin), columns='stop_id', values='arrival_delay', aggfunc='mean')\n    travel_time_matrix = travel_time_matrix.reindex(columns=unique_stops)\n\n    # Mask Creation\n    mask_matrix = (~travel_time_matrix.isna()).astype(np.float32)\n    travel_time_matrix_filled = travel_time_matrix.ffill(limit=4).fillna(0)\n\n    travel_time_np = travel_time_matrix_filled.values\n    mask_np = mask_matrix.values\n\n    # Use fixed split ratio for scaler fitting\n    split_idx = int(len(travel_time_np) * train_ratio)\n    train_data = travel_time_np[:split_idx]\n\n    scaler_time = StandardScaler()\n    scaler_time.fit(train_data.reshape(-1, 1))\n    travel_time_np = scaler_time.transform(travel_time_np.reshape(-1, 1)).reshape(travel_time_np.shape)\n\n    # Features and targets\n    node_features = []\n    targets = []\n    for t in range(len(travel_time_matrix) - 1):\n        node_features.append(np.stack([travel_time_np[t]], axis=1))\n        targets.append(travel_time_np[t+1])\n\n    valid_sources = set(src for src, dst in edge_list)\n    node_mask = np.array([1.0 if i in valid_sources else 0.0 for i in range(num_stops)], dtype=np.float32)\n\n    return stops_df, edge_list, np.array(node_features, dtype=np.float32), np.array(targets, dtype=np.float32), scaler_time, node_mask, mask_np\n\nstops_df, edge_list, X_all, y_all, scaler_time, node_mask, data_mask_all = load_real_translink_data()"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c654f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAT Layer & ST-GNN Model\n",
    "class GATLayer(layers.Layer):\n",
    "    def __init__(self, units, heads=1, concat=True, **kwargs):\n",
    "        super(GATLayer, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.heads = heads\n",
    "        self.concat = concat\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        feat_dim = input_shape[0][-1]\n",
    "        self.W = self.add_weight(shape=(feat_dim, self.units * self.heads), initializer='glorot_uniform', trainable=True)\n",
    "        self.a = self.add_weight(shape=(1, self.heads, 2 * self.units), initializer='glorot_uniform', trainable=True)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        h, adj = inputs\n",
    "        batch_size = tf.shape(h)[0]\n",
    "        num_nodes = tf.shape(h)[1]\n",
    "        \n",
    "        h_prime = tf.matmul(h, self.W)\n",
    "        h_prime = tf.reshape(h_prime, (batch_size, num_nodes, self.heads, self.units))\n",
    "        \n",
    "        a1 = self.a[:, :, :self.units]\n",
    "        a2 = self.a[:, :, self.units:]\n",
    "        \n",
    "        score_i = tf.reduce_sum(h_prime * a1, axis=-1)\n",
    "        score_j = tf.reduce_sum(h_prime * a2, axis=-1)\n",
    "        \n",
    "        score_i = tf.expand_dims(score_i, 2)\n",
    "        score_j = tf.expand_dims(score_j, 1)\n",
    "        \n",
    "        e = tf.nn.leaky_relu(score_i + score_j)\n",
    "        \n",
    "        mask = tf.cast(adj, dtype=tf.bool)\n",
    "        mask = tf.expand_dims(mask, 0)\n",
    "        mask = tf.expand_dims(mask, -1)\n",
    "        \n",
    "        # Optimized to avoid creating a full tensor for zero_vec\n",
    "        attention = tf.where(mask, e, -9e15)\n",
    "        attention = tf.nn.softmax(attention, axis=2)\n",
    "        \n",
    "        out = tf.einsum('bijh,bjhu->bihu', attention, h_prime)\n",
    "        \n",
    "        if self.concat:\n",
    "            out = tf.reshape(out, (batch_size, num_nodes, self.heads * self.units))\n",
    "        else:\n",
    "            out = tf.reduce_mean(out, axis=2)\n",
    "        return out\n",
    "\n",
    "class ST_GNN(models.Model):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_nodes, num_heads=2):\n",
    "        super(ST_GNN, self).__init__()\n",
    "        self.gat1 = GATLayer(hidden_channels, heads=num_heads, concat=True)\n",
    "        self.gat2 = GATLayer(hidden_channels, heads=1, concat=False)\n",
    "        self.lstm = layers.LSTM(hidden_channels, return_sequences=False)\n",
    "        self.fc = layers.Dense(out_channels)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x, adj = inputs\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        window_size = tf.shape(x)[1]\n",
    "        num_nodes = tf.shape(x)[2]\n",
    "        features = tf.shape(x)[3]\n",
    "        \n",
    "        x_reshaped = tf.reshape(x, (batch_size * window_size, num_nodes, features))\n",
    "        h = self.gat1([x_reshaped, adj])\n",
    "        h = tf.nn.elu(h)\n",
    "        h = self.gat2([h, adj])\n",
    "        h = tf.nn.elu(h)\n",
    "        \n",
    "        h = tf.reshape(h, (batch_size, window_size, num_nodes, -1))\n",
    "        h = tf.transpose(h, perm=[0, 2, 1, 3])\n",
    "        h_reshaped = tf.reshape(h, (batch_size * num_nodes, window_size, -1))\n",
    "        \n",
    "        lstm_out = self.lstm(h_reshaped)\n",
    "        out = self.fc(lstm_out)\n",
    "        out = tf.reshape(out, (batch_size, num_nodes, -1))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4abcd453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ST-GNN...\n",
      "Epoch 0, Loss: 0.4787\n",
      "Epoch 1, Loss: 0.4754\n",
      "Epoch 2, Loss: 0.4555\n",
      "Epoch 3, Loss: 0.4551\n",
      "Epoch 4, Loss: 0.4588\n"
     ]
    }
   ],
   "source": [
    "# Prepare Data and Train\n",
    "if X_all is not None:\n",
    "    def create_dataset(edge_list, node_features, targets, masks, window_size=12):\n",
    "        num_timesteps = len(node_features)\n",
    "        num_nodes = node_features.shape[1]\n",
    "        X, Y, M = [], [], []\n",
    "        for i in range(window_size, num_timesteps):\n",
    "            X.append(node_features[i-window_size:i])\n",
    "            Y.append(targets[i].reshape(num_nodes, 1))\n",
    "            M.append(masks[i].reshape(num_nodes, 1))\n",
    "        \n",
    "        adj = np.eye(num_nodes, dtype=np.float32)\n",
    "        for src, dst in edge_list:\n",
    "            adj[dst, src] = 1.0\n",
    "        return np.array(X, dtype=np.float32), np.array(Y, dtype=np.float32), np.array(M, dtype=np.float32), adj\n",
    "\n",
    "    window_size = 6\n",
    "    X_data, Y_data, M_data, adj_matrix = create_dataset(edge_list, X_all, y_all, data_mask_all, window_size)\n",
    "    \n",
    "    split_idx = int(len(X_data) * 0.8)\n",
    "    X_train, X_test = X_data[:split_idx], X_data[split_idx:]\n",
    "    Y_train, Y_test = Y_data[:split_idx], Y_data[split_idx:]\n",
    "    M_train, M_test = M_data[:split_idx], M_data[split_idx:]\n",
    "    \n",
    "    model = ST_GNN(in_channels=X_train.shape[3], hidden_channels=32, out_channels=1, num_nodes=X_train.shape[2])\n",
    "    \n",
    "    print(\"Training ST-GNN...\")\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "    \n",
    "    # Simplified training loop\n",
    "    batch_size = 4 # Reduced batch size to avoid OOM\n",
    "    for epoch in range(5):\n",
    "        with tf.GradientTape() as tape:\n",
    "            preds = model([X_train[:batch_size], adj_matrix]) \n",
    "            # Masked Loss\n",
    "            loss = tf.reduce_sum(tf.square(Y_train[:batch_size] - preds) * M_train[:batch_size]) / (tf.reduce_sum(M_train[:batch_size]) + 1e-5)\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.numpy():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50ff119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation on Test Set\n",
    "evaluation_results = []\n",
    "\n",
    "print(\"Evaluating ST-GNN on Test Set...\")\n",
    "scale_factor = scaler_time.scale_[0]\n",
    "\n",
    "# Collect all predictions and actuals\n",
    "all_preds = []\n",
    "all_true = []\n",
    "all_masks = []\n",
    "\n",
    "for i in range(0, len(X_test), batch_size):\n",
    "    end_idx = min(i + batch_size, len(X_test))\n",
    "    batch_X = X_test[i:end_idx]\n",
    "    batch_Y = Y_test[i:end_idx]\n",
    "    batch_M = M_test[i:end_idx]\n",
    "\n",
    "    preds = model([batch_X, adj_matrix], training=False)\n",
    "\n",
    "    all_preds.append(preds.numpy())\n",
    "    all_true.append(batch_Y)\n",
    "    all_masks.append(batch_M)\n",
    "\n",
    "# Concatenate all results\n",
    "all_preds = np.concatenate(all_preds, axis=0)\n",
    "all_true = np.concatenate(all_true, axis=0)\n",
    "all_masks = np.concatenate(all_masks, axis=0)\n",
    "\n",
    "# Inverse transform to original scale (seconds)\n",
    "n_samples, n_nodes, _ = all_preds.shape\n",
    "all_preds_flat = all_preds.reshape(-1, 1)\n",
    "all_true_flat = all_true.reshape(-1, 1)\n",
    "all_masks_flat = all_masks.reshape(-1, 1)\n",
    "\n",
    "all_preds_seconds = scaler_time.inverse_transform(all_preds_flat).flatten()\n",
    "all_true_seconds = scaler_time.inverse_transform(all_true_flat).flatten()\n",
    "mask_flat = all_masks_flat.flatten().astype(bool)\n",
    "\n",
    "# Apply mask\n",
    "y_pred_masked = all_preds_seconds[mask_flat]\n",
    "y_true_masked = all_true_seconds[mask_flat]\n",
    "\n",
    "# Unified evaluation\n",
    "result_stgnn = utils.evaluate_model(\n",
    "    y_true_masked, y_pred_masked,\n",
    "    model_name=\"ST-GNN\",\n",
    "    config={\n",
    "        \"hidden_channels\": 32,\n",
    "        \"num_heads\": 2,\n",
    "        \"window_size\": window_size,\n",
    "        \"scaler\": \"StandardScaler\"\n",
    "    }\n",
    ")\n",
    "evaluation_results.append(result_stgnn)\n",
    "print(result_stgnn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gwqnsd52j4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Comparison Table and Save Results\n",
    "utils.display_and_save_results(evaluation_results, 'data/evaluation_results_stgnn.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}