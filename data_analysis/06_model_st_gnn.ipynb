{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bdcded9",
   "metadata": {},
   "source": [
    "# 06. Model ST-GNN\n",
    "ST-GNN model implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c03702",
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom sklearn.preprocessing import StandardScaler\nimport utils\nimport os\n\n# ST-GNN specific data loading\ndef load_real_translink_data(\n    csv_path='data/processed_data/processed_trip_data.csv',\n    stops_txt_path='data/google_transit/stops.txt',\n    time_bin='15min'\n):\n    print(f\"Loading data from {csv_path}...\")\n    if os.path.exists(csv_path):\n        df = pd.read_csv(csv_path)\n    else:\n        print(\"Processed file not found. Please run 02_process_data.ipynb.\")\n        return None, None, None, None, None, None\n\n    # Filter routes (if needed, though processed data might already be filtered or contain all)\n    target_routes = [6641, 6636, 37810, 6622, 6705, 6627, 16718, 6624, 37807, 6617]\n    if 'route_id' in df.columns:\n        df = df[df['route_id'].isin(target_routes)]\n    \n    if 'actual_arrival_time' in df.columns:\n        df['actual_arrival_time'] = pd.to_datetime(df['actual_arrival_time'], utc=True)\n    \n    # Skip prepare_trip_data as it is already processed\n    # df = utils.prepare_trip_data(df)\n    \n    if 'arrival_delay_agg' in df.columns:\n        df['arrival_delay'] = df['arrival_delay_agg']\n        \n    if 'scheduled_arrival_time' in df.columns:\n        df['scheduled_arrival_time'] = pd.to_datetime(df['scheduled_arrival_time'], utc=True)\n        # Recalculate actual_arrival_time if needed or ensure it exists\n        if 'actual_arrival_time' not in df.columns:\n             df['actual_arrival_time'] = df['scheduled_arrival_time'] + pd.to_timedelta(df['arrival_delay'], unit='s')\n            \n    unique_stops = df['stop_id'].unique()\n    unique_stops.sort()\n    stop_to_idx = {stop_id: i for i, stop_id in enumerate(unique_stops)}\n    num_stops = len(unique_stops)\n    \n    # Stops meta\n    try:\n        stops_meta = pd.read_csv(stops_txt_path)\n        stops_meta['stop_id'] = stops_meta['stop_id'].astype(str)\n        unique_stops_str = [str(s) for s in unique_stops]\n        stops_meta = stops_meta[stops_meta['stop_id'].isin(unique_stops_str)]\n        stops_df = pd.DataFrame({'stop_id': unique_stops_str})\n        stops_df = stops_df.merge(stops_meta[['stop_id', 'stop_lat', 'stop_lon']], on='stop_id', how='left')\n    except:\n        stops_df = pd.DataFrame({'stop_id': unique_stops})\n\n    # Edges\n    df = df.sort_values(['trip_id', 'actual_arrival_time'])\n    df['next_trip_id'] = df['trip_id'].shift(-1)\n    df['next_stop_id'] = df['stop_id'].shift(-1)\n    \n    valid_connections = df[df['trip_id'] == df['next_trip_id']]\n    edges = set()\n    for _, row in valid_connections.iterrows():\n        src = stop_to_idx[row['stop_id']]\n        dst = stop_to_idx[row['next_stop_id']]\n        if src != dst:\n            edges.add((src, dst))\n    edge_list = list(edges)\n    \n    # Time series features\n    df.set_index('actual_arrival_time', inplace=True)\n    travel_time_matrix = df.pivot_table(index=pd.Grouper(freq=time_bin), columns='stop_id', values='arrival_delay', aggfunc='mean')\n    travel_time_matrix = travel_time_matrix.reindex(columns=unique_stops).ffill(limit=4).fillna(0)\n    \n    travel_time_np = travel_time_matrix.values\n    scaler_time = StandardScaler()\n    travel_time_np = scaler_time.fit_transform(travel_time_np.reshape(-1, 1)).reshape(travel_time_np.shape)\n    \n    # Simple features for demo\n    node_features = []\n    targets = []\n    num_timesteps = len(travel_time_matrix)\n    \n    for t in range(num_timesteps - 1):\n        features_t = np.stack([travel_time_np[t]], axis=1) # Simplified\n        node_features.append(features_t)\n        targets.append(travel_time_np[t+1])\n        \n    valid_sources = set(src for src, dst in edge_list)\n    node_mask = np.array([1.0 if i in valid_sources else 0.0 for i in range(num_stops)], dtype=np.float32)\n    \n    return stops_df, edge_list, np.array(node_features, dtype=np.float32), np.array(targets, dtype=np.float32), scaler_time, node_mask\n\nstops_df, edge_list, X_all, y_all, scaler_time, node_mask = load_real_translink_data()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c654f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAT Layer & ST-GNN Model\n",
    "class GATLayer(layers.Layer):\n",
    "    def __init__(self, units, heads=1, concat=True, **kwargs):\n",
    "        super(GATLayer, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.heads = heads\n",
    "        self.concat = concat\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        feat_dim = input_shape[0][-1]\n",
    "        self.W = self.add_weight(shape=(feat_dim, self.units * self.heads), initializer='glorot_uniform', trainable=True)\n",
    "        self.a = self.add_weight(shape=(1, self.heads, 2 * self.units), initializer='glorot_uniform', trainable=True)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        h, adj = inputs\n",
    "        batch_size = tf.shape(h)[0]\n",
    "        num_nodes = tf.shape(h)[1]\n",
    "        \n",
    "        h_prime = tf.matmul(h, self.W)\n",
    "        h_prime = tf.reshape(h_prime, (batch_size, num_nodes, self.heads, self.units))\n",
    "        \n",
    "        a1 = self.a[:, :, :self.units]\n",
    "        a2 = self.a[:, :, self.units:]\n",
    "        \n",
    "        score_i = tf.reduce_sum(h_prime * a1, axis=-1)\n",
    "        score_j = tf.reduce_sum(h_prime * a2, axis=-1)\n",
    "        \n",
    "        score_i = tf.expand_dims(score_i, 2)\n",
    "        score_j = tf.expand_dims(score_j, 1)\n",
    "        \n",
    "        e = tf.nn.leaky_relu(score_i + score_j)\n",
    "        \n",
    "        mask = tf.cast(adj, dtype=tf.bool)\n",
    "        mask = tf.expand_dims(mask, 0)\n",
    "        mask = tf.expand_dims(mask, -1)\n",
    "        \n",
    "        zero_vec = -9e15 * tf.ones_like(e)\n",
    "        attention = tf.where(mask, e, zero_vec)\n",
    "        attention = tf.nn.softmax(attention, axis=2)\n",
    "        \n",
    "        out = tf.einsum('bijh,bjhu->bihu', attention, h_prime)\n",
    "        \n",
    "        if self.concat:\n",
    "            out = tf.reshape(out, (batch_size, num_nodes, self.heads * self.units))\n",
    "        else:\n",
    "            out = tf.reduce_mean(out, axis=2)\n",
    "        return out\n",
    "\n",
    "class ST_GNN(models.Model):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_nodes, num_heads=2):\n",
    "        super(ST_GNN, self).__init__()\n",
    "        self.gat1 = GATLayer(hidden_channels, heads=num_heads, concat=True)\n",
    "        self.gat2 = GATLayer(hidden_channels, heads=1, concat=False)\n",
    "        self.lstm = layers.LSTM(hidden_channels, return_sequences=False)\n",
    "        self.fc = layers.Dense(out_channels)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x, adj = inputs\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        window_size = tf.shape(x)[1]\n",
    "        num_nodes = tf.shape(x)[2]\n",
    "        features = tf.shape(x)[3]\n",
    "        \n",
    "        x_reshaped = tf.reshape(x, (batch_size * window_size, num_nodes, features))\n",
    "        h = self.gat1([x_reshaped, adj])\n",
    "        h = tf.nn.elu(h)\n",
    "        h = self.gat2([h, adj])\n",
    "        h = tf.nn.elu(h)\n",
    "        \n",
    "        h = tf.reshape(h, (batch_size, window_size, num_nodes, -1))\n",
    "        h = tf.transpose(h, perm=[0, 2, 1, 3])\n",
    "        h_reshaped = tf.reshape(h, (batch_size * num_nodes, window_size, -1))\n",
    "        \n",
    "        lstm_out = self.lstm(h_reshaped)\n",
    "        out = self.fc(lstm_out)\n",
    "        out = tf.reshape(out, (batch_size, num_nodes, -1))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abcd453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Data and Train\n",
    "if X_all is not None:\n",
    "    def create_dataset(edge_list, node_features, targets, window_size=12):\n",
    "        num_timesteps = len(node_features)\n",
    "        num_nodes = node_features.shape[1]\n",
    "        X, Y = [], []\n",
    "        for i in range(window_size, num_timesteps):\n",
    "            X.append(node_features[i-window_size:i])\n",
    "            Y.append(targets[i].reshape(num_nodes, 1))\n",
    "        \n",
    "        adj = np.eye(num_nodes, dtype=np.float32)\n",
    "        for src, dst in edge_list:\n",
    "            adj[dst, src] = 1.0\n",
    "        return np.array(X, dtype=np.float32), np.array(Y, dtype=np.float32), adj\n",
    "\n",
    "    window_size = 6\n",
    "    X_data, Y_data, adj_matrix = create_dataset(edge_list, X_all, y_all, window_size)\n",
    "    \n",
    "    split_idx = int(len(X_data) * 0.8)\n",
    "    X_train, X_test = X_data[:split_idx], X_data[split_idx:]\n",
    "    Y_train, Y_test = Y_data[:split_idx], Y_data[split_idx:]\n",
    "    \n",
    "    model = ST_GNN(in_channels=X_train.shape[3], hidden_channels=32, out_channels=1, num_nodes=X_train.shape[2])\n",
    "    \n",
    "    print(\"Training ST-GNN...\")\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "    \n",
    "    # Simplified training loop\n",
    "    for epoch in range(5):\n",
    "        with tf.GradientTape() as tape:\n",
    "            preds = model([X_train[:32], adj_matrix]) # Small batch for demo\n",
    "            loss = tf.reduce_mean(tf.square(Y_train[:32] - preds))\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.numpy():.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}