{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4a2fef6",
   "metadata": {},
   "source": [
    "# 02. Process Data\n",
    "Weather processing, merge, feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b40caa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import utils\n",
    "\n",
    "# Load Data\n",
    "base_df = utils.load_data()\n",
    "if base_df is not None:\n",
    "    base_df_filtered = base_df[(base_df['arrival_delay'] >= -600) & (base_df['arrival_delay'] <= 1800)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20373a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf98a50",
   "metadata": {},
   "source": [
    "同じ日のtripでも2分以上最寄りのバス停が同じパターンが存在する。始発の停車、長距離移動、終点など。加工して１つに制限する必要がある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c29164",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = base_df.groupby(['start_date', 'trip_id', 'stop_sequence']).size()\n",
    "counts[counts > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7051b9",
   "metadata": {},
   "source": [
    "同じ日のtripで2分間で複数のバス停を通過するとその値は穴あきになる。それを調査する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e86097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_sequenceの連番に穴あきがあるかチェック\n",
    "def check_sequence_gaps(df):\n",
    "    \"\"\"各trip（日付+trip_id）ごとにstop_sequenceの欠番をチェック\"\"\"\n",
    "    gaps = []\n",
    "    for (date, trip_id), group in df.groupby(['start_date', 'trip_id']):\n",
    "        seqs = sorted(group['stop_sequence'].unique())\n",
    "        expected = list(range(min(seqs), max(seqs) + 1))\n",
    "        missing = set(expected) - set(seqs)\n",
    "        if missing:\n",
    "            gaps.append({\n",
    "                'start_date': date,\n",
    "                'trip_id': trip_id,\n",
    "                'min_seq': min(seqs),\n",
    "                'max_seq': max(seqs),\n",
    "                'missing_seqs': sorted(missing),\n",
    "                'gap_count': len(missing)\n",
    "            })\n",
    "    return pd.DataFrame(gaps)\n",
    "\n",
    "gaps_df = check_sequence_gaps(base_df)\n",
    "print(f\"穴あきがあるtrip数: {len(gaps_df)}\")\n",
    "gaps_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df45244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Trip Data (Aggregation)\n",
    "def prepare_trip_data(df, direction_id=None):\n",
    "    \"\"\"\n",
    "    Trip単位でデータを整理\n",
    "    Args:\n",
    "        df: 入力データフレーム\n",
    "        direction_id: 方向ID（None=全方向、特定値で絞り込み）\n",
    "    \"\"\"\n",
    "    if direction_id is not None:\n",
    "        data = df[df['direction_id'] == direction_id].copy()\n",
    "    else:\n",
    "        data = df.copy()\n",
    "\n",
    "    # route_id + direction_id + trip_id でユニークなtrip_keyを作成\n",
    "    data['trip_key'] = (\n",
    "        data['start_date'].astype(str) + '_' +\n",
    "        data['route_id'].astype(str) + '_' +\n",
    "        data['direction_id'].astype(str) + '_' +\n",
    "        data['trip_id'].astype(str)\n",
    "    )\n",
    "\n",
    "    # route_direction_key（route_id + direction_idの組み合わせ）\n",
    "    data['route_direction_key'] = (\n",
    "        data['route_id'].astype(str) + '_' +\n",
    "        data['direction_id'].astype(str)\n",
    "    )\n",
    "\n",
    "    # バス停タイプの判定\n",
    "    trip_seq_stats = data.groupby('trip_key')['stop_sequence'].agg(['min', 'max'])\n",
    "    trip_seq_stats.columns = ['seq_min', 'seq_max']\n",
    "    data = data.merge(trip_seq_stats, on='trip_key', how='left')\n",
    "\n",
    "    data['stop_type'] = 'middle'\n",
    "    data.loc[data['stop_sequence'] == data['seq_min'], 'stop_type'] = 'first'\n",
    "    data.loc[data['stop_sequence'] == data['seq_max'], 'stop_type'] = 'last'\n",
    "\n",
    "    # 集計\n",
    "    group_cols = ['trip_key', 'stop_sequence']\n",
    "    first_stops = data[data['stop_type'] == 'first'].groupby(group_cols)['arrival_delay'].max().reset_index()\n",
    "    first_stops.columns = ['trip_key', 'stop_sequence', 'arrival_delay_agg']\n",
    "    last_stops = data[data['stop_type'] == 'last'].groupby(group_cols)['arrival_delay'].min().reset_index()\n",
    "    last_stops.columns = ['trip_key', 'stop_sequence', 'arrival_delay_agg']\n",
    "    middle_stops = data[data['stop_type'] == 'middle'].groupby(group_cols)['arrival_delay'].first().reset_index()\n",
    "    middle_stops.columns = ['trip_key', 'stop_sequence', 'arrival_delay_agg']\n",
    "\n",
    "    agg_delays = pd.concat([first_stops, middle_stops, last_stops])\n",
    "\n",
    "    exclude_cols = ['arrival_delay', 'stop_type', 'seq_min', 'seq_max']\n",
    "    other_cols = [c for c in data.columns if c not in exclude_cols and c not in group_cols]\n",
    "    data_unique = data.groupby(group_cols, as_index=False)[other_cols].first()\n",
    "    data_unique = data_unique.merge(agg_delays, on=['trip_key', 'stop_sequence'], how='left')\n",
    "    data_unique = data_unique.sort_values(['trip_key', 'stop_sequence'])\n",
    "\n",
    "    # 欠損値を線形補間\n",
    "    data_unique['arrival_delay_agg'] = data_unique.groupby('trip_key')['arrival_delay_agg'].transform(\n",
    "        lambda x: x.interpolate(method='linear', limit_direction='both')\n",
    "    )\n",
    "    \n",
    "    # float32に変換\n",
    "    data_unique['arrival_delay_agg'] = data_unique['arrival_delay_agg'].astype('float32')\n",
    "\n",
    "    return data_unique\n",
    "\n",
    "if base_df is not None:\n",
    "    print(\"Preparing trip data...\")\n",
    "    df_process = prepare_trip_data(base_df_filtered, direction_id=None)\n",
    "    print(f\"Processed trips: {df_process['trip_key'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51922a3a",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d04685f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_features(df_process):\n",
    "    \"\"\"Apply feature engineering\"\"\"\n",
    "    scheduled_time = pd.to_datetime(df_process['scheduled_arrival_time'], utc=True)\n",
    "    df_process['time_of_day'] = scheduled_time.dt.hour + scheduled_time.dt.minute / 60\n",
    "    df_process['hour'] = scheduled_time.dt.hour\n",
    "    df_process['time_sin'] = np.sin(2 * np.pi * df_process['time_of_day'] / 24)\n",
    "    df_process['time_cos'] = np.cos(2 * np.pi * df_process['time_of_day'] / 24)\n",
    "    df_process['day_of_week'] = pd.to_datetime(df_process['start_date'], format='%Y%m%d').dt.dayofweek\n",
    "    df_process['is_weekend'] = (df_process['day_of_week'] >= 6).astype(int)\n",
    "\n",
    "    # v2 features\n",
    "    df_process['is_rush_hour'] = ((df_process['hour'] >= 14) & (df_process['hour'] <= 18)).astype(int)\n",
    "\n",
    "    if 'alert_effect_detour' in df_process.columns:\n",
    "        df_process['has_detour'] = (df_process['alert_effect_detour'] > 0).astype(int)\n",
    "    else:\n",
    "        df_process['has_detour'] = 0\n",
    "\n",
    "    if 'alert_police_activity' in df_process.columns:\n",
    "        df_process['has_police_alert'] = (df_process['alert_police_activity'] > 0).astype(int)\n",
    "    else:\n",
    "        df_process['has_police_alert'] = 0\n",
    "\n",
    "    rd_encoder = LabelEncoder()\n",
    "    df_process['route_direction_encoded'] = rd_encoder.fit_transform(df_process['route_direction_key'])\n",
    "    \n",
    "    return df_process\n",
    "\n",
    "if base_df is not None:\n",
    "    print(\"Applying feature engineering...\")\n",
    "    df_process = process_features(df_process)\n",
    "\n",
    "    print(\"New features created:\")\n",
    "    print(f\"  is_rush_hour: {df_process['is_rush_hour'].mean():.1%}\")\n",
    "    print(f\"  has_detour: {df_process['has_detour'].mean():.1%}\")\n",
    "    print(f\"  has_police_alert: {df_process['has_police_alert'].mean():.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27eb8ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_process.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f29572",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_process_selected = df_process[[\n",
    "    'trip_key', 'stop_sequence', 'route_id', 'trip_id', 'start_date', 'direction_id', 'stop_id', 'region_id',\n",
    "       'scheduled_arrival_time', 'actual_arrival_time', 'time_bucket',\n",
    "       'hour_of_day', 'day_of_week', 'has_active_alert',\n",
    "       'route_direction_key', 'arrival_delay_agg',\n",
    "       'time_of_day', 'hour', 'time_sin', 'time_cos', 'is_weekend',\n",
    "       'is_rush_hour', 'has_detour', 'has_police_alert',\n",
    "       'route_direction_encoded'\n",
    "    ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5efc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "if base_df is not None:\n",
    "    output_file = 'data/processed_data/processed_trip_data.csv'\n",
    "    df_process_selected.to_csv(output_file, index=False)\n",
    "    print(f\"Saved processed data to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
