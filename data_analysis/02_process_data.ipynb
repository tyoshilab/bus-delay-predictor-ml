{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4a2fef6",
   "metadata": {},
   "source": [
    "# 02. Process Data\n",
    "Weather processing, merge, feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b40caa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import utils\n",
    "\n",
    "# Load Data\n",
    "base_df = utils.load_data()\n",
    "if base_df is not None:\n",
    "    base_df_filtered = base_df[(base_df['arrival_delay'] >= -600) & (base_df['arrival_delay'] <= 1800)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf98a50",
   "metadata": {},
   "source": [
    "同じ日のtripでも2分以上最寄りのバス停が同じパターンが存在する。始発の停車、長距離移動、終点など。加工して１つに制限する必要がある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0c29164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts = base_df.groupby(['start_date', 'trip_id', 'stop_sequence']).size()\n",
    "# counts[counts > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7051b9",
   "metadata": {},
   "source": [
    "同じ日のtripで2分間で複数のバス停を通過するとその値は穴あきになる。それを調査する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14e86097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_sequenceの連番に穴あきがあるかチェック\n",
    "def check_sequence_gaps(df):\n",
    "    \"\"\"各trip（日付+trip_id）ごとにstop_sequenceの欠番をチェック\"\"\"\n",
    "    gaps = []\n",
    "    for (date, trip_id), group in df.groupby(['start_date', 'trip_id']):\n",
    "        seqs = sorted(group['stop_sequence'].unique())\n",
    "        expected = list(range(min(seqs), max(seqs) + 1))\n",
    "        missing = set(expected) - set(seqs)\n",
    "        if missing:\n",
    "            gaps.append({\n",
    "                'start_date': date,\n",
    "                'trip_id': trip_id,\n",
    "                'min_seq': min(seqs),\n",
    "                'max_seq': max(seqs),\n",
    "                'missing_seqs': sorted(missing),\n",
    "                'gap_count': len(missing)\n",
    "            })\n",
    "    return pd.DataFrame(gaps)\n",
    "\n",
    "# gaps_df = check_sequence_gaps(base_df)\n",
    "# print(f\"穴あきがあるtrip数: {len(gaps_df)}\")\n",
    "# gaps_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8df45244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Released base_df memory.\n",
      "Preparing trip data...\n",
      "Processed trips: 453676\n"
     ]
    }
   ],
   "source": [
    "# Prepare Trip Data (Aggregation)\n",
    "def prepare_trip_data(df, direction_id=None):\n",
    "    \"\"\"\n",
    "    Trip単位でデータを整理\n",
    "    Args:\n",
    "        df: 入力データフレーム\n",
    "        direction_id: 方向ID（None=全方向、特定値で絞り込み）\n",
    "    \"\"\"\n",
    "    if direction_id is not None:\n",
    "        data = df[df['direction_id'] == direction_id].copy()\n",
    "    else:\n",
    "        data = df.copy()\n",
    "\n",
    "    # route_id + direction_id + trip_id でユニークなtrip_keyを作成\n",
    "    data['trip_key'] = (\n",
    "        data['start_date'].astype(str) + '_' +\n",
    "        data['route_id'].astype(str) + '_' +\n",
    "        data['direction_id'].astype(str) + '_' +\n",
    "        data['trip_id'].astype(str)\n",
    "    )\n",
    "\n",
    "    # route_direction_key（route_id + direction_idの組み合わせ）\n",
    "    data['route_direction_key'] = (\n",
    "        data['route_id'].astype(str) + '_' +\n",
    "        data['direction_id'].astype(str)\n",
    "    )\n",
    "\n",
    "    # バス停タイプの判定\n",
    "    trip_seq_stats = data.groupby('trip_key')['stop_sequence'].agg(['min', 'max'])\n",
    "    trip_seq_stats.columns = ['seq_min', 'seq_max']\n",
    "    data = data.merge(trip_seq_stats, on='trip_key', how='left')\n",
    "\n",
    "    data['stop_type'] = 'middle'\n",
    "    data.loc[data['stop_sequence'] == data['seq_min'], 'stop_type'] = 'first'\n",
    "    data.loc[data['stop_sequence'] == data['seq_max'], 'stop_type'] = 'last'\n",
    "\n",
    "    # 集計\n",
    "    group_cols = ['trip_key', 'stop_sequence']\n",
    "    first_stops = data[data['stop_type'] == 'first'].groupby(group_cols)['arrival_delay'].max().reset_index()\n",
    "    first_stops.columns = ['trip_key', 'stop_sequence', 'arrival_delay_agg']\n",
    "    last_stops = data[data['stop_type'] == 'last'].groupby(group_cols)['arrival_delay'].min().reset_index()\n",
    "    last_stops.columns = ['trip_key', 'stop_sequence', 'arrival_delay_agg']\n",
    "    middle_stops = data[data['stop_type'] == 'middle'].groupby(group_cols)['arrival_delay'].first().reset_index()\n",
    "    middle_stops.columns = ['trip_key', 'stop_sequence', 'arrival_delay_agg']\n",
    "\n",
    "    agg_delays = pd.concat([first_stops, middle_stops, last_stops])\n",
    "\n",
    "    exclude_cols = ['arrival_delay', 'stop_type', 'seq_min', 'seq_max']\n",
    "    other_cols = [c for c in data.columns if c not in exclude_cols and c not in group_cols]\n",
    "    data_unique = data.groupby(group_cols, as_index=False)[other_cols].first()\n",
    "    data_unique = data_unique.merge(agg_delays, on=['trip_key', 'stop_sequence'], how='left')\n",
    "    data_unique = data_unique.sort_values(['trip_key', 'stop_sequence'])\n",
    "\n",
    "    # 欠損値を線形補間\n",
    "    data_unique['arrival_delay_agg'] = data_unique.groupby('trip_key')['arrival_delay_agg'].transform(\n",
    "        lambda x: x.interpolate(method='linear', limit_direction='both')\n",
    "    )\n",
    "    \n",
    "    # float32に変換\n",
    "    data_unique['arrival_delay_agg'] = data_unique['arrival_delay_agg'].astype('float32')\n",
    "\n",
    "    return data_unique\n",
    "\n",
    "if 'base_df_filtered' in locals() and base_df_filtered is not None:\n",
    "    # メモリ節約のため、使用済みのbase_dfを削除\n",
    "    if 'base_df' in locals():\n",
    "        del base_df\n",
    "        import gc\n",
    "        gc.collect()\n",
    "        print(\"Released base_df memory.\")\n",
    "\n",
    "    print(\"Preparing trip data...\")\n",
    "    df_process = prepare_trip_data(base_df_filtered, direction_id=None)\n",
    "    print(f\"Processed trips: {df_process['trip_key'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51922a3a",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d04685f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying feature engineering...\n",
      "New features created:\n",
      "  is_rush_hour: 26.3%\n",
      "  has_detour: 0.5%\n",
      "  has_police_alert: 1.1%\n"
     ]
    }
   ],
   "source": [
    "def process_features(df_process):\n",
    "    \"\"\"Apply feature engineering\"\"\"\n",
    "    scheduled_time = pd.to_datetime(df_process['scheduled_arrival_time'], utc=True)\n",
    "    df_process['time_of_day'] = scheduled_time.dt.hour + scheduled_time.dt.minute / 60\n",
    "    df_process['hour'] = scheduled_time.dt.hour\n",
    "    df_process['time_sin'] = np.sin(2 * np.pi * df_process['time_of_day'] / 24)\n",
    "    df_process['time_cos'] = np.cos(2 * np.pi * df_process['time_of_day'] / 24)\n",
    "    df_process['day_of_week'] = pd.to_datetime(df_process['start_date'], format='%Y%m%d').dt.dayofweek\n",
    "    df_process['is_weekend'] = (df_process['day_of_week'] >= 6).astype(int)\n",
    "\n",
    "    # v2 features\n",
    "    df_process['is_rush_hour'] = ((df_process['hour'] >= 14) & (df_process['hour'] <= 18)).astype(int)\n",
    "\n",
    "    if 'alert_effect_detour' in df_process.columns:\n",
    "        df_process['has_detour'] = (df_process['alert_effect_detour'] > 0).astype(int)\n",
    "    else:\n",
    "        df_process['has_detour'] = 0\n",
    "\n",
    "    if 'alert_police_activity' in df_process.columns:\n",
    "        df_process['has_police_alert'] = (df_process['alert_police_activity'] > 0).astype(int)\n",
    "    else:\n",
    "        df_process['has_police_alert'] = 0\n",
    "\n",
    "    rd_encoder = LabelEncoder()\n",
    "    df_process['route_direction_encoded'] = rd_encoder.fit_transform(df_process['route_direction_key'])\n",
    "    \n",
    "    return df_process\n",
    "\n",
    "if 'df_process' in locals() and df_process is not None:\n",
    "    print(\"Applying feature engineering...\")\n",
    "    df_process = process_features(df_process)\n",
    "\n",
    "    print(\"New features created:\")\n",
    "    print(f\"  is_rush_hour: {df_process['is_rush_hour'].mean():.1%}\")\n",
    "    print(f\"  has_detour: {df_process['has_detour'].mean():.1%}\")\n",
    "    print(f\"  has_police_alert: {df_process['has_police_alert'].mean():.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27eb8ec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "trip_key",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "stop_sequence",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "route_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "trip_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "start_date",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "arrival_day_offset",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "direction_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "stop_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "region_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "lat_sin",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "lat_cos",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "lon_sin",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "lon_cos",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "distance_from_downtown_km",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "scheduled_arrival_time",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "actual_arrival_time",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "time_bucket",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "hour_of_day",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "day_of_week",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "has_active_alert",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "high_impact_alert_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "alert_police_activity",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "alert_construction",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "alert_technical_problem",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "alert_effect_no_service",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "alert_effect_detour",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "alert_severity_score",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "route_direction_key",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "arrival_delay_agg",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "time_of_day",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "hour",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "time_sin",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "time_cos",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "is_weekend",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "is_rush_hour",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "has_detour",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "has_police_alert",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "route_direction_encoded",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "cfd9db20-65cc-4445-8623-9450585d83b8",
       "rows": [
        [
         "0",
         "20251203_10232_0_14895984",
         "1",
         "10232",
         "14895984",
         "20251203",
         "0",
         "0",
         "10947",
         "west_vancouver",
         "0.758439",
         "0.651744",
         "-0.837347",
         "-0.546672",
         "5.08",
         "2025-12-03 14:05:00+00:00",
         "2025-12-03 14:07:19+00:00",
         "2025-12-03 14:00:00+00:00",
         "6",
         "2",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "10232_0",
         "139.0",
         "14.083333333333334",
         "14",
         "-0.5187732581605216",
         "-0.8549118706729465",
         "0",
         "1",
         "0",
         "0",
         "0"
        ],
        [
         "1",
         "20251203_10232_0_14895984",
         "2",
         "10232",
         "14895984",
         "20251203",
         "0",
         "0",
         "4782",
         "west_vancouver",
         "0.758494",
         "0.65168",
         "-0.837387",
         "-0.54661",
         "5.53",
         "2025-12-03 14:07:23+00:00",
         "2025-12-03 14:09:30+00:00",
         "2025-12-03 14:00:00+00:00",
         "6",
         "2",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "10232_0",
         "127.0",
         "14.116666666666667",
         "14",
         "-0.5262139236518696",
         "-0.8503522249955628",
         "0",
         "1",
         "0",
         "0",
         "0"
        ],
        [
         "2",
         "20251203_10232_0_14895984",
         "3",
         "10232",
         "14895984",
         "20251203",
         "0",
         "0",
         "12883",
         "west_vancouver",
         "0.758526",
         "0.651643",
         "-0.837394",
         "-0.5466",
         "5.83",
         "2025-12-03 14:08:27+00:00",
         "2025-12-03 14:11:12+00:00",
         "2025-12-03 14:00:00+00:00",
         "6",
         "2",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "10232_0",
         "165.0",
         "14.133333333333333",
         "14",
         "-0.5299192642332043",
         "-0.8480480961564263",
         "0",
         "1",
         "0",
         "0",
         "0"
        ],
        [
         "3",
         "20251203_10232_0_14895984",
         "4",
         "10232",
         "14895984",
         "20251203",
         "0",
         "0",
         "11118",
         "west_vancouver",
         "0.758502",
         "0.65167",
         "-0.837523",
         "-0.546401",
         "5.52",
         "2025-12-03 14:13:00+00:00",
         "2025-12-03 14:15:15+00:00",
         "2025-12-03 14:00:00+00:00",
         "6",
         "2",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "10232_0",
         "135.0",
         "14.216666666666667",
         "14",
         "-0.548293229519914",
         "-0.8362861558477594",
         "0",
         "1",
         "0",
         "0",
         "0"
        ],
        [
         "4",
         "20251203_10232_0_14895984",
         "5",
         "10232",
         "14895984",
         "20251203",
         "0",
         "0",
         "4491",
         "west_vancouver",
         "0.758443",
         "0.65174",
         "-0.837348",
         "-0.546671",
         "5.11",
         "2025-12-03 14:21:00+00:00",
         "2025-12-03 14:26:14+00:00",
         "2025-12-03 14:00:00+00:00",
         "6",
         "2",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "10232_0",
         "314.0",
         "14.35",
         "14",
         "-0.5771451900372334",
         "-0.816641555161679",
         "0",
         "1",
         "0",
         "0",
         "0"
        ]
       ],
       "shape": {
        "columns": 38,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_key</th>\n",
       "      <th>stop_sequence</th>\n",
       "      <th>route_id</th>\n",
       "      <th>trip_id</th>\n",
       "      <th>start_date</th>\n",
       "      <th>arrival_day_offset</th>\n",
       "      <th>direction_id</th>\n",
       "      <th>stop_id</th>\n",
       "      <th>region_id</th>\n",
       "      <th>lat_sin</th>\n",
       "      <th>...</th>\n",
       "      <th>arrival_delay_agg</th>\n",
       "      <th>time_of_day</th>\n",
       "      <th>hour</th>\n",
       "      <th>time_sin</th>\n",
       "      <th>time_cos</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>is_rush_hour</th>\n",
       "      <th>has_detour</th>\n",
       "      <th>has_police_alert</th>\n",
       "      <th>route_direction_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20251203_10232_0_14895984</td>\n",
       "      <td>1</td>\n",
       "      <td>10232</td>\n",
       "      <td>14895984</td>\n",
       "      <td>20251203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10947</td>\n",
       "      <td>west_vancouver</td>\n",
       "      <td>0.758439</td>\n",
       "      <td>...</td>\n",
       "      <td>139.0</td>\n",
       "      <td>14.083333</td>\n",
       "      <td>14</td>\n",
       "      <td>-0.518773</td>\n",
       "      <td>-0.854912</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20251203_10232_0_14895984</td>\n",
       "      <td>2</td>\n",
       "      <td>10232</td>\n",
       "      <td>14895984</td>\n",
       "      <td>20251203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4782</td>\n",
       "      <td>west_vancouver</td>\n",
       "      <td>0.758494</td>\n",
       "      <td>...</td>\n",
       "      <td>127.0</td>\n",
       "      <td>14.116667</td>\n",
       "      <td>14</td>\n",
       "      <td>-0.526214</td>\n",
       "      <td>-0.850352</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20251203_10232_0_14895984</td>\n",
       "      <td>3</td>\n",
       "      <td>10232</td>\n",
       "      <td>14895984</td>\n",
       "      <td>20251203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12883</td>\n",
       "      <td>west_vancouver</td>\n",
       "      <td>0.758526</td>\n",
       "      <td>...</td>\n",
       "      <td>165.0</td>\n",
       "      <td>14.133333</td>\n",
       "      <td>14</td>\n",
       "      <td>-0.529919</td>\n",
       "      <td>-0.848048</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20251203_10232_0_14895984</td>\n",
       "      <td>4</td>\n",
       "      <td>10232</td>\n",
       "      <td>14895984</td>\n",
       "      <td>20251203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11118</td>\n",
       "      <td>west_vancouver</td>\n",
       "      <td>0.758502</td>\n",
       "      <td>...</td>\n",
       "      <td>135.0</td>\n",
       "      <td>14.216667</td>\n",
       "      <td>14</td>\n",
       "      <td>-0.548293</td>\n",
       "      <td>-0.836286</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20251203_10232_0_14895984</td>\n",
       "      <td>5</td>\n",
       "      <td>10232</td>\n",
       "      <td>14895984</td>\n",
       "      <td>20251203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4491</td>\n",
       "      <td>west_vancouver</td>\n",
       "      <td>0.758443</td>\n",
       "      <td>...</td>\n",
       "      <td>314.0</td>\n",
       "      <td>14.350000</td>\n",
       "      <td>14</td>\n",
       "      <td>-0.577145</td>\n",
       "      <td>-0.816642</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    trip_key  stop_sequence  route_id   trip_id start_date  \\\n",
       "0  20251203_10232_0_14895984              1     10232  14895984   20251203   \n",
       "1  20251203_10232_0_14895984              2     10232  14895984   20251203   \n",
       "2  20251203_10232_0_14895984              3     10232  14895984   20251203   \n",
       "3  20251203_10232_0_14895984              4     10232  14895984   20251203   \n",
       "4  20251203_10232_0_14895984              5     10232  14895984   20251203   \n",
       "\n",
       "   arrival_day_offset  direction_id  stop_id       region_id   lat_sin  ...  \\\n",
       "0                   0             0    10947  west_vancouver  0.758439  ...   \n",
       "1                   0             0     4782  west_vancouver  0.758494  ...   \n",
       "2                   0             0    12883  west_vancouver  0.758526  ...   \n",
       "3                   0             0    11118  west_vancouver  0.758502  ...   \n",
       "4                   0             0     4491  west_vancouver  0.758443  ...   \n",
       "\n",
       "   arrival_delay_agg  time_of_day  hour  time_sin  time_cos is_weekend  \\\n",
       "0              139.0    14.083333    14 -0.518773 -0.854912          0   \n",
       "1              127.0    14.116667    14 -0.526214 -0.850352          0   \n",
       "2              165.0    14.133333    14 -0.529919 -0.848048          0   \n",
       "3              135.0    14.216667    14 -0.548293 -0.836286          0   \n",
       "4              314.0    14.350000    14 -0.577145 -0.816642          0   \n",
       "\n",
       "  is_rush_hour  has_detour  has_police_alert  route_direction_encoded  \n",
       "0            1           0                 0                        0  \n",
       "1            1           0                 0                        0  \n",
       "2            1           0                 0                        0  \n",
       "3            1           0                 0                        0  \n",
       "4            1           0                 0                        0  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_process.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49f29572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 25 columns\n",
      "  - Base features: 25\n",
      "  - Feature Store features: 0\n"
     ]
    }
   ],
   "source": [
    "# Select columns including Feature Store features\n",
    "base_cols = [\n",
    "    'trip_key', 'stop_sequence', 'route_id', 'trip_id', 'start_date', 'direction_id', 'stop_id', 'region_id',\n",
    "    'scheduled_arrival_time', 'actual_arrival_time', 'time_bucket',\n",
    "    'hour_of_day', 'day_of_week', 'has_active_alert',\n",
    "    'route_direction_key', 'arrival_delay_agg',\n",
    "    'time_of_day', 'hour', 'time_sin', 'time_cos', 'is_weekend',\n",
    "    'is_rush_hour', 'has_detour', 'has_police_alert',\n",
    "    'route_direction_encoded'\n",
    "]\n",
    "\n",
    "# Feature Store columns\n",
    "feature_store_cols = [\n",
    "    # Lag features\n",
    "    'delay_lag_1', 'delay_lag_2', 'delay_lag_3', 'delay_lag_5',\n",
    "    # Rolling features\n",
    "    'delay_rolling_mean_3', 'delay_rolling_std_3',\n",
    "    'delay_rolling_mean_5', 'delay_rolling_std_5',\n",
    "    'delay_rolling_mean_10', 'delay_rolling_std_10',\n",
    "    # Time period features\n",
    "    'time_period', 'is_morning_rush', 'is_evening_rush'\n",
    "]\n",
    "\n",
    "# Filter to existing columns only\n",
    "available_cols = [c for c in base_cols + feature_store_cols if c in df_process.columns]\n",
    "df_process_selected = df_process[available_cols].copy()\n",
    "\n",
    "print(f\"Selected {len(available_cols)} columns\")\n",
    "print(f\"  - Base features: {len([c for c in base_cols if c in available_cols])}\")\n",
    "print(f\"  - Feature Store features: {len([c for c in feature_store_cols if c in available_cols])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03p42fe6t3g4",
   "metadata": {},
   "source": [
    "# Feature Store\n",
    "\n",
    "計算コストの高い特徴量（ラグ特徴量、移動平均など）を事前計算し、Parquet形式で保存します。\n",
    "これにより再計算の無駄を省き、モデル学習時に高速に読み込むことができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "i9y2lf9fndp",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_process (before): 6,276,723 rows, 4524.5 MB\n",
      "\n",
      "Computing Feature Store features (optimized)...\n",
      "Input data shape: (6276723, 38)\n",
      "Memory usage: 4524.5 MB\n",
      "\n",
      "[Step 1] Computing trip-level aggregates...\n",
      "  Trip aggregates: 453676 rows\n",
      "\n",
      "[Step 2] Sorting by group and time...\n",
      "\n",
      "[Step 3] Computing lag features...\n",
      "  - delay_lag_1 computed\n",
      "  - delay_lag_2 computed\n",
      "  - delay_lag_3 computed\n",
      "  - delay_lag_5 computed\n",
      "\n",
      "[Step 4] Computing rolling features...\n",
      "  - delay_rolling_mean_3, delay_rolling_std_3 computed\n",
      "  - delay_rolling_mean_5, delay_rolling_std_5 computed\n",
      "  - delay_rolling_mean_10, delay_rolling_std_10 computed\n",
      "\n",
      "[Step 5] Merging features back to original dataframe...\n",
      "  Processing in 125 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Chunk 10/125 done\n",
      "  - Chunk 20/125 done\n",
      "  - Chunk 30/125 done\n",
      "  - Chunk 40/125 done\n",
      "  - Chunk 50/125 done\n",
      "  - Chunk 60/125 done\n",
      "  - Chunk 70/125 done\n",
      "  - Chunk 80/125 done\n",
      "  - Chunk 90/125 done\n",
      "  - Chunk 100/125 done\n",
      "  - Chunk 110/125 done\n",
      "  - Chunk 120/125 done\n",
      "  - Chunk 125/125 done\n",
      "\n",
      "[Step 6] Concatenating chunks...\n",
      "\n",
      "Output data shape: (6276723, 48)\n",
      "Memory usage: 4764.0 MB\n",
      "\n",
      "Computing time-based features...\n",
      "df_process (after): 6,276,723 rows, 5162.8 MB\n",
      "\n",
      "Feature Store features computed:\n",
      "  - Lag features: ['delay_lag_1', 'delay_lag_2', 'delay_lag_3', 'delay_lag_5']\n",
      "  - Rolling features: ['delay_rolling_mean_3', 'delay_rolling_std_3', 'delay_rolling_mean_5', 'delay_rolling_std_5', 'delay_rolling_mean_10', 'delay_rolling_std_10']\n"
     ]
    }
   ],
   "source": [
    "# Feature Store計算の実行（最適化版）\n",
    "from feature_store_optimized import (\n",
    "    compute_all_features_optimized,\n",
    "    compute_time_features,\n",
    "    print_memory_usage\n",
    ")\n",
    "import gc\n",
    "\n",
    "if 'df_process' in locals() and df_process is not None:\n",
    "    print_memory_usage(df_process, \"df_process (before)\")\n",
    "    \n",
    "    print(\"\\nComputing Feature Store features (optimized)...\")\n",
    "    \n",
    "    # ラグ・移動統計量を一括計算\n",
    "    df_process = compute_all_features_optimized(\n",
    "        df_process,\n",
    "        lags=[1, 2, 3, 5],\n",
    "        windows=[3, 5, 10],\n",
    "        chunk_size=50000  # メモリに応じて調整\n",
    "    )\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    # 時間帯特徴量\n",
    "    print(\"\\nComputing time-based features...\")\n",
    "    df_process = compute_time_features(df_process)\n",
    "    \n",
    "    print_memory_usage(df_process, \"df_process (after)\")\n",
    "    \n",
    "    print(f\"\\nFeature Store features computed:\")\n",
    "    print(f\"  - Lag features: {[c for c in df_process.columns if 'lag_' in c]}\")\n",
    "    print(f\"  - Rolling features: {[c for c in df_process.columns if 'rolling_' in c]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5efc86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Feature Store (Parquet): data/processed_data/feature_store.parquet\n",
      "\n",
      "File size comparison:\n",
      "  - Parquet: 132.4 MB\n"
     ]
    }
   ],
   "source": [
    "# Save Feature Store (Parquet format for fast loading)\n",
    "import os\n",
    "\n",
    "output_dir = 'data/processed_data'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "if 'df_process_selected' in locals() and df_process_selected is not None:\n",
    "    # Ensure start_date is string\n",
    "    if 'start_date' in df_process_selected.columns:\n",
    "        df_process_selected['start_date'] = df_process_selected['start_date'].astype(str)\n",
    "\n",
    "    # CSV形式（互換性のため）\n",
    "    # csv_file = f'{output_dir}/processed_trip_data.csv'\n",
    "    # df_process_selected.to_csv(csv_file, index=False)\n",
    "    # print(f\"Saved CSV: {csv_file}\")\n",
    "    \n",
    "    # Parquet形式（Feature Store - 高速読み込み）\n",
    "    parquet_file = f'{output_dir}/feature_store.parquet'\n",
    "    df_process_selected.to_parquet(parquet_file, index=False, compression='snappy')\n",
    "    print(f\"Saved Feature Store (Parquet): {parquet_file}\")\n",
    "    \n",
    "    # ファイルサイズ比較\n",
    "    # csv_size = os.path.getsize(csv_file) / (1024 * 1024)\n",
    "    parquet_size = os.path.getsize(parquet_file) / (1024 * 1024)\n",
    "    print(f\"\\nFile size comparison:\")\n",
    "    # print(f\"  - CSV: {csv_size:.1f} MB\")\n",
    "    print(f\"  - Parquet: {parquet_size:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "epjh0ydxzm9",
   "metadata": {},
   "source": [
    "# Train / Valid / Test Split\n",
    "\n",
    "評価データセットの「完全固定」のため、時系列データを時間軸で分割して保存します。\n",
    "- 訓練期間: 70%\n",
    "- 検証期間: 15%\n",
    "- テスト期間: 15%\n",
    "\n",
    "すべてのモデル（03〜06）はこの同一のファイルを使用して評価を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "lf0p7r90z8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Time Series Split Information\n",
      "==================================================\n",
      "\n",
      "TRAIN:\n",
      "  Date range: 20251203 ~ 20251217\n",
      "  Samples: 4,292,431 (68.4%)\n",
      "\n",
      "VALID:\n",
      "  Date range: 20251218 ~ 20251220\n",
      "  Samples: 864,598 (13.8%)\n",
      "\n",
      "TEST:\n",
      "  Date range: 20251221 ~ 20251224\n",
      "  Samples: 1,119,694 (17.8%)\n",
      "==================================================\n",
      "Saved split data to data/processed_data/\n",
      "  - train.parquet: 4,292,431 samples\n",
      "  - valid.parquet: 864,598 samples\n",
      "  - test.parquet: 1,119,694 samples\n",
      "  - split_info.json\n"
     ]
    }
   ],
   "source": [
    "# Split data into train/valid/test and save\n",
    "from data_splitter import (\n",
    "    temporal_train_valid_test_split,\n",
    "    print_split_info,\n",
    "    save_split_data\n",
    ")\n",
    "\n",
    "if 'df_process_selected' in locals() and df_process_selected is not None:\n",
    "    # 時系列分割（70% train, 15% valid, 15% test）\n",
    "    df_train, df_valid, df_test, split_info = temporal_train_valid_test_split(\n",
    "        df_process_selected,\n",
    "        date_column='start_date',\n",
    "        train_ratio=0.7,\n",
    "        valid_ratio=0.15,\n",
    "        test_ratio=0.15\n",
    "    )\n",
    "    \n",
    "    # 分割情報を表示\n",
    "    print_split_info(split_info)\n",
    "    \n",
    "    # ファイルに保存（parquet形式）\n",
    "    save_split_data(\n",
    "        df_train, df_valid, df_test, split_info,\n",
    "        output_dir='data/processed_data',\n",
    "        format='parquet'\n",
    "    )\n",
    "else:\n",
    "    print(\"Error: df_process_selected is not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phtas227vm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Time Series Cross-Validation Summary (EXPANDING)\n",
      "======================================================================\n",
      "\n",
      "Fold 0:\n",
      "  Train: 20251203 ~ 20251205 (3 days, 934,277 samples)\n",
      "  Test:  20251206 ~ 20251207 (2 days, 450,316 samples)\n",
      "\n",
      "Fold 1:\n",
      "  Train: 20251203 ~ 20251208 (6 days, 1,692,470 samples)\n",
      "  Test:  20251209 ~ 20251210 (2 days, 619,814 samples)\n",
      "\n",
      "Fold 2:\n",
      "  Train: 20251203 ~ 20251211 (9 days, 2,624,390 samples)\n",
      "  Test:  20251212 ~ 20251213 (2 days, 547,295 samples)\n",
      "\n",
      "Fold 3:\n",
      "  Train: 20251203 ~ 20251214 (12 days, 3,379,155 samples)\n",
      "  Test:  20251215 ~ 20251216 (2 days, 600,570 samples)\n",
      "\n",
      "Fold 4:\n",
      "  Train: 20251203 ~ 20251217 (15 days, 4,292,431 samples)\n",
      "  Test:  20251218 ~ 20251219 (2 days, 625,058 samples)\n",
      "======================================================================\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Preview Time Series Cross-Validation folds\n",
    "from data_splitter import TimeSeriesCrossValidator\n",
    "\n",
    "if 'df_process_selected' in locals() and df_process_selected is not None:\n",
    "    # Expanding Window 交差検証のプレビュー\n",
    "    cv = TimeSeriesCrossValidator(\n",
    "        n_splits=5,\n",
    "        test_size=2,  # 2日分をテスト\n",
    "        gap=0,\n",
    "        method='expanding',\n",
    "        date_column='start_date'\n",
    "    )\n",
    "    cv.print_fold_summary(df_process_selected)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
