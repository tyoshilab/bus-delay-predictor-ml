{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12bfc4be",
   "metadata": {},
   "source": [
    "# 05. Model LSTM\n",
    "LSTM model implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4174693f",
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Dropout, BatchNormalization, Concatenate\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.regularizers import l2\nfrom sklearn.preprocessing import StandardScaler\nimport utils\n\n# Load Split Data using shared function\ndf_train, df_valid, df_test, df_process, split_info = utils.load_split_data_with_combined()\n\nif df_process is None:\n    raise RuntimeError(\"Data not found. Please run 02_process_data.ipynb first.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ca1efc",
   "metadata": {},
   "outputs": [],
   "source": "# Hyperparameters\nN_PAST_TRIPS = 5  # Try: 3, 5, 7, 10 to find optimal sequence length\nprint(f\"Sequence Length (n_past_trips): {N_PAST_TRIPS}\")\n\n# Create Sequences using shared function\nn_past_trips = N_PAST_TRIPS\ndata = utils.prepare_model_data(df_train, df_test, df_process, n_past_trips=n_past_trips)\n\n# Extract variables\nX_delays_train, X_features_train, X_agg_train, y_train = \\\n    data['X_delays_train'], data['X_features_train'], data['X_agg_train'], data['y_train']\nX_delays_test, X_features_test, X_agg_test, y_test = \\\n    data['X_delays_test'], data['X_features_test'], data['X_agg_test'], data['y_test']\nn_stops = data['n_stops']\nstops_dict = data['stops_dict']"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d81ba4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling\n",
    "if df_process is not None:\n",
    "    delay_scaler = StandardScaler()\n",
    "    X_delays_train_scaled = delay_scaler.fit_transform(X_delays_train.reshape(-1, n_stops)).reshape(X_delays_train.shape)\n",
    "    X_delays_test_scaled = delay_scaler.transform(X_delays_test.reshape(-1, n_stops)).reshape(X_delays_test.shape)\n",
    "\n",
    "    y_train_scaled = delay_scaler.transform(y_train)\n",
    "    y_test_scaled = delay_scaler.transform(y_test)\n",
    "\n",
    "    feature_scaler = StandardScaler()\n",
    "    X_combined_train = np.concatenate([X_features_train, X_agg_train], axis=1)\n",
    "    X_combined_test = np.concatenate([X_features_test, X_agg_test], axis=1)\n",
    "\n",
    "    X_combined_train_scaled = feature_scaler.fit_transform(X_combined_train)\n",
    "    X_combined_test_scaled = feature_scaler.transform(X_combined_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b748c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer, Attention, MultiHeadAttention, LayerNormalization, GlobalAveragePooling1D\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# Custom Attention Layer\n",
    "class TemporalAttention(Layer):\n",
    "    \"\"\"\n",
    "    Attention mechanism for LSTM sequences.\n",
    "    \n",
    "    Learns which past time steps are most important for predicting current delays.\n",
    "    Returns attention weights for interpretability.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super(TemporalAttention, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(\n",
    "            name='attention_weight',\n",
    "            shape=(input_shape[-1], input_shape[-1]),\n",
    "            initializer='glorot_uniform',\n",
    "            trainable=True\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            name='attention_bias',\n",
    "            shape=(input_shape[-1],),\n",
    "            initializer='zeros',\n",
    "            trainable=True\n",
    "        )\n",
    "        self.u = self.add_weight(\n",
    "            name='attention_context',\n",
    "            shape=(input_shape[-1],),\n",
    "            initializer='glorot_uniform',\n",
    "            trainable=True\n",
    "        )\n",
    "        super(TemporalAttention, self).build(input_shape)\n",
    "    \n",
    "    def call(self, x):\n",
    "        # x shape: (batch, time_steps, features)\n",
    "        # Compute attention scores\n",
    "        uit = K.tanh(K.dot(x, self.W) + self.b)  # (batch, time_steps, features)\n",
    "        ait = K.dot(uit, K.expand_dims(self.u))  # (batch, time_steps, 1)\n",
    "        ait = K.squeeze(ait, -1)  # (batch, time_steps)\n",
    "        \n",
    "        # Softmax to get attention weights\n",
    "        ait = K.softmax(ait)  # (batch, time_steps)\n",
    "        \n",
    "        # Apply attention weights\n",
    "        weighted = x * K.expand_dims(ait)  # (batch, time_steps, features)\n",
    "        output = K.sum(weighted, axis=1)  # (batch, features)\n",
    "        \n",
    "        return output, ait\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return [(input_shape[0], input_shape[-1]), (input_shape[0], input_shape[1])]\n",
    "\n",
    "\n",
    "def build_lstm_model_basic(n_past_trips, n_stops, n_features):\n",
    "    \"\"\"Basic LSTM model (original)\"\"\"\n",
    "    delay_input = Input(shape=(n_past_trips, n_stops), name='delay_input')\n",
    "    x = LSTM(64, return_sequences=True, kernel_regularizer=l2(0.001))(delay_input)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = LSTM(32, return_sequences=False, kernel_regularizer=l2(0.001))(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    feature_input = Input(shape=(n_features,), name='feature_input')\n",
    "    f = Dense(32, activation='relu')(feature_input)\n",
    "    f = Dropout(0.2)(f)\n",
    "    f = Dense(16, activation='relu')(f)\n",
    "\n",
    "    combined = Concatenate()([x, f])\n",
    "    combined = Dense(64, activation='relu', kernel_regularizer=l2(0.001))(combined)\n",
    "    combined = BatchNormalization()(combined)\n",
    "    combined = Dropout(0.3)(combined)\n",
    "    combined = Dense(32, activation='relu')(combined)\n",
    "\n",
    "    output = Dense(n_stops, activation='linear')(combined)\n",
    "\n",
    "    model = Model(inputs=[delay_input, feature_input], outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_lstm_model_with_attention(n_past_trips, n_stops, n_features):\n",
    "    \"\"\"\n",
    "    LSTM with Temporal Attention.\n",
    "    \n",
    "    Attention mechanism allows the model to learn which past trips are most\n",
    "    important for predicting the current delay. This provides:\n",
    "    1. Better performance by focusing on relevant past data\n",
    "    2. Interpretability - we can see attention weights\n",
    "    \"\"\"\n",
    "    # Delay sequence input\n",
    "    delay_input = Input(shape=(n_past_trips, n_stops), name='delay_input')\n",
    "    \n",
    "    # LSTM encoding\n",
    "    x = LSTM(64, return_sequences=True, kernel_regularizer=l2(0.001))(delay_input)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = LSTM(32, return_sequences=True, kernel_regularizer=l2(0.001))(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # Apply temporal attention\n",
    "    attention_layer = TemporalAttention(name='temporal_attention')\n",
    "    attended, attention_weights = attention_layer(x)\n",
    "    \n",
    "    # Feature input\n",
    "    feature_input = Input(shape=(n_features,), name='feature_input')\n",
    "    f = Dense(32, activation='relu')(feature_input)\n",
    "    f = Dropout(0.2)(f)\n",
    "    f = Dense(16, activation='relu')(f)\n",
    "    \n",
    "    # Combine attended sequence with features\n",
    "    combined = Concatenate()([attended, f])\n",
    "    combined = Dense(64, activation='relu', kernel_regularizer=l2(0.001))(combined)\n",
    "    combined = BatchNormalization()(combined)\n",
    "    combined = Dropout(0.3)(combined)\n",
    "    combined = Dense(32, activation='relu')(combined)\n",
    "    \n",
    "    output = Dense(n_stops, activation='linear')(combined)\n",
    "    \n",
    "    # Main model\n",
    "    model = Model(inputs=[delay_input, feature_input], outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "    \n",
    "    # Attention model for interpretation\n",
    "    attention_model = Model(inputs=[delay_input, feature_input], \n",
    "                           outputs=[output, attention_weights])\n",
    "    \n",
    "    return model, attention_model\n",
    "\n",
    "\n",
    "def build_lstm_model_with_multihead_attention(n_past_trips, n_stops, n_features, n_heads=4):\n",
    "    \"\"\"\n",
    "    LSTM with Multi-Head Attention (Transformer-style).\n",
    "    \n",
    "    Uses multiple attention heads to capture different aspects of temporal patterns.\n",
    "    \"\"\"\n",
    "    # Delay sequence input\n",
    "    delay_input = Input(shape=(n_past_trips, n_stops), name='delay_input')\n",
    "    \n",
    "    # Project to attention dimension\n",
    "    x = Dense(64)(delay_input)\n",
    "    \n",
    "    # Multi-head self-attention\n",
    "    attention_output = MultiHeadAttention(\n",
    "        num_heads=n_heads, \n",
    "        key_dim=16,\n",
    "        name='multihead_attention'\n",
    "    )(x, x)\n",
    "    \n",
    "    # Add & Norm\n",
    "    x = LayerNormalization()(x + attention_output)\n",
    "    \n",
    "    # LSTM for sequential processing\n",
    "    x = LSTM(32, return_sequences=False, kernel_regularizer=l2(0.001))(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # Feature input\n",
    "    feature_input = Input(shape=(n_features,), name='feature_input')\n",
    "    f = Dense(32, activation='relu')(feature_input)\n",
    "    f = Dropout(0.2)(f)\n",
    "    f = Dense(16, activation='relu')(f)\n",
    "    \n",
    "    # Combine\n",
    "    combined = Concatenate()([x, f])\n",
    "    combined = Dense(64, activation='relu', kernel_regularizer=l2(0.001))(combined)\n",
    "    combined = BatchNormalization()(combined)\n",
    "    combined = Dropout(0.3)(combined)\n",
    "    combined = Dense(32, activation='relu')(combined)\n",
    "    \n",
    "    output = Dense(n_stops, activation='linear')(combined)\n",
    "    \n",
    "    model = Model(inputs=[delay_input, feature_input], outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Model Selection\n",
    "MODEL_TYPE = 'attention'  # Options: 'basic', 'attention', 'multihead'\n",
    "\n",
    "if df_process is not None:\n",
    "    n_features = X_combined_train.shape[1]\n",
    "    \n",
    "    print(f\"Building {MODEL_TYPE.upper()} LSTM model...\")\n",
    "    print(f\"  - Sequence length: {n_past_trips}\")\n",
    "    print(f\"  - Stops per route: {n_stops}\")\n",
    "    print(f\"  - Number of features: {n_features}\")\n",
    "    \n",
    "    if MODEL_TYPE == 'basic':\n",
    "        model = build_lstm_model_basic(n_past_trips, n_stops, n_features)\n",
    "        attention_model = None\n",
    "    elif MODEL_TYPE == 'attention':\n",
    "        model, attention_model = build_lstm_model_with_attention(n_past_trips, n_stops, n_features)\n",
    "    elif MODEL_TYPE == 'multihead':\n",
    "        model = build_lstm_model_with_multihead_attention(n_past_trips, n_stops, n_features, n_heads=4)\n",
    "        attention_model = None\n",
    "    \n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7fb4fa",
   "metadata": {},
   "outputs": [],
   "source": "# Train\nevaluation_results = []\n\nhistory = model.fit(\n    [X_delays_train_scaled, X_combined_train_scaled],\n    y_train_scaled,\n    validation_split=0.2,\n    epochs=20,\n    batch_size=64,\n    verbose=1\n)\n\n# Evaluate\ny_pred_scaled = model.predict([X_delays_test_scaled, X_combined_test_scaled])\ny_pred = delay_scaler.inverse_transform(y_pred_scaled)\n\nresult_lstm = utils.evaluate_model(\n    y_test, y_pred,\n    model_name=f\"LSTM ({MODEL_TYPE.capitalize()})\",\n    config={\n        \"model_type\": MODEL_TYPE,\n        \"n_past_trips\": n_past_trips,\n        \"epochs\": 20,\n        \"batch_size\": 64,\n        \"scaler\": \"StandardScaler\"\n    }\n)\nevaluation_results.append(result_lstm)\nprint(result_lstm.summary())"
  },
  {
   "cell_type": "markdown",
   "id": "5kqbtuhi8p7",
   "metadata": {},
   "source": [
    "# Attention Weights Visualization\n",
    "\n",
    "Attentionモデルを使用している場合、どの過去の時点が現在の遅延予測に寄与しているかを可視化できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2qth8n92qz7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if MODEL_TYPE == 'attention' and attention_model is not None:\n",
    "    # Get attention weights for test samples\n",
    "    _, attention_weights = attention_model.predict(\n",
    "        [X_delays_test_scaled[:1000], X_combined_test_scaled[:1000]]\n",
    "    )\n",
    "    \n",
    "    # Average attention weights across samples\n",
    "    avg_attention = attention_weights.mean(axis=0)\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Bar plot of average attention weights\n",
    "    time_labels = [f't-{n_past_trips-i}' for i in range(n_past_trips)]\n",
    "    colors = plt.cm.Blues(np.linspace(0.3, 0.9, n_past_trips))\n",
    "    \n",
    "    axes[0].bar(time_labels, avg_attention, color=colors, edgecolor='white')\n",
    "    axes[0].set_xlabel('Past Trip (Time Step)')\n",
    "    axes[0].set_ylabel('Average Attention Weight')\n",
    "    axes[0].set_title('Temporal Attention Weights\\n(Which past trips influence the prediction?)')\n",
    "    \n",
    "    for i, (label, weight) in enumerate(zip(time_labels, avg_attention)):\n",
    "        axes[0].text(i, weight + 0.01, f'{weight:.2%}', ha='center', fontsize=10)\n",
    "    \n",
    "    # Heatmap of attention weights for first 20 samples\n",
    "    im = axes[1].imshow(attention_weights[:20], aspect='auto', cmap='Blues')\n",
    "    axes[1].set_xlabel('Time Step')\n",
    "    axes[1].set_ylabel('Sample Index')\n",
    "    axes[1].set_title('Attention Weights Heatmap (First 20 Samples)')\n",
    "    axes[1].set_xticks(range(n_past_trips))\n",
    "    axes[1].set_xticklabels(time_labels)\n",
    "    plt.colorbar(im, ax=axes[1], label='Attention Weight')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('data/processed_data/attention_weights.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nAttention Weight Statistics:\")\n",
    "    print(f\"  Most attended time step: {time_labels[np.argmax(avg_attention)]} ({avg_attention.max():.2%})\")\n",
    "    print(f\"  Least attended time step: {time_labels[np.argmin(avg_attention)]} ({avg_attention.min():.2%})\")\n",
    "    \n",
    "    # Interpretation\n",
    "    print(\"\\nInterpretation:\")\n",
    "    if np.argmax(avg_attention) == n_past_trips - 1:\n",
    "        print(\"  -> The model focuses most on the most recent trip (t-1).\")\n",
    "        print(\"     This suggests short-term patterns are most predictive.\")\n",
    "    elif np.argmax(avg_attention) < n_past_trips // 2:\n",
    "        print(\"  -> The model focuses more on older trips.\")\n",
    "        print(\"     This may indicate longer-term patterns or regular schedules.\")\n",
    "    else:\n",
    "        print(\"  -> The model uses a balanced mix of recent and older information.\")\n",
    "else:\n",
    "    print(\"Attention visualization is only available when MODEL_TYPE='attention'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pty1f5kvaji",
   "metadata": {},
   "source": [
    "# Sequence Length Experiment\n",
    "\n",
    "シーケンス長（過去何便を見るか）を変えて実験し、最適な値を見つけます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x49zng7vdql",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence Length Experiment (Optional - can be time-consuming)\n",
    "RUN_SEQUENCE_EXPERIMENT = False  # Set to True to run experiment\n",
    "\n",
    "if RUN_SEQUENCE_EXPERIMENT and df_process is not None:\n",
    "    sequence_lengths = [3, 5, 7, 10]\n",
    "    results = []\n",
    "    \n",
    "    print(\"Running Sequence Length Experiment...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for seq_len in sequence_lengths:\n",
    "        print(f\"\\n--- Sequence Length: {seq_len} ---\")\n",
    "        \n",
    "        # Create sequences with current length\n",
    "        X_del_train, X_feat_train, X_agg_train, y_tr, _, _, n_st = utils.create_trip_based_sequences_multi_route(\n",
    "            df_train, seq_len, stops_dict=stops_dict\n",
    "        )\n",
    "        X_del_test, X_feat_test, X_agg_test, y_te, _, _, _ = utils.create_trip_based_sequences_multi_route(\n",
    "            df_test, seq_len, stops_dict=stops_dict\n",
    "        )\n",
    "        \n",
    "        # Scale\n",
    "        d_scaler = StandardScaler()\n",
    "        X_del_train_sc = d_scaler.fit_transform(X_del_train.reshape(-1, n_st)).reshape(X_del_train.shape)\n",
    "        X_del_test_sc = d_scaler.transform(X_del_test.reshape(-1, n_st)).reshape(X_del_test.shape)\n",
    "        y_tr_sc = d_scaler.transform(y_tr)\n",
    "        \n",
    "        f_scaler = StandardScaler()\n",
    "        X_comb_train = np.concatenate([X_feat_train, X_agg_train], axis=1)\n",
    "        X_comb_test = np.concatenate([X_feat_test, X_agg_test], axis=1)\n",
    "        X_comb_train_sc = f_scaler.fit_transform(X_comb_train)\n",
    "        X_comb_test_sc = f_scaler.transform(X_comb_test)\n",
    "        \n",
    "        # Build and train model (basic LSTM for speed)\n",
    "        n_feat = X_comb_train.shape[1]\n",
    "        exp_model = build_lstm_model_basic(seq_len, n_st, n_feat)\n",
    "        \n",
    "        exp_model.fit(\n",
    "            [X_del_train_sc, X_comb_train_sc],\n",
    "            y_tr_sc,\n",
    "            validation_split=0.2,\n",
    "            epochs=10,  # Reduced for speed\n",
    "            batch_size=64,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Evaluate\n",
    "        y_pred_sc = exp_model.predict([X_del_test_sc, X_comb_test_sc])\n",
    "        y_pred_exp = d_scaler.inverse_transform(y_pred_sc)\n",
    "        \n",
    "        mae = mean_absolute_error(y_te.flatten(), y_pred_exp.flatten())\n",
    "        r2 = r2_score(y_te.flatten(), y_pred_exp.flatten())\n",
    "        \n",
    "        results.append({\n",
    "            'sequence_length': seq_len,\n",
    "            'mae': mae,\n",
    "            'r2': r2,\n",
    "            'train_samples': len(y_tr),\n",
    "            'test_samples': len(y_te)\n",
    "        })\n",
    "        \n",
    "        print(f\"  MAE: {mae:.2f}s, R2: {r2:.4f}\")\n",
    "        \n",
    "        # Clean up\n",
    "        del exp_model, X_del_train, X_del_test, y_tr, y_te\n",
    "        import gc\n",
    "        gc.collect()\n",
    "    \n",
    "    # Summary\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Sequence Length Experiment Results:\")\n",
    "    print(results_df.to_string(index=False))\n",
    "    \n",
    "    best_idx = results_df['mae'].idxmin()\n",
    "    print(f\"\\nBest sequence length: {results_df.loc[best_idx, 'sequence_length']} (MAE: {results_df.loc[best_idx, 'mae']:.2f}s)\")\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax.plot(results_df['sequence_length'], results_df['mae'], 'bo-', markersize=10, linewidth=2)\n",
    "    ax.set_xlabel('Sequence Length (n_past_trips)')\n",
    "    ax.set_ylabel('MAE (seconds)')\n",
    "    ax.set_title('Sequence Length vs. Prediction Error')\n",
    "    ax.set_xticks(sequence_lengths)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    for _, row in results_df.iterrows():\n",
    "        ax.annotate(f\"{row['mae']:.1f}s\", \n",
    "                   (row['sequence_length'], row['mae']),\n",
    "                   textcoords=\"offset points\", xytext=(0, 10), ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('data/processed_data/sequence_length_experiment.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Sequence length experiment skipped. Set RUN_SEQUENCE_EXPERIMENT=True to run.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "togw9ud94m",
   "metadata": {},
   "outputs": [],
   "source": "# Model Comparison Table and Save Results\nutils.display_and_save_results(evaluation_results, 'data/evaluation_results_lstm.json')"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}