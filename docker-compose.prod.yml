version: '3.8'

# Production Docker Compose Configuration
# Usage: docker-compose -f docker-compose.prod.yml up -d

services:
  # PostgreSQL Database with PostGIS
  postgres:
    build:
      context: .
      dockerfile: Dockerfile.db
      args:
        GITHUB_REPO_URL: ${GITHUB_REPO_URL:-https://github.com/taitalabs/vancoucer-bus-science/releases/download}
        DUMP_VERSION: ${DUMP_VERSION:-v1.0.0}
        DUMP_FILENAME: ${DUMP_FILENAME:-gtfs_db.dump}
    image: gtfs-postgres:latest
    container_name: gtfs-postgres-prod
    environment:
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_USER: ${POSTGRES_USER:-postgres}
      POSTGRES_DB: ${POSTGRES_DB:-gtfs_production}
      TZ: ${TZ:-America/Vancouver}
    command: ["postgres", "-c", "ssl=off"]
    ports:
      - "${DB_PORT:-5432}:5432"
    volumes:
      # Named volume for data persistence
      - postgres_data_prod:/var/lib/postgresql/data
      # Optional: Mount dump file for initial data load
      # - ./backup.dump:/docker-entrypoint-initdb.d/backup.dump:ro
    networks:
      - gtfs-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-postgres} -d ${POSTGRES_DB:-gtfs_production}"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped
    # Security: Limit resources
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 512M

  # FastAPI Application
  api:
    build:
      context: .
      dockerfile: Dockerfile.api
    image: gtfs-api:latest
    container_name: gtfs-api-prod
    environment:
      DATABASE_URL: postgresql://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB:-gtfs_production}
      ENVIRONMENT: production
      DEBUG: "false"
      LOG_LEVEL: ${LOG_LEVEL:-INFO}
      TZ: ${TZ:-America/Vancouver}
    ports:
      - "${API_PORT:-8000}:8000"
    volumes:
      # Mount model files (read-only in production)
      - ./files:/app/files:ro
    networks:
      - gtfs-network
    depends_on:
      postgres:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8000/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    # Security: Limit resources
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 256M

  # Batch Processing Jobs
  batch:
    build:
      context: .
      dockerfile: Dockerfile.batch
    image: gtfs-batch:latest
    container_name: gtfs-batch-prod
    environment:
      DATABASE_URL: postgresql://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB:-gtfs_production}
      TRANSLINK_API_KEY: ${TRANSLINK_API_KEY}
      LOG_LEVEL: ${LOG_LEVEL:-INFO}
      TZ: ${TZ:-America/Vancouver}
      PREDICTION_MODEL_PATH: ${PREDICTION_MODEL_PATH:-files/model/best_delay_model.h5}
      GTFS_RT_CLEANUP_DAYS: ${GTFS_RT_CLEANUP_DAYS:-7}
      WEATHER_SCRAPER_ROW_LIMIT: ${WEATHER_SCRAPER_ROW_LIMIT:-120}
      WEATHER_FILE_CLEANUP_DAYS: ${WEATHER_FILE_CLEANUP_DAYS:-7}
      # TensorFlow optimizations
      TF_CPP_MIN_LOG_LEVEL: "2"  # Reduce TensorFlow logging
      OMP_NUM_THREADS: "4"  # Limit OpenMP threads
      # Optional: Run an initial job on startup
      # INITIAL_JOB: "load-realtime"
    volumes:
      # Persistent volumes for logs and downloads
      - batch_logs_prod:/app/batch/logs
      - batch_downloads_prod:/app/batch/downloads
      # Mount model files (read-only in production)
      - ./files:/app/files:ro
    networks:
      - gtfs-network
    depends_on:
      postgres:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pgrep -x cron > /dev/null || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    # Resource limits optimized for ML prediction workload
    # Memory breakdown:
    # - TensorFlow/Keras runtime: ~500-600 MB
    # - Model + data processing: ~400-500 MB
    # - Playwright browser (weather scraper): ~300 MB
    # - OS/Python overhead: ~200 MB
    # Total peak: ~1.5 GB, limit set to 3 GB for safety margin
    deploy:
      resources:
        limits:
          cpus: '4.0'  # Increased for parallel prediction processing
          memory: 3G  # Increased for TensorFlow + Playwright workload
        reservations:
          cpus: '1.0'  # Higher reservation for consistent ML performance
          memory: 1G  # Reserve enough for base TensorFlow operations

networks:
  gtfs-network:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.28.0.0/16

volumes:
  postgres_data_prod:
    driver: local
  batch_logs_prod:
    driver: local
  batch_downloads_prod:
    driver: local
