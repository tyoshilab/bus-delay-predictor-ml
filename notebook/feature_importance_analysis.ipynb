{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a97f784",
   "metadata": {},
   "source": [
    "# ç‰¹å¾´é‡é‡è¦åº¦åˆ†æ (Feature Importance Analysis)\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€ãƒã‚¹é…å»¶äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã§ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ç‰¹å¾´é‡ã®é‡è¦åº¦ã‚’åˆ†æã—ã¾ã™ã€‚\n",
    "\n",
    "## âš ï¸ é‡è¦ãªåˆ¶ç´„äº‹é …\n",
    "\n",
    "**æ—¢å­˜ã®å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã¯å…¥åŠ›å½¢çŠ¶ãŒå›ºå®šã•ã‚Œã¦ã„ã¾ã™**:\n",
    "- ãƒ¢ãƒ‡ãƒ«: `delay_prediction_final_region.h5`\n",
    "- ãƒ¢ãƒ‡ãƒ«å…¥åŠ›: `(batch, 8, 1, 17, 1)` â†’ **17å€‹ã®ç‰¹å¾´é‡ã§å›ºå®š**\n",
    "- ç‰¹å¾´é‡ã‚’è¿½åŠ /å‰Šé™¤ã™ã‚‹ã¨ã€ãƒ¢ãƒ‡ãƒ«ãŒä½¿ç”¨ã§ããªããªã‚Šã¾ã™\n",
    "- **ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯**: æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã‚’ä¿æŒã—ãŸã¾ã¾ã€ç‰¹å¾´é‡ã®é‡è¦åº¦ã®ã¿ã‚’åˆ†æã—ã¾ã™\n",
    "\n",
    "## ğŸ“Š åˆ†æç›®çš„\n",
    "\n",
    "- å…¨ç‰¹å¾´é‡ã®é‡è¦åº¦ã‚’å®šé‡çš„ã«è©•ä¾¡ï¼ˆPermutation Importanceï¼‰\n",
    "- ç‰¹å¾´é‡ã‚’æ™‚é–“/ä½ç½®/çµ±è¨ˆ/æ°—å€™ã®4ã‚«ãƒ†ã‚´ãƒªã«åˆ†é¡\n",
    "- ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®é‡è¦åº¦ã‚’å¯è¦–åŒ–\n",
    "- ãƒ¢ãƒ‡ãƒ«æ€§èƒ½å‘ä¸Šã¨æ¨è«–é€Ÿåº¦æ”¹å–„ã®ãŸã‚ã®ç‰¹å¾´é‡é¸æŠã®åŸºç¤ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ\n",
    "\n",
    "## ğŸ”¬ ä½¿ç”¨ã™ã‚‹æ‰‹æ³•\n",
    "\n",
    "- **Custom Permutation Importance for ConvLSTM**: \n",
    "  - 4æ¬¡å…ƒå…¥åŠ›æ§‹é€ ã‚’ä¿æŒ `(batch, timesteps, height, width, channels)`\n",
    "  - å„ç‰¹å¾´é‡ï¼ˆwidthæ¬¡å…ƒã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼‰ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«\n",
    "  - MAEå¢—åŠ é‡ã§é‡è¦åº¦ã‚’æ¸¬å®š\n",
    "- **ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æ**: ç‰¹å¾´é‡ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã®é‡è¦åº¦é›†è¨ˆ\n",
    "\n",
    "## ğŸ“¦ æœŸå¾…ã•ã‚Œã‚‹æˆæœç‰©\n",
    "\n",
    "1. ç‰¹å¾´é‡é‡è¦åº¦ã‚¹ã‚³ã‚¢ï¼ˆCSVå½¢å¼ï¼‰\n",
    "2. å¯è¦–åŒ–ã‚°ãƒ©ãƒ•ï¼ˆTop-Nç‰¹å¾´é‡ã€ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æï¼‰\n",
    "3. åˆ†æã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆ\n",
    "4. å‰Šæ¸›å€™è£œãƒªã‚¹ãƒˆï¼ˆä½é‡è¦åº¦ç‰¹å¾´é‡ï¼‰\n",
    "\n",
    "## ğŸ”„ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆTODO 1.2-1.4ï¼‰\n",
    "\n",
    "ã“ã®åˆ†æçµæœã‚’åŸºã«:\n",
    "- ç‰¹å¾´é‡å‰Šæ¸›å®Ÿé¨“ï¼ˆãƒ¢ãƒ‡ãƒ«å†å­¦ç¿’ãŒå¿…è¦ï¼‰\n",
    "- æ–°è¦ç‰¹å¾´é‡ã®è¿½åŠ å®Ÿé¨“\n",
    "- è©³ç´°ã¯ `doc/FEATURE_IMPORTANCE_STRATEGY.md` ã‚’å‚ç…§"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43328a4a",
   "metadata": {},
   "source": [
    "## 1. ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5d4d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sys.path.append('/app')\n",
    "\n",
    "# ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "from src.timeseries_processing import SequenceCreator, DataSplitter, DataStandardizer\n",
    "from src.model_training import DelayPredictionModel\n",
    "\n",
    "# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®šï¼ˆå¯è¦–åŒ–ç”¨ï¼‰\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Analysis started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b847131e",
   "metadata": {},
   "source": [
    "## 2. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8d22e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿\n",
    "print(\"Loading dataset...\")\n",
    "delay_features = pd.read_csv('/app/data/merged_dataset.csv')\n",
    "\n",
    "print(f\"Dataset shape: {delay_features.shape}\")\n",
    "print(f\"\\nColumn names ({len(delay_features.columns)} features):\")\n",
    "print(delay_features.columns.tolist())\n",
    "\n",
    "# åŸºæœ¬çµ±è¨ˆ\n",
    "print(f\"\\nBasic statistics:\")\n",
    "print(f\"- Routes: {delay_features['route_id'].nunique()}\")\n",
    "print(f\"- Date range: {delay_features['time_bucket'].min()} to {delay_features['time_bucket'].max()}\")\n",
    "print(f\"- Mean delay: {delay_features['arrival_delay'].mean():.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef8058a",
   "metadata": {},
   "source": [
    "## 3. ç‰¹å¾´é‡ã®ã‚«ãƒ†ã‚´ãƒªåˆ†é¡\n",
    "\n",
    "å…¨ç‰¹å¾´é‡ã‚’ä»¥ä¸‹ã®4ã¤ã®ã‚«ãƒ†ã‚´ãƒªã«åˆ†é¡ã—ã¾ã™ï¼š\n",
    "- **æ™‚é–“ç‰¹å¾´é‡ (Temporal)**: æ™‚åˆ»ã€æ›œæ—¥ã€ãƒ”ãƒ¼ã‚¯æ™‚é–“ãªã©\n",
    "- **ä½ç½®ç‰¹å¾´é‡ (Location)**: ç·¯åº¦çµŒåº¦ã€åœ°åŸŸã€éƒ½å¿ƒã‹ã‚‰ã®è·é›¢ãªã©\n",
    "- **çµ±è¨ˆç‰¹å¾´é‡ (Statistical)**: éå»ã®å¹³å‡é…å»¶ã€ç§»å‹•æ™‚é–“ãªã©\n",
    "- **æ°—å€™ç‰¹å¾´é‡ (Climate)**: å¤©å€™ã€æ°—æ¸©ã€é™æ°´é‡ãªã©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d09cf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç‰¹å¾´é‡ã®ã‚«ãƒ†ã‚´ãƒªå®šç¾©\n",
    "feature_categories = {\n",
    "    'temporal': [\n",
    "        'hour_sin', 'hour_cos', 'day_sin', 'day_cos',\n",
    "        'hour_of_day', 'day_of_week', 'is_peak_hour', 'is_weekend',\n",
    "        'time_period_detailed'\n",
    "    ],\n",
    "    'location': [\n",
    "        'stop_lat', 'stop_lon', 'lat_sin', 'lat_cos', 'lon_sin', 'lon_cos',\n",
    "        'lat_relative', 'lon_relative', 'distance_from_downtown_km',\n",
    "        'region_id', 'region_id_encoded', 'area_type', 'area_type_encoded',\n",
    "        'area_density_score'\n",
    "    ],\n",
    "    'statistical': [\n",
    "        'delay_mean_by_route_hour', 'travel_mean_by_route_hour',\n",
    "        'travel_time_duration', 'observation_count'\n",
    "    ],\n",
    "    'climate': [\n",
    "        'temp', 'precipitation', 'humidex', 'wind_speed',\n",
    "        'weather_sunny', 'weather_cloudy', 'weather_rainy'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«å…¥åŠ›ç”¨ã®ç‰¹å¾´é‡ï¼ˆarrival_delayã¯ç›®çš„å¤‰æ•°ãªã®ã§é™¤å¤–ï¼‰\n",
    "model_features = {\n",
    "    'temporal': ['hour_sin', 'hour_cos', 'day_sin', 'day_cos', 'is_peak_hour', 'is_weekend'],\n",
    "    'location': ['region_id_encoded', 'area_type_encoded', 'distance_from_downtown_km'],\n",
    "    'statistical': ['delay_mean_by_route_hour', 'travel_mean_by_route_hour'],\n",
    "    'climate': ['weather_sunny', 'weather_cloudy', 'weather_rainy', 'temp', 'precipitation']\n",
    "}\n",
    "\n",
    "# å…¨ãƒ¢ãƒ‡ãƒ«ç‰¹å¾´é‡ã®ãƒªã‚¹ãƒˆ\n",
    "all_model_features = []\n",
    "for category, features in model_features.items():\n",
    "    all_model_features.extend(features)\n",
    "\n",
    "print(\"Feature categorization:\")\n",
    "for category, features in model_features.items():\n",
    "    print(f\"\\n{category.upper()} ({len(features)} features):\")\n",
    "    for feat in features:\n",
    "        print(f\"  - {feat}\")\n",
    "\n",
    "print(f\"\\nTotal model features: {len(all_model_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b415db",
   "metadata": {},
   "source": [
    "## 4. æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ed01e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConvLSTMç”¨ã®ç‰¹å¾´é‡ã‚°ãƒ«ãƒ¼ãƒ—å®šç¾©\n",
    "# ãƒ¢ãƒ‡ãƒ«å­¦ç¿’æ™‚ã¨åŒã˜å®šç¾©ï¼ˆbus_arrival_forecast_model.ipynbã‹ã‚‰ï¼‰\n",
    "# æ³¨: 'region'ã‚°ãƒ«ãƒ¼ãƒ—ã«distance_from_downtown_kmãŒ2å›å«ã¾ã‚Œã¦ã„ã‚‹ï¼ˆæ„å›³çš„ãªé‡è¤‡ï¼‰\n",
    "feature_groups = {\n",
    "    'temporal': ['hour_sin', 'hour_cos', 'day_sin', 'day_cos', 'is_peak_hour', 'is_weekend', 'arrival_delay'],\n",
    "    'region': ['region_id_encoded', 'area_type_encoded', 'distance_from_downtown_km', 'distance_from_downtown_km'],\n",
    "    'weather': ['weather_sunny', 'weather_cloudy', 'weather_rainy', 'temp', 'precipitation'],\n",
    "    'target': ['arrival_delay']\n",
    "}\n",
    "\n",
    "# æ™‚ç³»åˆ—ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä½œæˆ\n",
    "print(\"Creating time series sequences...\")\n",
    "print(\"Note: feature_groups has intentional duplicate (distance_from_downtown_km appears twice)\")\n",
    "sequence_creator = SequenceCreator(\n",
    "    input_timesteps=8, \n",
    "    output_timesteps=3,\n",
    "    feature_groups=feature_groups\n",
    ")\n",
    "\n",
    "X_delay, y_delay, route_direction_info, used_features, feature_group_info = sequence_creator.create_route_direction_aware_sequences(\n",
    "    delay_features,\n",
    "    spatial_organization=True\n",
    ")\n",
    "\n",
    "print(f\"\\nSequence shapes:\")\n",
    "print(f\"  X: {X_delay.shape}\")\n",
    "print(f\"  y: {y_delay.shape}\")\n",
    "print(f\"\\nUsed features ({len(used_features)}):\")\n",
    "for i, feat in enumerate(used_features, 1):\n",
    "    print(f\"  {i}. {feat}\")\n",
    "\n",
    "if feature_group_info:\n",
    "    print(f\"\\n=== Feature Group Info ===\")\n",
    "    for group_name, info in feature_group_info.items():\n",
    "        print(f\"{group_name}: {info['features']} (indices {info['start_idx']}:{info['end_idx']})\")\n",
    "\n",
    "# ç‰¹å¾´é‡æ•°ãŒ17ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª\n",
    "if X_delay.shape[2] != 17:\n",
    "    print(f\"\\nâš ï¸ WARNING: Expected 17 features, but got {X_delay.shape[2]}\")\n",
    "    print(\"This will cause shape mismatch with the model!\")\n",
    "else:\n",
    "    print(f\"\\nâœ“ Confirmed: {X_delay.shape[2]} features match model requirement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2a7634",
   "metadata": {},
   "source": [
    "## 5. ãƒ‡ãƒ¼ã‚¿åˆ†å‰²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c9f95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿åˆ†å‰²\n",
    "print(\"Splitting data...\")\n",
    "splitter = DataSplitter()\n",
    "\n",
    "X_train, X_test, y_train, y_test, train_routes, test_routes = splitter.train_test_split_by_route_direction(\n",
    "    X_delay, y_delay, route_direction_info, train_ratio=0.9\n",
    ")\n",
    "standardizer = DataStandardizer()\n",
    "X_train_scaled = standardizer.fit_transform_features(X_train)\n",
    "X_test_scaled = standardizer.transform_features(X_test)\n",
    "\n",
    "actual_feature_count = X_train.shape[2]\n",
    "X_train_reshaped = splitter.reshape_for_convlstm(\n",
    "    X_train_scaled, target_height=1, target_width=actual_feature_count\n",
    ")\n",
    "X_test_reshaped = splitter.reshape_for_convlstm(\n",
    "    X_test_scaled, target_height=1, target_width=actual_feature_count\n",
    ") \n",
    "print(f\"\\nData split:\")\n",
    "print(f\"  Train: X={X_train_reshaped.shape}, y={y_train.shape}\")\n",
    "print(f\"  Test:  X={X_test_reshaped.shape}, y={y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27691a1c",
   "metadata": {},
   "source": [
    "## 6. ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿\n",
    "\n",
    "å­¦ç¿’æ¸ˆã¿ã®ConvLSTMãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3184586e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿\n",
    "# æ­£ã—ã„ãƒ¢ãƒ‡ãƒ«: delay_prediction_final_region.h5 (17ç‰¹å¾´é‡)\n",
    "model_path = '/app/files/model/delay_prediction_final_region.h5'\n",
    "\n",
    "print(f\"Loading model from {model_path}...\")\n",
    "model = tf.keras.models.load_model(model_path, compile=False)\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«æƒ…å ±è¡¨ç¤º\n",
    "print(\"\\nModel loaded successfully!\")\n",
    "print(f\"Input shape: {model.input.shape}\")\n",
    "print(f\"  â†’ Expected: (batch, 8 timesteps, 1 height, 17 width/features, 1 channels)\")\n",
    "print(f\"Output shape: {model.output.shape}\")\n",
    "print(f\"Total parameters: {model.count_params():,}\")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶ãŒä¸€è‡´ã™ã‚‹ã‹ç¢ºèª\n",
    "expected_width = model.input.shape[3]\n",
    "actual_width = X_train_reshaped.shape[3]\n",
    "print(f\"\\n=== Shape Compatibility Check ===\")\n",
    "print(f\"Model expects: width={expected_width} features\")\n",
    "print(f\"Data provides: width={actual_width} features\")\n",
    "\n",
    "if expected_width != actual_width:\n",
    "    raise ValueError(\n",
    "        f\"Shape mismatch! Model expects {expected_width} features, \"\n",
    "        f\"but data has {actual_width} features. \"\n",
    "        f\"Check feature_groups definition.\"\n",
    "    )\n",
    "\n",
    "print(\"âœ“ Shape compatibility confirmed!\")\n",
    "\n",
    "# åŸºæœ¬æ€§èƒ½è©•ä¾¡\n",
    "print(\"\\nEvaluating baseline performance...\")\n",
    "y_pred = model.predict(X_test_reshaped, verbose=0)\n",
    "\n",
    "# æœ€å¾Œã®ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã§è©•ä¾¡\n",
    "y_test_last = y_test[:, -1] if len(y_test.shape) > 1 else y_test\n",
    "y_pred_last = y_pred[:, -1] if len(y_pred.shape) > 1 else y_pred\n",
    "\n",
    "baseline_mae = mean_absolute_error(y_test_last, y_pred_last)\n",
    "baseline_rmse = np.sqrt(mean_squared_error(y_test_last, y_pred_last))\n",
    "baseline_r2 = r2_score(y_test_last, y_pred_last)\n",
    "\n",
    "print(f\"\\nBaseline Performance (last timestep):\")\n",
    "print(f\"  MAE:  {baseline_mae:.4f} seconds ({baseline_mae/60:.2f} minutes)\")\n",
    "print(f\"  RMSE: {baseline_rmse:.4f} seconds ({baseline_rmse/60:.2f} minutes)\")\n",
    "print(f\"  RÂ²:   {baseline_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54089e2a",
   "metadata": {},
   "source": [
    "## 7. ã‚«ã‚¹ã‚¿ãƒ Permutation Importanceã®å®Ÿè£…\n",
    "\n",
    "**é‡è¦**: æ—¢å­˜ã®å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã¯å…¥åŠ›å½¢çŠ¶ãŒå›ºå®šã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€ç‰¹å¾´é‡ã‚’å‰Šé™¤/è¿½åŠ ã™ã‚‹ã¨ä½¿ç”¨ã§ãã¾ã›ã‚“ã€‚\n",
    "ãã®ãŸã‚ã€ä»¥ä¸‹ã®æˆ¦ç•¥ã‚’æ¡ç”¨ã—ã¾ã™ï¼š\n",
    "\n",
    "### æˆ¦ç•¥\n",
    "1. **4æ¬¡å…ƒå…¥åŠ›ã‚’ä¿æŒ**: ConvLSTMã®å…¥åŠ›å½¢çŠ¶ (samples, timesteps, height, width) ã‚’ãã®ã¾ã¾ä½¿ç”¨\n",
    "2. **ç‰¹å¾´é‡ãƒ¬ãƒ™ãƒ«ã§ã‚·ãƒ£ãƒƒãƒ•ãƒ«**: widthæ¬¡å…ƒå†…ã®å„ç‰¹å¾´é‡ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«\n",
    "3. **å…¨ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã«é©ç”¨**: æ™‚ç³»åˆ—ã®é€£ç¶šæ€§ã‚’ç¶­æŒã—ãªãŒã‚‰ç‰¹å¾´é‡ã®å¯„ä¸ã‚’æ¸¬å®š\n",
    "\n",
    "### åˆ¶é™äº‹é …\n",
    "- ãƒ¢ãƒ‡ãƒ«æ§‹é€ ã®å¤‰æ›´ã¯ä¸å¯ï¼ˆå†å­¦ç¿’ãŒå¿…è¦ï¼‰\n",
    "- ç‰¹å¾´é‡ã®è¿½åŠ /å‰Šé™¤ã¯ä¸å¯ï¼ˆå†å­¦ç¿’ãŒå¿…è¦ï¼‰\n",
    "- ç‰¹å¾´é‡ã®é‡è¦åº¦ã®ã¿ã‚’åˆ†æå¯èƒ½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062a3890",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_permutation_importance_convlstm(model, X, y, feature_group_info, n_repeats=10, random_state=42):\n",
    "    \"\"\"\n",
    "    ConvLSTMç”¨ã®ã‚«ã‚¹ã‚¿ãƒ Permutation Importance\n",
    "    \n",
    "    Args:\n",
    "        model: å­¦ç¿’æ¸ˆã¿ConvLSTMãƒ¢ãƒ‡ãƒ«\n",
    "        X: 4æ¬¡å…ƒå…¥åŠ› (samples, timesteps, height, width)\n",
    "        y: ç›®æ¨™å€¤ (samples, output_timesteps)\n",
    "        feature_group_info: ç‰¹å¾´é‡ã‚°ãƒ«ãƒ¼ãƒ—æƒ…å ±\n",
    "        n_repeats: ã‚·ãƒ£ãƒƒãƒ•ãƒ«å›æ•°\n",
    "        random_state: ä¹±æ•°ã‚·ãƒ¼ãƒ‰\n",
    "    \n",
    "    Returns:\n",
    "        importance_dict: {feature_name: {'mean': float, 'std': float}}\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ€§èƒ½ã‚’è¨ˆç®—\n",
    "    y_pred_baseline = model.predict(X, verbose=0)\n",
    "    y_true_last = y[:, -1] if len(y.shape) > 1 else y\n",
    "    y_pred_last = y_pred_baseline[:, -1] if len(y_pred_baseline.shape) > 1 else y_pred_baseline\n",
    "    baseline_mae = mean_absolute_error(y_true_last, y_pred_last)\n",
    "    \n",
    "    print(f\"Baseline MAE: {baseline_mae:.4f} seconds\")\n",
    "    print(f\"\\nCalculating feature importance...\")\n",
    "    \n",
    "    # ç‰¹å¾´é‡åã¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒãƒƒãƒ”ãƒ³ã‚°\n",
    "    feature_indices = {}\n",
    "    for group_name, info in feature_group_info.items():\n",
    "        for i, feat in enumerate(info['features']):\n",
    "            if feat != 'arrival_delay':  # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã¯é™¤å¤–\n",
    "                width_idx = info['start_idx'] + i\n",
    "                feature_indices[feat] = width_idx\n",
    "    \n",
    "    importance_scores = {}\n",
    "    \n",
    "    # å„ç‰¹å¾´é‡ã«ã¤ã„ã¦\n",
    "    for feat_name, width_idx in feature_indices.items():\n",
    "        print(f\"  Processing: {feat_name} (index {width_idx})...\")\n",
    "        mae_increases = []\n",
    "        \n",
    "        # n_repeatså›ã‚·ãƒ£ãƒƒãƒ•ãƒ«\n",
    "        for repeat in range(n_repeats):\n",
    "            # ãƒ‡ãƒ¼ã‚¿ã®ã‚³ãƒ”ãƒ¼\n",
    "            X_shuffled = X.copy()\n",
    "            \n",
    "            # ã“ã®ç‰¹å¾´é‡ã ã‘ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«ï¼ˆå…¨ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã€å…¨ã‚µãƒ³ãƒ—ãƒ«ï¼‰\n",
    "            # widthæ¬¡å…ƒã®ç‰¹å®šã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«\n",
    "            shuffle_idx = np.random.permutation(X.shape[0])\n",
    "            X_shuffled[:, :, :, width_idx] = X_shuffled[shuffle_idx, :, :, width_idx]\n",
    "            \n",
    "            # ã‚·ãƒ£ãƒƒãƒ•ãƒ«å¾Œã®äºˆæ¸¬\n",
    "            y_pred_shuffled = model.predict(X_shuffled, verbose=0)\n",
    "            y_pred_shuffled_last = y_pred_shuffled[:, -1] if len(y_pred_shuffled.shape) > 1 else y_pred_shuffled\n",
    "            \n",
    "            # MAEå¢—åŠ é‡ã‚’è¨ˆç®—\n",
    "            shuffled_mae = mean_absolute_error(y_true_last, y_pred_shuffled_last)\n",
    "            mae_increase = shuffled_mae - baseline_mae\n",
    "            mae_increases.append(mae_increase)\n",
    "        \n",
    "        # çµ±è¨ˆé‡ã‚’ä¿å­˜\n",
    "        importance_scores[feat_name] = {\n",
    "            'mean': np.mean(mae_increases),\n",
    "            'std': np.std(mae_increases),\n",
    "            'raw_scores': mae_increases\n",
    "        }\n",
    "    \n",
    "    return importance_scores, baseline_mae\n",
    "\n",
    "# ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’åˆ¶é™ï¼ˆè¨ˆç®—æ™‚é–“çŸ­ç¸®ï¼‰\n",
    "n_samples = min(500, len(X_test))  # 500ã‚µãƒ³ãƒ—ãƒ«ã§ç´„10-15åˆ†\n",
    "X_sample = X_train_reshaped[:n_samples].copy()\n",
    "y_sample = y_test[:n_samples].copy()\n",
    "\n",
    "print(f\"Using {n_samples} samples for permutation importance\")\n",
    "print(f\"Data shape: X={X_sample.shape}, y={y_sample.shape}\")\n",
    "\n",
    "# ã‚«ã‚¹ã‚¿ãƒ Permutation Importanceå®Ÿè¡Œ\n",
    "importance_scores, baseline_mae = custom_permutation_importance_convlstm(\n",
    "    model, X_sample, y_sample, feature_group_info, n_repeats=10, random_state=42\n",
    ")\n",
    "\n",
    "print(\"\\nPermutation importance calculation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c511ed",
   "metadata": {},
   "source": [
    "## 8. ç‰¹å¾´é‡é‡è¦åº¦ã®é›†è¨ˆã¨åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d22c2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# é‡è¦åº¦ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ\n",
    "importance_data = []\n",
    "for feat_name, scores in importance_scores.items():\n",
    "    importance_data.append({\n",
    "        'feature': feat_name,\n",
    "        'importance_mean': scores['mean'],\n",
    "        'importance_std': scores['std']\n",
    "    })\n",
    "\n",
    "importance_df = pd.DataFrame(importance_data)\n",
    "\n",
    "# ã‚«ãƒ†ã‚´ãƒªã‚’è¿½åŠ \n",
    "def assign_category(feature_name):\n",
    "    for category, features in model_features.items():\n",
    "        if feature_name in features:\n",
    "            return category\n",
    "    return 'other'\n",
    "\n",
    "importance_df['category'] = importance_df['feature'].apply(assign_category)\n",
    "\n",
    "# é‡è¦åº¦ã§ã‚½ãƒ¼ãƒˆ\n",
    "importance_df = importance_df.sort_values('importance_mean', ascending=False)\n",
    "\n",
    "print(\"Feature Importance (Top 20):\")\n",
    "print(importance_df.head(20).to_string(index=False))\n",
    "\n",
    "# ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®çµ±è¨ˆ\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Category-wise Importance Summary:\")\n",
    "print(\"=\"*60)\n",
    "category_stats = importance_df.groupby('category').agg({\n",
    "    'importance_mean': ['sum', 'mean', 'count']\n",
    "}).round(4)\n",
    "category_stats.columns = ['total_importance', 'avg_importance', 'n_features']\n",
    "category_stats = category_stats.sort_values('total_importance', ascending=False)\n",
    "print(category_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871e8a88",
   "metadata": {},
   "source": [
    "## 9. å¯è¦–åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e771cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚«ãƒ©ãƒ¼ãƒãƒƒãƒ—ã®å®šç¾©\n",
    "category_colors = {\n",
    "    'temporal': '#FF6B6B',\n",
    "    'location': '#4ECDC4',\n",
    "    'statistical': '#45B7D1',\n",
    "    'climate': '#FFA07A'\n",
    "}\n",
    "\n",
    "# 9.1 Top 20ç‰¹å¾´é‡ã®é‡è¦åº¦ï¼ˆã‚«ãƒ†ã‚´ãƒªåˆ¥è‰²åˆ†ã‘ï¼‰\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "top_n = 20\n",
    "plot_data = importance_df.head(top_n).sort_values('importance_mean')\n",
    "\n",
    "colors = [category_colors.get(cat, '#95A5A6') for cat in plot_data['category']]\n",
    "\n",
    "bars = ax.barh(range(len(plot_data)), plot_data['importance_mean'], color=colors, alpha=0.7)\n",
    "ax.set_yticks(range(len(plot_data)))\n",
    "ax.set_yticklabels(plot_data['feature'])\n",
    "ax.set_xlabel('Importance (MAE Increase)', fontsize=12)\n",
    "ax.set_title(f'Top {top_n} Feature Importance\\n(Permutation Importance)', fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# å‡¡ä¾‹\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor=color, label=cat.capitalize(), alpha=0.7) \n",
    "                   for cat, color in category_colors.items()]\n",
    "ax.legend(handles=legend_elements, loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/app/notebook/feature_importance_top20.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Chart saved: feature_importance_top20.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffa81b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.2 ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®é‡è¦åº¦ï¼ˆåˆè¨ˆã¨å¹³å‡ï¼‰\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# å·¦: ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®åˆè¨ˆé‡è¦åº¦\n",
    "category_total = importance_df.groupby('category')['importance_mean'].sum().sort_values(ascending=False)\n",
    "colors_total = [category_colors.get(cat, '#95A5A6') for cat in category_total.index]\n",
    "axes[0].bar(range(len(category_total)), category_total.values, color=colors_total, alpha=0.7)\n",
    "axes[0].set_xticks(range(len(category_total)))\n",
    "axes[0].set_xticklabels([cat.capitalize() for cat in category_total.index], rotation=45)\n",
    "axes[0].set_ylabel('Total Importance', fontsize=12)\n",
    "axes[0].set_title('Total Importance by Category', fontsize=13, fontweight='bold')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# å³: ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®å¹³å‡é‡è¦åº¦\n",
    "category_mean = importance_df.groupby('category')['importance_mean'].mean().sort_values(ascending=False)\n",
    "colors_mean = [category_colors.get(cat, '#95A5A6') for cat in category_mean.index]\n",
    "axes[1].bar(range(len(category_mean)), category_mean.values, color=colors_mean, alpha=0.7)\n",
    "axes[1].set_xticks(range(len(category_mean)))\n",
    "axes[1].set_xticklabels([cat.capitalize() for cat in category_mean.index], rotation=45)\n",
    "axes[1].set_ylabel('Average Importance', fontsize=12)\n",
    "axes[1].set_title('Average Importance by Category', fontsize=13, fontweight='bold')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/app/notebook/category_importance_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Chart saved: category_importance_comparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c32c1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.3 ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®è©³ç´°ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "\n",
    "# ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã¦ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ä½œæˆ\n",
    "pivot_data = importance_df.pivot_table(\n",
    "    index='feature',\n",
    "    columns='category',\n",
    "    values='importance_mean',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "# ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ä½œæˆ\n",
    "sns.heatmap(\n",
    "    pivot_data.T,\n",
    "    annot=False,\n",
    "    cmap='YlOrRd',\n",
    "    cbar_kws={'label': 'Importance'},\n",
    "    linewidths=0.5,\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "ax.set_title('Feature Importance Heatmap by Category', fontsize=14, fontweight='bold', pad=20)\n",
    "ax.set_xlabel('Features', fontsize=12)\n",
    "ax.set_ylabel('Categories', fontsize=12)\n",
    "plt.xticks(rotation=90, fontsize=8)\n",
    "plt.yticks(rotation=0, fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/app/notebook/feature_importance_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Chart saved: feature_importance_heatmap.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b73b634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.4 ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®ç‰¹å¾´é‡æ•°ã¨é‡è¦åº¦ã®é–¢ä¿‚\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "category_summary = importance_df.groupby('category').agg({\n",
    "    'importance_mean': ['sum', 'count']\n",
    "}).reset_index()\n",
    "category_summary.columns = ['category', 'total_importance', 'n_features']\n",
    "\n",
    "colors_scatter = [category_colors.get(cat, '#95A5A6') for cat in category_summary['category']]\n",
    "\n",
    "scatter = ax.scatter(\n",
    "    category_summary['n_features'],\n",
    "    category_summary['total_importance'],\n",
    "    s=300,\n",
    "    c=colors_scatter,\n",
    "    alpha=0.6,\n",
    "    edgecolors='black',\n",
    "    linewidth=1.5\n",
    ")\n",
    "\n",
    "# ãƒ©ãƒ™ãƒ«è¿½åŠ \n",
    "for idx, row in category_summary.iterrows():\n",
    "    ax.annotate(\n",
    "        row['category'].capitalize(),\n",
    "        (row['n_features'], row['total_importance']),\n",
    "        xytext=(5, 5),\n",
    "        textcoords='offset points',\n",
    "        fontsize=11,\n",
    "        fontweight='bold'\n",
    "    )\n",
    "\n",
    "ax.set_xlabel('Number of Features', fontsize=12)\n",
    "ax.set_ylabel('Total Importance', fontsize=12)\n",
    "ax.set_title('Feature Count vs Total Importance by Category', fontsize=14, fontweight='bold')\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/app/notebook/category_count_vs_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Chart saved: category_count_vs_importance.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d7b267",
   "metadata": {},
   "source": [
    "## 10. çµæœã®ä¿å­˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ac114e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.1 ç‰¹å¾´é‡é‡è¦åº¦ã®è©³ç´°ã‚’CSVä¿å­˜\n",
    "output_dir = '/app/notebook'\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# è©³ç´°CSV\n",
    "csv_path = os.path.join(output_dir, f'feature_importance_detailed_{timestamp}.csv')\n",
    "importance_df.to_csv(csv_path, index=False)\n",
    "print(f\"Detailed importance saved to: {csv_path}\")\n",
    "\n",
    "# ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚µãƒãƒªãƒ¼CSV\n",
    "category_csv_path = os.path.join(output_dir, f'category_importance_summary_{timestamp}.csv')\n",
    "category_stats.to_csv(category_csv_path)\n",
    "print(f\"Category summary saved to: {category_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7758a353",
   "metadata": {},
   "source": [
    "## 11. åˆ†æã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0075d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ\n",
    "report = f\"\"\"\n",
    "# ç‰¹å¾´é‡é‡è¦åº¦åˆ†æãƒ¬ãƒãƒ¼ãƒˆ\n",
    "\n",
    "**åˆ†ææ—¥æ™‚**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "**ãƒ¢ãƒ‡ãƒ«**: Bidirectional ConvLSTM\n",
    "**æ‰‹æ³•**: Permutation Importance (n_repeats=10)\n",
    "**è©•ä¾¡æŒ‡æ¨™**: MAE (Mean Absolute Error)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. åŸºæœ¬æƒ…å ±\n",
    "\n",
    "- **ç·ç‰¹å¾´é‡æ•°**: {len(importance_df)}\n",
    "- **åˆ†æã‚µãƒ³ãƒ—ãƒ«æ•°**: {n_samples}\n",
    "- **ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³MAE**: {baseline_mae:.4f} ç§’ ({baseline_mae/60:.2f} åˆ†)\n",
    "- **ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³RMSE**: {baseline_rmse:.4f} ç§’ ({baseline_rmse/60:.2f} åˆ†)\n",
    "- **ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³RÂ²**: {baseline_r2:.4f}\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Top 10 æœ€é‡è¦ç‰¹å¾´é‡\n",
    "\n",
    "| Rank | Feature | Category | Importance | Std |\n",
    "|------|---------|----------|------------|----- |\n",
    "\"\"\"\n",
    "\n",
    "for idx, row in importance_df.head(10).iterrows():\n",
    "    report += f\"| {idx+1} | {row['feature']} | {row['category']} | {row['importance_mean']:.6f} | {row['importance_std']:.6f} |\\n\"\n",
    "\n",
    "report += f\"\"\"\n",
    "\n",
    "---\n",
    "\n",
    "## 3. ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æ\n",
    "\n",
    "| Category | Total Importance | Avg Importance | N Features |\n",
    "|----------|------------------|----------------|------------|\n",
    "\"\"\"\n",
    "\n",
    "for cat, row in category_stats.iterrows():\n",
    "    report += f\"| {cat.capitalize()} | {row['total_importance']:.6f} | {row['avg_importance']:.6f} | {int(row['n_features'])} |\\n\"\n",
    "\n",
    "report += f\"\"\"\n",
    "\n",
    "---\n",
    "\n",
    "## 4. ä¸»è¦ãªç™ºè¦‹\n",
    "\n",
    "### 4.1 ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®é‡è¦åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°\n",
    "\n",
    "1. **{category_stats.index[0].capitalize()}**: åˆè¨ˆé‡è¦åº¦ {category_stats.iloc[0]['total_importance']:.4f}\n",
    "2. **{category_stats.index[1].capitalize()}**: åˆè¨ˆé‡è¦åº¦ {category_stats.iloc[1]['total_importance']:.4f}\n",
    "3. **{category_stats.index[2].capitalize()}**: åˆè¨ˆé‡è¦åº¦ {category_stats.iloc[2]['total_importance']:.4f}\n",
    "4. **{category_stats.index[3].capitalize()}**: åˆè¨ˆé‡è¦åº¦ {category_stats.iloc[3]['total_importance']:.4f}\n",
    "\n",
    "### 4.2 æœ€ã‚‚é‡è¦ãªå˜ä¸€ç‰¹å¾´é‡\n",
    "\n",
    "**{importance_df.iloc[0]['feature']}** ({importance_df.iloc[0]['category']}ã‚«ãƒ†ã‚´ãƒª)\n",
    "- é‡è¦åº¦: {importance_df.iloc[0]['importance_mean']:.6f}\n",
    "- ã“ã®ç‰¹å¾´é‡ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã™ã‚‹ã¨MAEãŒç´„ {importance_df.iloc[0]['importance_mean']:.2f}ç§’å¢—åŠ \n",
    "\n",
    "### 4.3 æ¨å¥¨äº‹é …\n",
    "\n",
    "1. **é‡è¦ç‰¹å¾´é‡ã®ä¿æŒ**: Top 20ç‰¹å¾´é‡ã¯å¿…ãšä¿æŒã™ã‚‹ã“ã¨\n",
    "2. **ä½é‡è¦åº¦ç‰¹å¾´é‡ã®å‰Šé™¤å€™è£œ**: é‡è¦åº¦ãŒ0.001æœªæº€ã®ç‰¹å¾´é‡ã¯å‰Šé™¤ã‚’æ¤œè¨\n",
    "3. **ã‚«ãƒ†ã‚´ãƒªãƒãƒ©ãƒ³ã‚¹**: æœ€ã‚‚é‡è¦ãªã‚«ãƒ†ã‚´ãƒªï¼ˆ{category_stats.index[0]}ï¼‰ã‹ã‚‰ã®ç‰¹å¾´é‡ã‚’å„ªå…ˆçš„ã«ä¿æŒ\n",
    "\n",
    "---\n",
    "\n",
    "## 5. æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—\n",
    "\n",
    "- [ ] ä½é‡è¦åº¦ç‰¹å¾´é‡ï¼ˆä¸‹ä½20%ï¼‰ã‚’é™¤å¤–ã—ãŸãƒ¢ãƒ‡ãƒ«ã§æ€§èƒ½è©•ä¾¡\n",
    "- [ ] Top 30ç‰¹å¾´é‡ã®ã¿ã§ãƒ¢ãƒ‡ãƒ«ã‚’å†å­¦ç¿’\n",
    "- [ ] ç›¸é–¢åˆ†æã«ã‚ˆã‚Šå†—é•·ãªç‰¹å¾´é‡ã‚’ç‰¹å®š\n",
    "- [ ] æ–°è¦ç‰¹å¾´é‡ã®è¿½åŠ æ¤œè¨ï¼ˆç§»å‹•å¹³å‡ã€äº¤äº’ä½œç”¨é …ãªã©ï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "**ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«**:\n",
    "- `feature_importance_top20.png`\n",
    "- `category_importance_comparison.png`\n",
    "- `feature_importance_heatmap.png`\n",
    "- `category_count_vs_importance.png`\n",
    "- `feature_importance_detailed_{timestamp}.csv`\n",
    "- `category_importance_summary_{timestamp}.csv`\n",
    "\"\"\"\n",
    "\n",
    "# ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜\n",
    "report_path = os.path.join(output_dir, f'feature_importance_report_{timestamp}.md')\n",
    "with open(report_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(f\"\\nReport saved to: {report_path}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(report)\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9333efef",
   "metadata": {},
   "source": [
    "## 12. ã¾ã¨ã‚\n",
    "\n",
    "ã“ã®åˆ†æã«ã‚ˆã‚Šã€ä»¥ä¸‹ãŒæ˜ã‚‰ã‹ã«ãªã‚Šã¾ã—ãŸï¼š\n",
    "\n",
    "1. **ç‰¹å¾´é‡ã®é‡è¦åº¦**: å…¨ç‰¹å¾´é‡ã®å®šé‡çš„ãªé‡è¦åº¦ã‚¹ã‚³ã‚¢ã‚’å–å¾—\n",
    "2. **ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®å¯„ä¸**: ã©ã®ã‚«ãƒ†ã‚´ãƒªï¼ˆæ™‚é–“/ä½ç½®/çµ±è¨ˆ/æ°—å€™ï¼‰ãŒæœ€ã‚‚äºˆæ¸¬ã«è²¢çŒ®ã—ã¦ã„ã‚‹ã‹\n",
    "3. **å‰Šæ¸›å€™è£œ**: æ€§èƒ½ã«å¤§ããå½±éŸ¿ã—ãªã„ä½é‡è¦åº¦ç‰¹å¾´é‡ã®ç‰¹å®š\n",
    "\n",
    "æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã¨ã—ã¦ã€ã“ã®åˆ†æçµæœã‚’åŸºã«ï¼š\n",
    "- ç‰¹å¾´é‡é¸æŠã®æœ€é©åŒ–ï¼ˆTODO 1.2-1.3ï¼‰\n",
    "- æ–°è¦ç‰¹å¾´é‡ã®è¿½åŠ ï¼ˆTODO 1.4ï¼‰\n",
    "\n",
    "ã‚’å®Ÿæ–½ã™ã‚‹ã“ã¨ã§ã€ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½å‘ä¸Šã¨æ¨è«–é€Ÿåº¦æ”¹å–„ãŒæœŸå¾…ã§ãã¾ã™ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
