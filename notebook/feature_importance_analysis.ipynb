{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a97f784",
   "metadata": {},
   "source": [
    "# ç‰¹å¾´é‡é‡è¦åº¦åˆ†æ (Feature Importance Analysis)\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€ãƒã‚¹é…å»¶äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã§ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ç‰¹å¾´é‡ã®é‡è¦åº¦ã‚’åˆ†æã—ã¾ã™ã€‚\n",
    "\n",
    "## âš ï¸ é‡è¦ãªåˆ¶ç´„äº‹é …\n",
    "\n",
    "**æ—¢å­˜ã®å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã¯å…¥åŠ›å½¢çŠ¶ãŒå›ºå®šã•ã‚Œã¦ã„ã¾ã™**:\n",
    "- ãƒ¢ãƒ‡ãƒ«: `delay_prediction_final_region.h5`\n",
    "- ãƒ¢ãƒ‡ãƒ«å…¥åŠ›: `(batch, 8, 1, 17, 1)` â†’ **17å€‹ã®ç‰¹å¾´é‡ã§å›ºå®š**\n",
    "- ç‰¹å¾´é‡ã‚’è¿½åŠ /å‰Šé™¤ã™ã‚‹ã¨ã€ãƒ¢ãƒ‡ãƒ«ãŒä½¿ç”¨ã§ããªããªã‚Šã¾ã™\n",
    "- **ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯**: æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã‚’ä¿æŒã—ãŸã¾ã¾ã€ç‰¹å¾´é‡ã®é‡è¦åº¦ã®ã¿ã‚’åˆ†æã—ã¾ã™\n",
    "\n",
    "## ğŸ“Š åˆ†æç›®çš„\n",
    "\n",
    "- å…¨ç‰¹å¾´é‡ã®é‡è¦åº¦ã‚’å®šé‡çš„ã«è©•ä¾¡ï¼ˆPermutation Importanceï¼‰\n",
    "- ç‰¹å¾´é‡ã‚’æ™‚é–“/ä½ç½®/çµ±è¨ˆ/æ°—å€™ã®4ã‚«ãƒ†ã‚´ãƒªã«åˆ†é¡\n",
    "- ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®é‡è¦åº¦ã‚’å¯è¦–åŒ–\n",
    "- ãƒ¢ãƒ‡ãƒ«æ€§èƒ½å‘ä¸Šã¨æ¨è«–é€Ÿåº¦æ”¹å–„ã®ãŸã‚ã®ç‰¹å¾´é‡é¸æŠã®åŸºç¤ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ\n",
    "\n",
    "## ğŸ”¬ ä½¿ç”¨ã™ã‚‹æ‰‹æ³•\n",
    "\n",
    "- **Custom Permutation Importance for ConvLSTM**: \n",
    "  - 4æ¬¡å…ƒå…¥åŠ›æ§‹é€ ã‚’ä¿æŒ `(batch, timesteps, height, width, channels)`\n",
    "  - å„ç‰¹å¾´é‡ï¼ˆwidthæ¬¡å…ƒã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼‰ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«\n",
    "  - MAEå¢—åŠ é‡ã§é‡è¦åº¦ã‚’æ¸¬å®š\n",
    "- **ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æ**: ç‰¹å¾´é‡ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã®é‡è¦åº¦é›†è¨ˆ\n",
    "\n",
    "## ğŸ“¦ æœŸå¾…ã•ã‚Œã‚‹æˆæœç‰©\n",
    "\n",
    "1. ç‰¹å¾´é‡é‡è¦åº¦ã‚¹ã‚³ã‚¢ï¼ˆCSVå½¢å¼ï¼‰\n",
    "2. å¯è¦–åŒ–ã‚°ãƒ©ãƒ•ï¼ˆTop-Nç‰¹å¾´é‡ã€ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æï¼‰\n",
    "3. åˆ†æã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆ\n",
    "4. å‰Šæ¸›å€™è£œãƒªã‚¹ãƒˆï¼ˆä½é‡è¦åº¦ç‰¹å¾´é‡ï¼‰\n",
    "\n",
    "## ğŸ”„ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆTODO 1.2-1.4ï¼‰\n",
    "\n",
    "ã“ã®åˆ†æçµæœã‚’åŸºã«:\n",
    "- ç‰¹å¾´é‡å‰Šæ¸›å®Ÿé¨“ï¼ˆãƒ¢ãƒ‡ãƒ«å†å­¦ç¿’ãŒå¿…è¦ï¼‰\n",
    "- æ–°è¦ç‰¹å¾´é‡ã®è¿½åŠ å®Ÿé¨“\n",
    "- è©³ç´°ã¯ `doc/FEATURE_IMPORTANCE_STRATEGY.md` ã‚’å‚ç…§"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43328a4a",
   "metadata": {},
   "source": [
    "## 1. ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5d4d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "from pathlib import Path\n",
    "\n",
    "rootPath = Path.cwd().parent\n",
    "sys.path.append(str(rootPath))\n",
    "from src.timeseries_processing import SequenceCreator, DataSplitter, DataStandardizer\n",
    "from src.model_training import DelayPredictionModel\n",
    "\n",
    "# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®šï¼ˆå¯è¦–åŒ–ç”¨ï¼‰\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Analysis started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "data_path = f'{rootPath}/data/merged_dataset_20251014_182958.csv'\n",
    "print(f\"Loading data from {data_path}...\")\n",
    "model_path = f'{rootPath}/files/model/best_delay_model_20251013_202104.h5'\n",
    "print(f\"Loading model from {model_path}...\")\n",
    "# ConvLSTMç”¨ã®ç‰¹å¾´é‡ã‚°ãƒ«ãƒ¼ãƒ—å®šç¾©\n",
    "# ãƒ¢ãƒ‡ãƒ«å­¦ç¿’æ™‚ã¨åŒã˜å®šç¾©ï¼ˆbus_arrival_forecast_model.ipynbã‹ã‚‰ï¼‰\n",
    "feature_groups = {\n",
    "    'temporal': ['hour_sin', 'hour_cos', 'day_sin', 'day_cos', 'is_peak_hour', 'is_weekend', 'arrival_delay'],\n",
    "    'region': ['direction_id', 'line_direction_link_order', 'distance_from_downtown_km', 'area_density_score'],\n",
    "    'weather': ['humidex', 'wind_speed', 'weather_rainy'],\n",
    "    'target': ['arrival_delay']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b847131e",
   "metadata": {},
   "source": [
    "## 2. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8d22e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿\n",
    "print(\"Loading dataset...\")\n",
    "delay_features = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"Dataset shape: {delay_features.shape}\")\n",
    "print(f\"\\nColumn names ({len(delay_features.columns)} features):\")\n",
    "print(delay_features.columns.tolist())\n",
    "\n",
    "# åŸºæœ¬çµ±è¨ˆ\n",
    "print(f\"\\nBasic statistics:\")\n",
    "print(f\"- Routes: {delay_features['route_id'].nunique()}\")\n",
    "print(f\"- Date range: {delay_features['time_bucket'].min()} to {delay_features['time_bucket'].max()}\")\n",
    "print(f\"- Mean delay: {delay_features['arrival_delay'].mean():.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b415db",
   "metadata": {},
   "source": [
    "## 4. æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ed01e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# æ™‚ç³»åˆ—ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä½œæˆ\n",
    "print(\"Creating time series sequences...\")\n",
    "print(\"Note: feature_groups has intentional duplicate (distance_from_downtown_km appears twice)\")\n",
    "sequence_creator = SequenceCreator(\n",
    "    input_timesteps=8, \n",
    "    output_timesteps=3,\n",
    "    feature_groups=feature_groups\n",
    ")\n",
    "\n",
    "X_delay, y_delay, route_direction_info, used_features, feature_group_info = sequence_creator.create_route_direction_aware_sequences(\n",
    "    delay_features,\n",
    "    spatial_organization=True\n",
    ")\n",
    "\n",
    "print(f\"\\nSequence shapes:\")\n",
    "print(f\"  X: {X_delay.shape}\")\n",
    "print(f\"  y: {y_delay.shape}\")\n",
    "print(f\"\\nUsed features ({len(used_features)}):\")\n",
    "for i, feat in enumerate(used_features, 1):\n",
    "    print(f\"  {i}. {feat}\")\n",
    "\n",
    "if feature_group_info:\n",
    "    print(f\"\\n=== Feature Group Info ===\")\n",
    "    for group_name, info in feature_group_info.items():\n",
    "        print(f\"{group_name}: {info['features']} (indices {info['start_idx']}:{info['end_idx']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2a7634",
   "metadata": {},
   "source": [
    "## 5. ãƒ‡ãƒ¼ã‚¿åˆ†å‰²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c9f95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿åˆ†å‰²\n",
    "print(\"Splitting data...\")\n",
    "splitter = DataSplitter()\n",
    "\n",
    "X_train, X_test, y_train, y_test, train_routes, test_routes = splitter.train_test_split_by_route_direction(\n",
    "    X_delay, y_delay, route_direction_info, train_ratio=0.9\n",
    ")\n",
    "standardizer = DataStandardizer()\n",
    "X_train_scaled = standardizer.fit_transform_features(X_train)\n",
    "X_test_scaled = standardizer.transform_features(X_test)\n",
    "\n",
    "actual_feature_count = X_train.shape[2]\n",
    "X_train_reshaped = splitter.reshape_for_convlstm(\n",
    "    X_train_scaled, target_height=1, target_width=actual_feature_count\n",
    ")\n",
    "X_test_reshaped = splitter.reshape_for_convlstm(\n",
    "    X_test_scaled, target_height=1, target_width=actual_feature_count\n",
    ") \n",
    "print(f\"\\nData split:\")\n",
    "print(f\"  Train: X={X_train_reshaped.shape}, y={y_train.shape}\")\n",
    "print(f\"  Test:  X={X_test_reshaped.shape}, y={y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27691a1c",
   "metadata": {},
   "source": [
    "## 6. ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿\n",
    "\n",
    "å­¦ç¿’æ¸ˆã¿ã®ConvLSTMãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3184586e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿\n",
    "print(f\"Loading model from {model_path}...\")\n",
    "model = tf.keras.models.load_model(model_path, compile=False)\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«æƒ…å ±è¡¨ç¤º\n",
    "print(\"\\nModel loaded successfully!\")\n",
    "print(f\"Input shape: {model.input.shape}\")\n",
    "print(f\"  â†’ Expected: (batch, 8 timesteps, 1 height, 17 width/features, 1 channels)\")\n",
    "print(f\"Output shape: {model.output.shape}\")\n",
    "print(f\"Total parameters: {model.count_params():,}\")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶ãŒä¸€è‡´ã™ã‚‹ã‹ç¢ºèª\n",
    "expected_width = model.input.shape[3]\n",
    "actual_width = X_train_reshaped.shape[3]\n",
    "print(f\"\\n=== Shape Compatibility Check ===\")\n",
    "print(f\"Model expects: width={expected_width} features\")\n",
    "print(f\"Data provides: width={actual_width} features\")\n",
    "\n",
    "if expected_width != actual_width:\n",
    "    raise ValueError(\n",
    "        f\"Shape mismatch! Model expects {expected_width} features, \"\n",
    "        f\"but data has {actual_width} features. \"\n",
    "        f\"Check feature_groups definition.\"\n",
    "    )\n",
    "\n",
    "print(\"âœ“ Shape compatibility confirmed!\")\n",
    "\n",
    "# åŸºæœ¬æ€§èƒ½è©•ä¾¡\n",
    "print(\"\\nEvaluating baseline performance...\")\n",
    "y_pred = model.predict(X_test_reshaped, verbose=0)\n",
    "\n",
    "# æœ€å¾Œã®ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã§è©•ä¾¡\n",
    "y_test_last = y_test[:, -1] if len(y_test.shape) > 1 else y_test\n",
    "y_pred_last = y_pred[:, -1] if len(y_pred.shape) > 1 else y_pred\n",
    "\n",
    "baseline_mae = mean_absolute_error(y_test_last, y_pred_last)\n",
    "baseline_rmse = np.sqrt(mean_squared_error(y_test_last, y_pred_last))\n",
    "baseline_r2 = r2_score(y_test_last, y_pred_last)\n",
    "\n",
    "print(f\"\\nBaseline Performance (last timestep):\")\n",
    "print(f\"  MAE:  {baseline_mae:.4f} seconds ({baseline_mae/60:.2f} minutes)\")\n",
    "print(f\"  RMSE: {baseline_rmse:.4f} seconds ({baseline_rmse/60:.2f} minutes)\")\n",
    "print(f\"  RÂ²:   {baseline_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54089e2a",
   "metadata": {},
   "source": [
    "## 7. ã‚«ã‚¹ã‚¿ãƒ Permutation Importanceã®å®Ÿè£…\n",
    "\n",
    "**é‡è¦**: æ—¢å­˜ã®å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã¯å…¥åŠ›å½¢çŠ¶ãŒå›ºå®šã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€ç‰¹å¾´é‡ã‚’å‰Šé™¤/è¿½åŠ ã™ã‚‹ã¨ä½¿ç”¨ã§ãã¾ã›ã‚“ã€‚\n",
    "ãã®ãŸã‚ã€ä»¥ä¸‹ã®æˆ¦ç•¥ã‚’æ¡ç”¨ã—ã¾ã™ï¼š\n",
    "\n",
    "### æˆ¦ç•¥\n",
    "1. **4æ¬¡å…ƒå…¥åŠ›ã‚’ä¿æŒ**: ConvLSTMã®å…¥åŠ›å½¢çŠ¶ (samples, timesteps, height, width) ã‚’ãã®ã¾ã¾ä½¿ç”¨\n",
    "2. **ç‰¹å¾´é‡ãƒ¬ãƒ™ãƒ«ã§ã‚·ãƒ£ãƒƒãƒ•ãƒ«**: widthæ¬¡å…ƒå†…ã®å„ç‰¹å¾´é‡ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«\n",
    "3. **å…¨ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã«é©ç”¨**: æ™‚ç³»åˆ—ã®é€£ç¶šæ€§ã‚’ç¶­æŒã—ãªãŒã‚‰ç‰¹å¾´é‡ã®å¯„ä¸ã‚’æ¸¬å®š\n",
    "\n",
    "### åˆ¶é™äº‹é …\n",
    "- ãƒ¢ãƒ‡ãƒ«æ§‹é€ ã®å¤‰æ›´ã¯ä¸å¯ï¼ˆå†å­¦ç¿’ãŒå¿…è¦ï¼‰\n",
    "- ç‰¹å¾´é‡ã®è¿½åŠ /å‰Šé™¤ã¯ä¸å¯ï¼ˆå†å­¦ç¿’ãŒå¿…è¦ï¼‰\n",
    "- ç‰¹å¾´é‡ã®é‡è¦åº¦ã®ã¿ã‚’åˆ†æå¯èƒ½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062a3890",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_permutation_importance_convlstm(model, X, y, feature_group_info, \n",
    "                                           n_repeats=10, random_state=42,\n",
    "                                           shuffle_strategy='cross_sample'):\n",
    "    \"\"\"\n",
    "    ConvLSTMç”¨ã®ã‚«ã‚¹ã‚¿ãƒ Permutation Importanceï¼ˆæ™‚ç³»åˆ—å¯¾å¿œç‰ˆãƒ»å …ç‰¢åŒ–ï¼‰\n",
    "    \n",
    "    Args:\n",
    "        model: å­¦ç¿’æ¸ˆã¿ConvLSTMãƒ¢ãƒ‡ãƒ«\n",
    "        X: 4æ¬¡å…ƒå…¥åŠ› (samples, timesteps, height, width)\n",
    "        y: ç›®æ¨™å€¤ (samples, output_timesteps)\n",
    "        feature_group_info: ç‰¹å¾´é‡ã‚°ãƒ«ãƒ¼ãƒ—æƒ…å ±\n",
    "        n_repeats: ã‚·ãƒ£ãƒƒãƒ•ãƒ«å›æ•°\n",
    "        random_state: ä¹±æ•°ã‚·ãƒ¼ãƒ‰\n",
    "        shuffle_strategy: ã‚·ãƒ£ãƒƒãƒ•ãƒ«æˆ¦ç•¥\n",
    "            - 'cross_sample': ã‚µãƒ³ãƒ—ãƒ«é–“ã§ã‚·ãƒ£ãƒƒãƒ•ãƒ«ï¼ˆæ™‚ç³»åˆ—é †åºã¯ä¿æŒï¼‰ã€æ¨å¥¨ã€‘\n",
    "            - 'block': ãƒ–ãƒ­ãƒƒã‚¯å˜ä½ã§ã‚·ãƒ£ãƒƒãƒ•ãƒ«ï¼ˆå±€æ‰€æ€§ã‚’ä¿æŒï¼‰\n",
    "            - 'within_sample': ã‚µãƒ³ãƒ—ãƒ«å†…ã§ã‚·ãƒ£ãƒƒãƒ•ãƒ«ï¼ˆæ™‚ç³»åˆ—ç ´å£Šãƒ»éæ¨å¥¨ï¼‰\n",
    "    \n",
    "    Returns:\n",
    "        importance_dict: {feature_name: {'mean': float, 'std': float, 'raw_scores': list}}\n",
    "        baseline_mae: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã®MAE\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ€§èƒ½ã‚’è¨ˆç®—\n",
    "    y_pred_baseline = model.predict(X, verbose=0)\n",
    "    y_true_last = y[:, -1] if len(y.shape) > 1 else y\n",
    "    y_pred_last = y_pred_baseline[:, -1] if len(y_pred_baseline.shape) > 1 else y_pred_baseline\n",
    "    baseline_mae = mean_absolute_error(y_true_last, y_pred_last)\n",
    "    \n",
    "    print(f\"Baseline MAE: {baseline_mae:.4f} seconds\")\n",
    "    print(f\"Shuffle strategy: {shuffle_strategy}\")\n",
    "    print(f\"Sample size: {X.shape[0]}\")\n",
    "    print(f\"\\nCalculating feature importance...\")\n",
    "    \n",
    "    # ç‰¹å¾´é‡åã¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒãƒƒãƒ”ãƒ³ã‚°\n",
    "    feature_indices = {}\n",
    "    for group_name, info in feature_group_info.items():\n",
    "        for i, feat in enumerate(info['features']):\n",
    "            if feat != 'arrival_delay':  # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã¯é™¤å¤–\n",
    "                width_idx = info['start_idx'] + i\n",
    "                feature_indices[feat] = width_idx\n",
    "    \n",
    "    importance_scores = {}\n",
    "    \n",
    "    # å„ç‰¹å¾´é‡ã«ã¤ã„ã¦\n",
    "    for feat_name, width_idx in feature_indices.items():\n",
    "        print(f\"  Processing: {feat_name} (index {width_idx})...\")\n",
    "        mae_increases = []\n",
    "        \n",
    "        # n_repeatså›ã‚·ãƒ£ãƒƒãƒ•ãƒ«\n",
    "        for repeat in range(n_repeats):\n",
    "            # ãƒ‡ãƒ¼ã‚¿ã®ã‚³ãƒ”ãƒ¼\n",
    "            X_shuffled = X.copy()\n",
    "            \n",
    "            if shuffle_strategy == 'cross_sample':\n",
    "                # ã€æ¨å¥¨ã€‘ã‚µãƒ³ãƒ—ãƒ«é–“ã§ã‚·ãƒ£ãƒƒãƒ•ãƒ«ï¼ˆæ™‚ç³»åˆ—ã®é †åºã¯ä¿æŒï¼‰\n",
    "                # ä¾‹: ã‚µãƒ³ãƒ—ãƒ«Aã®ç‰¹å¾´é‡ â†’ ã‚µãƒ³ãƒ—ãƒ«Bã®æ™‚ç³»åˆ—ã«é…ç½®\n",
    "                shuffle_idx = np.random.permutation(X.shape[0])\n",
    "                # æ™‚ç³»åˆ—å…¨ä½“ã‚’ã¾ã¨ã‚ã¦ã‚·ãƒ£ãƒƒãƒ•ãƒ«ï¼ˆtimestepæ¬¡å…ƒã¯ãã®ã¾ã¾ï¼‰\n",
    "                X_shuffled[:, :, :, width_idx] = X[shuffle_idx, :, :, width_idx]\n",
    "                \n",
    "            elif shuffle_strategy == 'block':\n",
    "                # ãƒ–ãƒ­ãƒƒã‚¯å˜ä½ã§ã‚·ãƒ£ãƒƒãƒ•ãƒ«ï¼ˆæ™‚ç³»åˆ—ã®å±€æ‰€æ€§ã‚’ä¿æŒï¼‰\n",
    "                block_size = 2  # 2ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã‚’1ãƒ–ãƒ­ãƒƒã‚¯\n",
    "                n_timesteps = X.shape[1]\n",
    "                \n",
    "                for sample_idx in range(X.shape[0]):\n",
    "                    # ãƒ–ãƒ­ãƒƒã‚¯æ•°\n",
    "                    n_blocks = n_timesteps // block_size\n",
    "                    block_perm = np.random.permutation(n_blocks)\n",
    "                    \n",
    "                    X_temp = X_shuffled[sample_idx, :, :, width_idx].copy()\n",
    "                    for new_pos, old_pos in enumerate(block_perm):\n",
    "                        start_new = new_pos * block_size\n",
    "                        end_new = min(start_new + block_size, n_timesteps)\n",
    "                        start_old = old_pos * block_size\n",
    "                        end_old = min(start_old + block_size, n_timesteps)\n",
    "                        X_shuffled[sample_idx, start_new:end_new, :, width_idx] = X_temp[start_old:end_old, :]\n",
    "                        \n",
    "            elif shuffle_strategy == 'within_sample':\n",
    "                # ã‚µãƒ³ãƒ—ãƒ«å†…ã§ã‚·ãƒ£ãƒƒãƒ•ãƒ«ï¼ˆæ™‚ç³»åˆ—ç ´å£Šãƒ»éæ¨å¥¨ï¼‰\n",
    "                for sample_idx in range(X.shape[0]):\n",
    "                    timestep_perm = np.random.permutation(X.shape[1])\n",
    "                    X_shuffled[sample_idx, :, :, width_idx] = X_shuffled[sample_idx, timestep_perm, :, width_idx]\n",
    "            \n",
    "            # ã‚·ãƒ£ãƒƒãƒ•ãƒ«å¾Œã®äºˆæ¸¬\n",
    "            y_pred_shuffled = model.predict(X_shuffled, verbose=0)\n",
    "            y_pred_shuffled_last = y_pred_shuffled[:, -1] if len(y_pred_shuffled.shape) > 1 else y_pred_shuffled\n",
    "            \n",
    "            # MAEå¢—åŠ é‡ã‚’è¨ˆç®—\n",
    "            shuffled_mae = mean_absolute_error(y_true_last, y_pred_shuffled_last)\n",
    "            mae_increase = shuffled_mae - baseline_mae\n",
    "            mae_increases.append(mae_increase)\n",
    "        \n",
    "        # çµ±è¨ˆé‡ã‚’ä¿å­˜\n",
    "        importance_scores[feat_name] = {\n",
    "            'mean': np.mean(mae_increases),\n",
    "            'std': np.std(mae_increases),\n",
    "            'raw_scores': mae_increases\n",
    "        }\n",
    "    \n",
    "    return importance_scores, baseline_mae\n",
    "\n",
    "# ã€ä¿®æ­£1ã€‘ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’å¤§å¹…ã«å¢—ã‚„ã™ï¼ˆ500 â†’ 5000ï¼‰\n",
    "n_samples = min(5000, len(X_train_reshaped))  # 5000ã‚µãƒ³ãƒ—ãƒ«ã§çµ±è¨ˆçš„ã«å®‰å®š\n",
    "X_sample = X_train_reshaped[:n_samples].copy()\n",
    "y_sample = y_train[:n_samples].copy()  # ã€ä¿®æ­£2ã€‘y_testã§ã¯ãªãy_trainã‚’ä½¿ç”¨\n",
    "\n",
    "print(f\"Using {n_samples} samples for permutation importance\")\n",
    "print(f\"Data shape: X={X_sample.shape}, y={y_sample.shape}\")\n",
    "print(f\"Data source: X_train[:n_samples] and y_train[:n_samples]\")\n",
    "\n",
    "# ã€ä¿®æ­£3ã€‘cross_sampleæˆ¦ç•¥ã§å®Ÿè¡Œï¼ˆæ™‚ç³»åˆ—ã®é †åºã‚’ä¿æŒï¼‰\n",
    "importance_scores, baseline_mae = custom_permutation_importance_convlstm(\n",
    "    model, X_sample, y_sample, feature_group_info, \n",
    "    n_repeats=10, \n",
    "    random_state=42,\n",
    "    shuffle_strategy='cross_sample'  # æ™‚ç³»åˆ—é †åºã‚’ä¿æŒã—ã¤ã¤ã‚µãƒ³ãƒ—ãƒ«é–“ã§ã‚·ãƒ£ãƒƒãƒ•ãƒ«\n",
    ")\n",
    "\n",
    "print(\"\\nPermutation importance calculation completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c511ed",
   "metadata": {},
   "source": [
    "## 8. ç‰¹å¾´é‡é‡è¦åº¦ã®é›†è¨ˆã¨åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d22c2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# é‡è¦åº¦ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ\n",
    "importance_data = []\n",
    "for feat_name, scores in importance_scores.items():\n",
    "    importance_data.append({\n",
    "        'feature': feat_name,\n",
    "        'importance_mean': scores['mean'],\n",
    "        'importance_std': scores['std']\n",
    "    })\n",
    "\n",
    "importance_df = pd.DataFrame(importance_data)\n",
    "\n",
    "# ã‚«ãƒ†ã‚´ãƒªã‚’è¿½åŠ \n",
    "def assign_category(feature_name):\n",
    "    for category, features in feature_groups.items():\n",
    "        if feature_name in features:\n",
    "            return category\n",
    "    return 'other'\n",
    "\n",
    "importance_df['category'] = importance_df['feature'].apply(assign_category)\n",
    "\n",
    "# é‡è¦åº¦ã§ã‚½ãƒ¼ãƒˆ\n",
    "importance_df = importance_df.sort_values('importance_mean', ascending=False)\n",
    "\n",
    "print(\"Feature Importance (Top 20):\")\n",
    "print(importance_df.head(20).to_string(index=False))\n",
    "\n",
    "# ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®çµ±è¨ˆ\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Category-wise Importance Summary:\")\n",
    "print(\"=\"*60)\n",
    "category_stats = importance_df.groupby('category').agg({\n",
    "    'importance_mean': ['sum', 'mean', 'count']\n",
    "}).round(4)\n",
    "category_stats.columns = ['total_importance', 'avg_importance', 'n_features']\n",
    "category_stats = category_stats.sort_values('total_importance', ascending=False)\n",
    "print(category_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d7b267",
   "metadata": {},
   "source": [
    "## 10. çµæœã®ä¿å­˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ac114e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.1 ç‰¹å¾´é‡é‡è¦åº¦ã®è©³ç´°ã‚’CSVä¿å­˜\n",
    "output_dir = f'{rootPath}/notebook'\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# è©³ç´°CSV\n",
    "csv_path = os.path.join(output_dir, f'feature_importance_detailed_{timestamp}.csv')\n",
    "importance_df.to_csv(csv_path, index=False)\n",
    "print(f\"Detailed importance saved to: {csv_path}\")\n",
    "\n",
    "# ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚µãƒãƒªãƒ¼CSV\n",
    "category_csv_path = os.path.join(output_dir, f'category_importance_summary_{timestamp}.csv')\n",
    "category_stats.to_csv(category_csv_path)\n",
    "print(f\"Category summary saved to: {category_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7758a353",
   "metadata": {},
   "source": [
    "## 11. åˆ†æã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0075d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ\n",
    "report = f\"\"\"\n",
    "# ç‰¹å¾´é‡é‡è¦åº¦åˆ†æãƒ¬ãƒãƒ¼ãƒˆ\n",
    "\n",
    "**åˆ†ææ—¥æ™‚**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "**ãƒ¢ãƒ‡ãƒ«**: Bidirectional ConvLSTM {model_path.split('/')[-1]}\n",
    "**ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ**: {data_path.split('/')[-1]}\n",
    "**æ‰‹æ³•**: Permutation Importance (n_repeats=10)\n",
    "**è©•ä¾¡æŒ‡æ¨™**: MAE (Mean Absolute Error)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. åŸºæœ¬æƒ…å ±\n",
    "\n",
    "- **ç·ç‰¹å¾´é‡æ•°**: {len(importance_df)}\n",
    "- **åˆ†æã‚µãƒ³ãƒ—ãƒ«æ•°**: {n_samples}\n",
    "- **ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³MAE**: {baseline_mae:.4f} ç§’ ({baseline_mae/60:.2f} åˆ†)\n",
    "- **ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³RMSE**: {baseline_rmse:.4f} ç§’ ({baseline_rmse/60:.2f} åˆ†)\n",
    "- **ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³RÂ²**: {baseline_r2:.4f}\n",
    "\n",
    "---\n",
    "\n",
    "## 2. ç‰¹å¾´é‡ãƒ©ãƒ³ã‚­ãƒ³ã‚°\n",
    "\n",
    "| Rank | Feature | Category | Importance | Std |\n",
    "|------|---------|----------|------------|----- |\n",
    "\"\"\"\n",
    "\n",
    "for idx, row in importance_df.iterrows():\n",
    "    report += f\"| {idx+1} | {row['feature']} | {row['category']} | {row['importance_mean']:.6f} | {row['importance_std']:.6f} |\\n\"\n",
    "\n",
    "report += f\"\"\"\n",
    "\n",
    "---\n",
    "\n",
    "## 3. ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æ\n",
    "\n",
    "| Category | Total Importance | Avg Importance | N Features |\n",
    "|----------|------------------|----------------|------------|\n",
    "\"\"\"\n",
    "\n",
    "for cat, row in category_stats.iterrows():\n",
    "    report += f\"| {cat.capitalize()} | {row['total_importance']:.6f} | {row['avg_importance']:.6f} | {int(row['n_features'])} |\\n\"\n",
    "\n",
    "report += f\"\"\"\n",
    "\n",
    "---\n",
    "\n",
    "## 4. ä¸»è¦ãªç™ºè¦‹\n",
    "\n",
    "### 4.1 ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®é‡è¦åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°\n",
    "\n",
    "1. **{category_stats.index[0].capitalize()}**: åˆè¨ˆé‡è¦åº¦ {category_stats.iloc[0]['total_importance']:.4f}\n",
    "2. **{category_stats.index[1].capitalize()}**: åˆè¨ˆé‡è¦åº¦ {category_stats.iloc[1]['total_importance']:.4f}\n",
    "3. **{category_stats.index[2].capitalize()}**: åˆè¨ˆé‡è¦åº¦ {category_stats.iloc[2]['total_importance']:.4f}\n",
    "\n",
    "### 4.2 æœ€ã‚‚é‡è¦ãªå˜ä¸€ç‰¹å¾´é‡\n",
    "\n",
    "**{importance_df.iloc[0]['feature']}** ({importance_df.iloc[0]['category']}ã‚«ãƒ†ã‚´ãƒª)\n",
    "- é‡è¦åº¦: {importance_df.iloc[0]['importance_mean']:.6f}\n",
    "- ã“ã®ç‰¹å¾´é‡ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã™ã‚‹ã¨MAEãŒç´„ {importance_df.iloc[0]['importance_mean']:.2f}ç§’å¢—åŠ \n",
    "\n",
    "### 4.3 æ¨å¥¨äº‹é …\n",
    "\n",
    "1. **é‡è¦ç‰¹å¾´é‡ã®ä¿æŒ**: Top 20ç‰¹å¾´é‡ã¯å¿…ãšä¿æŒã™ã‚‹ã“ã¨\n",
    "2. **ä½é‡è¦åº¦ç‰¹å¾´é‡ã®å‰Šé™¤å€™è£œ**: é‡è¦åº¦ãŒ0.001æœªæº€ã®ç‰¹å¾´é‡ã¯å‰Šé™¤ã‚’æ¤œè¨\n",
    "3. **ã‚«ãƒ†ã‚´ãƒªãƒãƒ©ãƒ³ã‚¹**: æœ€ã‚‚é‡è¦ãªã‚«ãƒ†ã‚´ãƒªï¼ˆ{category_stats.index[0]}ï¼‰ã‹ã‚‰ã®ç‰¹å¾´é‡ã‚’å„ªå…ˆçš„ã«ä¿æŒ\n",
    "\n",
    "---\n",
    "\n",
    "## 5. æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—\n",
    "\n",
    "- [ ] ä½é‡è¦åº¦ç‰¹å¾´é‡ï¼ˆä¸‹ä½20%ï¼‰ã‚’é™¤å¤–ã—ãŸãƒ¢ãƒ‡ãƒ«ã§æ€§èƒ½è©•ä¾¡\n",
    "- [ ] Top 30ç‰¹å¾´é‡ã®ã¿ã§ãƒ¢ãƒ‡ãƒ«ã‚’å†å­¦ç¿’\n",
    "- [ ] ç›¸é–¢åˆ†æã«ã‚ˆã‚Šå†—é•·ãªç‰¹å¾´é‡ã‚’ç‰¹å®š\n",
    "- [ ] æ–°è¦ç‰¹å¾´é‡ã®è¿½åŠ æ¤œè¨ï¼ˆç§»å‹•å¹³å‡ã€äº¤äº’ä½œç”¨é …ãªã©ï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "**ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«**:\n",
    "- `feature_importance_top20.png`\n",
    "- `category_importance_comparison.png`\n",
    "- `feature_importance_heatmap.png`\n",
    "- `category_count_vs_importance.png`\n",
    "- `feature_importance_detailed_{timestamp}.csv`\n",
    "- `category_importance_summary_{timestamp}.csv`\n",
    "\"\"\"\n",
    "\n",
    "# ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜\n",
    "report_path = os.path.join(output_dir, f'feature_importance_report_{timestamp}.md')\n",
    "with open(report_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(f\"\\nReport saved to: {report_path}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(report)\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9333efef",
   "metadata": {},
   "source": [
    "## 12. ã¾ã¨ã‚\n",
    "\n",
    "ã“ã®åˆ†æã«ã‚ˆã‚Šã€ä»¥ä¸‹ãŒæ˜ã‚‰ã‹ã«ãªã‚Šã¾ã—ãŸï¼š\n",
    "\n",
    "1. **ç‰¹å¾´é‡ã®é‡è¦åº¦**: å…¨ç‰¹å¾´é‡ã®å®šé‡çš„ãªé‡è¦åº¦ã‚¹ã‚³ã‚¢ã‚’å–å¾—\n",
    "2. **ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®å¯„ä¸**: ã©ã®ã‚«ãƒ†ã‚´ãƒªï¼ˆæ™‚é–“/ä½ç½®/çµ±è¨ˆ/æ°—å€™ï¼‰ãŒæœ€ã‚‚äºˆæ¸¬ã«è²¢çŒ®ã—ã¦ã„ã‚‹ã‹\n",
    "3. **å‰Šæ¸›å€™è£œ**: æ€§èƒ½ã«å¤§ããå½±éŸ¿ã—ãªã„ä½é‡è¦åº¦ç‰¹å¾´é‡ã®ç‰¹å®š\n",
    "\n",
    "æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã¨ã—ã¦ã€ã“ã®åˆ†æçµæœã‚’åŸºã«ï¼š\n",
    "- ç‰¹å¾´é‡é¸æŠã®æœ€é©åŒ–ï¼ˆTODO 1.2-1.3ï¼‰\n",
    "- æ–°è¦ç‰¹å¾´é‡ã®è¿½åŠ ï¼ˆTODO 1.4ï¼‰\n",
    "\n",
    "ã‚’å®Ÿæ–½ã™ã‚‹ã“ã¨ã§ã€ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½å‘ä¸Šã¨æ¨è«–é€Ÿåº¦æ”¹å–„ãŒæœŸå¾…ã§ãã¾ã™ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
