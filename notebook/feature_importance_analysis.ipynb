{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43328a4a",
   "metadata": {},
   "source": [
    "## 1. セットアップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5d4d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なライブラリのインポート\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "from pathlib import Path\n",
    "\n",
    "rootPath = Path.cwd().parent\n",
    "sys.path.append(str(rootPath))\n",
    "from src.timeseries_processing import SequenceCreator, DataSplitter, DataStandardizer, feature_groups_route_based\n",
    "\n",
    "feature_groups = feature_groups_route_based\n",
    "\n",
    "# 日本語フォント設定（可視化用）\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Analysis started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "data_path = f'{rootPath}/data/delay_analysis_route_based.csv'\n",
    "print(f\"Loading data from {data_path}...\")\n",
    "model_path = f'{rootPath}/files/model/best_delay_model_route_based_20251102_061332.h5'\n",
    "print(f\"Loading model from {model_path}...\")\n",
    "# ConvLSTM用の特徴量グループ定義"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b847131e",
   "metadata": {},
   "source": [
    "## 2. データ読み込みと前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8d22e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ読み込み\n",
    "print(\"Loading dataset...\")\n",
    "delay_features = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"Dataset shape: {delay_features.shape}\")\n",
    "print(f\"\\nColumn names ({len(delay_features.columns)} features):\")\n",
    "print(delay_features.columns.tolist())\n",
    "\n",
    "# 基本統計\n",
    "print(f\"\\nBasic statistics:\")\n",
    "print(f\"- Routes: {delay_features['route_id'].nunique()}\")\n",
    "print(f\"- Date range: {delay_features['time_bucket'].min()} to {delay_features['time_bucket'].max()}\")\n",
    "print(f\"- Mean delay: {delay_features['arrival_delay'].mean():.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b415db",
   "metadata": {},
   "source": [
    "## 4. 時系列データの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ed01e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 時系列シーケンス作成\n",
    "print(\"Creating time series sequences...\")\n",
    "print(\"Note: feature_groups has intentional duplicate (distance_from_downtown_km appears twice)\")\n",
    "\n",
    "sequence_creator = SequenceCreator(\n",
    "    input_timesteps=8, \n",
    "    output_timesteps=3,\n",
    "    feature_groups=feature_groups\n",
    ")\n",
    "\n",
    "X_delay, y_delay, route_direction_info, used_features, feature_group_info = sequence_creator.create_route_direction_aware_sequences(\n",
    "    delay_features,\n",
    "    spatial_organization=True\n",
    ")\n",
    "\n",
    "print(f\"\\nSequence shapes:\")\n",
    "print(f\"  X: {X_delay.shape}\")\n",
    "print(f\"  y: {y_delay.shape}\")\n",
    "print(f\"\\nUsed features ({len(used_features)}):\")\n",
    "for i, feat in enumerate(used_features, 1):\n",
    "    print(f\"  {i}. {feat}\")\n",
    "\n",
    "if feature_group_info:\n",
    "    print(f\"\\n=== Feature Group Info ===\")\n",
    "    for group_name, info in feature_group_info.items():\n",
    "        print(f\"{group_name}: {info['features']} (indices {info['start_idx']}:{info['end_idx']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2a7634",
   "metadata": {},
   "source": [
    "## 5. データ分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c9f95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ分割\n",
    "print(\"Splitting data...\")\n",
    "splitter = DataSplitter()\n",
    "\n",
    "X_train, X_test, y_train, y_test, train_routes, test_routes = splitter.train_test_split_by_route_direction(\n",
    "    X_delay, y_delay, route_direction_info, train_ratio=0.9\n",
    ")\n",
    "standardizer = DataStandardizer()\n",
    "X_train_scaled = standardizer.fit_transform_features(X_train)\n",
    "X_test_scaled = standardizer.transform_features(X_test)\n",
    "\n",
    "actual_feature_count = X_train.shape[2]\n",
    "X_train_reshaped = splitter.reshape_for_convlstm(\n",
    "    X_train_scaled, target_height=1, target_width=actual_feature_count\n",
    ")\n",
    "X_test_reshaped = splitter.reshape_for_convlstm(\n",
    "    X_test_scaled, target_height=1, target_width=actual_feature_count\n",
    ") \n",
    "print(f\"\\nData split:\")\n",
    "print(f\"  Train: X={X_train_reshaped.shape}, y={y_train.shape}\")\n",
    "print(f\"  Test:  X={X_test_reshaped.shape}, y={y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27691a1c",
   "metadata": {},
   "source": [
    "## 6. モデル読み込み\n",
    "\n",
    "学習済みのConvLSTMモデルを読み込みます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3184586e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル読み込み\n",
    "print(f\"Loading model from {model_path}...\")\n",
    "model = tf.keras.models.load_model(model_path, compile=False)\n",
    "\n",
    "# モデル情報表示\n",
    "print(\"\\nModel loaded successfully!\")\n",
    "print(f\"Input shape: {model.input.shape}\")\n",
    "print(f\"  → Expected: (batch, 8 timesteps, 1 height, 17 width/features, 1 channels)\")\n",
    "print(f\"Output shape: {model.output.shape}\")\n",
    "print(f\"Total parameters: {model.count_params():,}\")\n",
    "\n",
    "# データ形状が一致するか確認\n",
    "expected_width = model.input.shape[3]\n",
    "actual_width = X_train_reshaped.shape[3]\n",
    "print(f\"\\n=== Shape Compatibility Check ===\")\n",
    "print(f\"Model expects: width={expected_width} features\")\n",
    "print(f\"Data provides: width={actual_width} features\")\n",
    "\n",
    "if expected_width != actual_width:\n",
    "    raise ValueError(\n",
    "        f\"Shape mismatch! Model expects {expected_width} features, \"\n",
    "        f\"but data has {actual_width} features. \"\n",
    "        f\"Check feature_groups definition.\"\n",
    "    )\n",
    "\n",
    "print(\"✓ Shape compatibility confirmed!\")\n",
    "\n",
    "# 基本性能評価\n",
    "print(\"\\nEvaluating baseline performance...\")\n",
    "y_pred = model.predict(X_test_reshaped, verbose=0)\n",
    "\n",
    "# 最後のタイムステップで評価\n",
    "y_test_last = y_test[:, -1] if len(y_test.shape) > 1 else y_test\n",
    "y_pred_last = y_pred[:, -1] if len(y_pred.shape) > 1 else y_pred\n",
    "\n",
    "baseline_mae = mean_absolute_error(y_test_last, y_pred_last)\n",
    "baseline_rmse = np.sqrt(mean_squared_error(y_test_last, y_pred_last))\n",
    "baseline_r2 = r2_score(y_test_last, y_pred_last)\n",
    "\n",
    "print(f\"\\nBaseline Performance (last timestep):\")\n",
    "print(f\"  MAE:  {baseline_mae:.4f} seconds ({baseline_mae/60:.2f} minutes)\")\n",
    "print(f\"  RMSE: {baseline_rmse:.4f} seconds ({baseline_rmse/60:.2f} minutes)\")\n",
    "print(f\"  R²:   {baseline_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54089e2a",
   "metadata": {},
   "source": [
    "## 7. カスタムPermutation Importanceの実装\n",
    "\n",
    "**重要**: 既存の学習済みモデルは入力形状が固定されているため、特徴量を削除/追加すると使用できません。\n",
    "そのため、以下の戦略を採用します：\n",
    "\n",
    "### 戦略\n",
    "1. **4次元入力を保持**: ConvLSTMの入力形状 (samples, timesteps, height, width) をそのまま使用\n",
    "2. **特徴量レベルでシャッフル**: width次元内の各特徴量インデックスをシャッフル\n",
    "3. **全タイムステップに適用**: 時系列の連続性を維持しながら特徴量の寄与を測定\n",
    "\n",
    "### 制限事項\n",
    "- モデル構造の変更は不可（再学習が必要）\n",
    "- 特徴量の追加/削除は不可（再学習が必要）\n",
    "- 特徴量の重要度のみを分析可能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062a3890",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_permutation_importance_convlstm(model, X, y, feature_group_info, \n",
    "                                           n_repeats=10, random_state=42,\n",
    "                                           shuffle_strategy='cross_sample'):\n",
    "    \"\"\"\n",
    "    ConvLSTM用のカスタムPermutation Importance（時系列対応版・堅牢化）\n",
    "    \n",
    "    Args:\n",
    "        model: 学習済みConvLSTMモデル\n",
    "        X: 4次元入力 (samples, timesteps, height, width)\n",
    "        y: 目標値 (samples, output_timesteps)\n",
    "        feature_group_info: 特徴量グループ情報\n",
    "        n_repeats: シャッフル回数\n",
    "        random_state: 乱数シード\n",
    "        shuffle_strategy: シャッフル戦略\n",
    "            - 'cross_sample': サンプル間でシャッフル（時系列順序は保持）【推奨】\n",
    "            - 'block': ブロック単位でシャッフル（局所性を保持）\n",
    "            - 'within_sample': サンプル内でシャッフル（時系列破壊・非推奨）\n",
    "    \n",
    "    Returns:\n",
    "        importance_dict: {feature_name: {'mean': float, 'std': float, 'raw_scores': list}}\n",
    "        baseline_mae: ベースラインのMAE\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # ベースライン性能を計算\n",
    "    y_pred_baseline = model.predict(X, verbose=0)\n",
    "    y_true_last = y[:, -1] if len(y.shape) > 1 else y\n",
    "    y_pred_last = y_pred_baseline[:, -1] if len(y_pred_baseline.shape) > 1 else y_pred_baseline\n",
    "    baseline_mae = mean_absolute_error(y_true_last, y_pred_last)\n",
    "    \n",
    "    print(f\"Baseline MAE: {baseline_mae:.4f} seconds\")\n",
    "    print(f\"Shuffle strategy: {shuffle_strategy}\")\n",
    "    print(f\"Sample size: {X.shape[0]}\")\n",
    "    print(f\"\\nCalculating feature importance...\")\n",
    "    \n",
    "    # 特徴量名とインデックスのマッピング\n",
    "    feature_indices = {}\n",
    "    for group_name, info in feature_group_info.items():\n",
    "        for i, feat in enumerate(info['features']):\n",
    "            if feat != 'arrival_delay':  # ターゲットは除外\n",
    "                width_idx = info['start_idx'] + i\n",
    "                feature_indices[feat] = width_idx\n",
    "    \n",
    "    importance_scores = {}\n",
    "    \n",
    "    # 各特徴量について\n",
    "    for feat_name, width_idx in feature_indices.items():\n",
    "        print(f\"  Processing: {feat_name} (index {width_idx})...\")\n",
    "        mae_increases = []\n",
    "        \n",
    "        # n_repeats回シャッフル\n",
    "        for repeat in range(n_repeats):\n",
    "            # データのコピー\n",
    "            X_shuffled = X.copy()\n",
    "            \n",
    "            if shuffle_strategy == 'cross_sample':\n",
    "                # 【推奨】サンプル間でシャッフル（時系列の順序は保持）\n",
    "                # 例: サンプルAの特徴量 → サンプルBの時系列に配置\n",
    "                shuffle_idx = np.random.permutation(X.shape[0])\n",
    "                # 時系列全体をまとめてシャッフル（timestep次元はそのまま）\n",
    "                X_shuffled[:, :, :, width_idx] = X[shuffle_idx, :, :, width_idx]\n",
    "                \n",
    "            elif shuffle_strategy == 'block':\n",
    "                # ブロック単位でシャッフル（時系列の局所性を保持）\n",
    "                block_size = 2  # 2タイムステップを1ブロック\n",
    "                n_timesteps = X.shape[1]\n",
    "                \n",
    "                for sample_idx in range(X.shape[0]):\n",
    "                    # ブロック数\n",
    "                    n_blocks = n_timesteps // block_size\n",
    "                    block_perm = np.random.permutation(n_blocks)\n",
    "                    \n",
    "                    X_temp = X_shuffled[sample_idx, :, :, width_idx].copy()\n",
    "                    for new_pos, old_pos in enumerate(block_perm):\n",
    "                        start_new = new_pos * block_size\n",
    "                        end_new = min(start_new + block_size, n_timesteps)\n",
    "                        start_old = old_pos * block_size\n",
    "                        end_old = min(start_old + block_size, n_timesteps)\n",
    "                        X_shuffled[sample_idx, start_new:end_new, :, width_idx] = X_temp[start_old:end_old, :]\n",
    "                        \n",
    "            elif shuffle_strategy == 'within_sample':\n",
    "                # サンプル内でシャッフル（時系列破壊・非推奨）\n",
    "                for sample_idx in range(X.shape[0]):\n",
    "                    timestep_perm = np.random.permutation(X.shape[1])\n",
    "                    X_shuffled[sample_idx, :, :, width_idx] = X_shuffled[sample_idx, timestep_perm, :, width_idx]\n",
    "            \n",
    "            # シャッフル後の予測\n",
    "            y_pred_shuffled = model.predict(X_shuffled, verbose=0)\n",
    "            y_pred_shuffled_last = y_pred_shuffled[:, -1] if len(y_pred_shuffled.shape) > 1 else y_pred_shuffled\n",
    "            \n",
    "            # MAE増加量を計算\n",
    "            shuffled_mae = mean_absolute_error(y_true_last, y_pred_shuffled_last)\n",
    "            mae_increase = shuffled_mae - baseline_mae\n",
    "            mae_increases.append(mae_increase)\n",
    "        \n",
    "        # 統計量を保存\n",
    "        importance_scores[feat_name] = {\n",
    "            'mean': np.mean(mae_increases),\n",
    "            'std': np.std(mae_increases),\n",
    "            'raw_scores': mae_increases\n",
    "        }\n",
    "    \n",
    "    return importance_scores, baseline_mae\n",
    "\n",
    "# 【修正1】サンプル数を大幅に増やす（500 → 5000）\n",
    "n_samples = min(5000, len(X_train_reshaped))  # 5000サンプルで統計的に安定\n",
    "X_sample = X_train_reshaped[:n_samples].copy()\n",
    "y_sample = y_train[:n_samples].copy()  # 【修正2】y_testではなくy_trainを使用\n",
    "\n",
    "print(f\"Using {n_samples} samples for permutation importance\")\n",
    "print(f\"Data shape: X={X_sample.shape}, y={y_sample.shape}\")\n",
    "print(f\"Data source: X_train[:n_samples] and y_train[:n_samples]\")\n",
    "\n",
    "# 【修正3】cross_sample戦略で実行（時系列の順序を保持）\n",
    "importance_scores, baseline_mae = custom_permutation_importance_convlstm(\n",
    "    model, X_sample, y_sample, feature_group_info, \n",
    "    n_repeats=10, \n",
    "    random_state=42,\n",
    "    shuffle_strategy='cross_sample'  # 時系列順序を保持しつつサンプル間でシャッフル\n",
    ")\n",
    "\n",
    "print(\"\\nPermutation importance calculation completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c511ed",
   "metadata": {},
   "source": [
    "## 8. 特徴量重要度の集計と分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d22c2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重要度データフレーム作成\n",
    "importance_data = []\n",
    "for feat_name, scores in importance_scores.items():\n",
    "    importance_data.append({\n",
    "        'feature': feat_name,\n",
    "        'importance_mean': scores['mean'],\n",
    "        'importance_std': scores['std']\n",
    "    })\n",
    "\n",
    "importance_df = pd.DataFrame(importance_data)\n",
    "\n",
    "# カテゴリを追加\n",
    "def assign_category(feature_name):\n",
    "    for category, features in feature_groups.items():\n",
    "        if feature_name in features:\n",
    "            return category\n",
    "    return 'other'\n",
    "\n",
    "importance_df['category'] = importance_df['feature'].apply(assign_category)\n",
    "\n",
    "# 重要度でソート\n",
    "importance_df = importance_df.sort_values('importance_mean', ascending=False)\n",
    "\n",
    "print(\"Feature Importance (Top 20):\")\n",
    "print(importance_df.head(20).to_string(index=False))\n",
    "\n",
    "# カテゴリ別の統計\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Category-wise Importance Summary:\")\n",
    "print(\"=\"*60)\n",
    "category_stats = importance_df.groupby('category').agg({\n",
    "    'importance_mean': ['sum', 'mean', 'count']\n",
    "}).round(4)\n",
    "category_stats.columns = ['total_importance', 'avg_importance', 'n_features']\n",
    "category_stats = category_stats.sort_values('total_importance', ascending=False)\n",
    "print(category_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d7b267",
   "metadata": {},
   "source": [
    "## 10. 結果の保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ac114e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.1 特徴量重要度の詳細をCSV保存\n",
    "output_dir = f'{rootPath}/notebook'\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# 詳細CSV\n",
    "csv_path = os.path.join(output_dir, f'feature_importance_detailed_{timestamp}.csv')\n",
    "importance_df.to_csv(csv_path, index=False)\n",
    "print(f\"Detailed importance saved to: {csv_path}\")\n",
    "\n",
    "# カテゴリ別サマリーCSV\n",
    "category_csv_path = os.path.join(output_dir, f'category_importance_summary_{timestamp}.csv')\n",
    "category_stats.to_csv(category_csv_path)\n",
    "print(f\"Category summary saved to: {category_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7758a353",
   "metadata": {},
   "source": [
    "## 11. 分析サマリーレポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0075d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# レポート生成\n",
    "report = f\"\"\"\n",
    "# 特徴量重要度分析レポート\n",
    "\n",
    "**分析日時**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "**モデル**: Bidirectional ConvLSTM {model_path.split('/')[-1]}\n",
    "**データセット**: {data_path.split('/')[-1]}\n",
    "**手法**: Permutation Importance (n_repeats=10)\n",
    "**評価指標**: MAE (Mean Absolute Error)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. 基本情報\n",
    "\n",
    "- **総特徴量数**: {len(importance_df)}\n",
    "- **分析サンプル数**: {n_samples}\n",
    "- **ベースラインMAE**: {baseline_mae:.4f} 秒 ({baseline_mae/60:.2f} 分)\n",
    "- **ベースラインRMSE**: {baseline_rmse:.4f} 秒 ({baseline_rmse/60:.2f} 分)\n",
    "- **ベースラインR²**: {baseline_r2:.4f}\n",
    "\n",
    "---\n",
    "\n",
    "## 2. 特徴量ランキング\n",
    "\n",
    "| Rank | Feature | Category | Importance | Std |\n",
    "|------|---------|----------|------------|----- |\n",
    "\"\"\"\n",
    "\n",
    "for idx, row in importance_df.iterrows():\n",
    "    report += f\"| {idx+1} | {row['feature']} | {row['category']} | {row['importance_mean']:.6f} | {row['importance_std']:.6f} |\\n\"\n",
    "\n",
    "report += f\"\"\"\n",
    "\n",
    "---\n",
    "\n",
    "## 3. カテゴリ別分析\n",
    "\n",
    "| Category | Total Importance | Avg Importance | N Features |\n",
    "|----------|------------------|----------------|------------|\n",
    "\"\"\"\n",
    "\n",
    "for cat, row in category_stats.iterrows():\n",
    "    report += f\"| {cat.capitalize()} | {row['total_importance']:.6f} | {row['avg_importance']:.6f} | {int(row['n_features'])} |\\n\"\n",
    "\n",
    "report += f\"\"\"\n",
    "\n",
    "---\n",
    "\n",
    "## 4. 主要な発見\n",
    "\n",
    "### 4.1 カテゴリ別の重要度ランキング\n",
    "\n",
    "1. **{category_stats.index[0].capitalize()}**: 合計重要度 {category_stats.iloc[0]['total_importance']:.4f}\n",
    "2. **{category_stats.index[1].capitalize()}**: 合計重要度 {category_stats.iloc[1]['total_importance']:.4f}\n",
    "3. **{category_stats.index[2].capitalize()}**: 合計重要度 {category_stats.iloc[2]['total_importance']:.4f}\n",
    "\n",
    "### 4.2 最も重要な単一特徴量\n",
    "\n",
    "**{importance_df.iloc[0]['feature']}** ({importance_df.iloc[0]['category']}カテゴリ)\n",
    "- 重要度: {importance_df.iloc[0]['importance_mean']:.6f}\n",
    "- この特徴量をシャッフルするとMAEが約 {importance_df.iloc[0]['importance_mean']:.2f}秒増加\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# レポート保存\n",
    "report_path = os.path.join(output_dir, f'feature_importance_report_{timestamp}.md')\n",
    "with open(report_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(f\"\\nReport saved to: {report_path}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(report)\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
